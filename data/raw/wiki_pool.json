[
  {
    "id": "wiki_14400",
    "text": "The history of science covers the development of science from ancient times to the present. It encompasses all three major branches of science: natural, social, and formal. Protoscience, early sciences, and natural philosophies such as alchemy and astrology that existed during the Bronze Age, Iron Age, classical antiquity and the Middle Ages, declined during the early modern period after the establishment of formal disciplines of science in the Age of Enlightenment.\nThe earliest roots of scientific thinking and practice can be traced to Ancient Egypt and Mesopotamia during the 3rd and 2nd millennia BCE. These civilizations' contributions to mathematics, astronomy, and medicine influenced later Greek natural philosophy of classical antiquity, wherein formal attempts were made to provide explanations of events in the physical world based on natural causes. After the fall of the Western Roman Empire, knowledge of Greek conceptions of the world deteriorated in Latin-speaking Western Europe during the early centuries (400 to 1000 CE) of the Middle Ages, but continued to thrive in the Greek-speaking Byzantine Empire. Aided by translations of Greek texts, the Hellenistic worldview was preserved and absorbed into the Arabic-speaking Muslim world during the Islamic Golden Age. The recovery and assimilation of Greek works and Islamic inquiries into Western Europe from the 10th to 13th century revived the learning of natural philosophy in the West. Traditions of early science were also developed in ancient India and separately in ancient China, the Chinese model having influenced Vietnam, Korea and Japan before Western exploration. Among the Pre-Columbian peoples of Mesoamerica, the Zapotec civilization established their first known traditions of astronomy and mathematics for producing calendars, followed by other civilizations such as the Maya.\nNatural philosophy was transformed by the Scientific Revolution that transpired during the 16th and 17th centuries in Europe, as new ideas and discoveries departed from previous Greek conceptions and traditions. The New Science that emerged was more mechanistic in its worldview, more integrated with mathematics, and more reliable and open as its knowledge was based on a newly defined scientific method. More \"revolutions\" in subsequent centuries soon followed. The chemical revolution of the 18th century, for instance, introduced new quantitative methods and measurements for chemistry. In the 19th century, new perspectives regarding the conservation of energy, age of Earth, and evolution came into focus. And in the 20th century, new discoveries in genetics and physics laid the foundations for new sub disciplines such as molecular biology and particle physics. Moreover, industrial and military concerns as well as the increasing complexity of new research endeavors ushered in the era of \"big science,\" particularly after World War II.\n\nApproaches to history of science\nThe nature of the history of science - including both",
    "source": "wikipedia",
    "title": "History of science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1802769",
    "text": "The history and philosophy of science (HPS) is an academic discipline that encompasses the philosophy of science and the history of science. Although many scholars in the field are trained primarily as either historians or as philosophers, there are degree-granting departments of HPS at several prominent universities. Though philosophy of science and history of science are their own disciplines, history and philosophy of science is a discipline in its own right.\nPhilosophy of science is a branch of philosophy concerned with the foundations, methods, and implications of science. The central questions of this study concern what qualifies as science, the reliability of scientific theories, and the ultimate purpose of science. This discipline overlaps with metaphysics/ontology and epistemology, for example, when it explores the relationship between science and truth. Philosophy of science focuses on metaphysical, epistemic and semantic aspects of science. Ethical issues such as bioethics and scientific misconduct are often considered ethics or science studies rather than philosophy of science.\nThere is no consensus among philosophers about many of the central problems concerned with the philosophy of science, including whether science can reveal the truth about unobservable things and whether scientific reasoning can be justified at all. In addition to these general questions about science as a whole, philosophers of science consider problems that apply to particular sciences (such as astronomy, biology, chemistry, Earth science, or physics). Some philosophers of science also use contemporary results in science to reach conclusions about philosophy itself.\n\nHistory\nOne origin of the unified discipline is the historical approach to the discipline of the philosophy of science. This hybrid approach is reflected in the career of Thomas Kuhn. His first permanent appointment, at the University of California, Berkeley, was to a position advertised by the philosophy department, but he also taught courses from the history department. When he was promoted to full professor in the history department only, Kuhn was offended at the philosophers' rejection because \"I sure as hell wanted to be there, and it was my philosophy students who were working with me, not on philosophy but on history, were nevertheless my more important students\". This attitude is also reflected in his historicist approach, as outlined in Kuhn's seminal Structure of Scientific Revolutions (1962, 2nd ed. 1970), wherein philosophical questions about scientific theories and, especially, theory change are understood in historical terms, employing concepts such as paradigm shift.\nHowever, Kuhn was also critical of attempts fully to unify the methods of history and philosophy of science: \"Subversion is not, I think, too strong a term for the likely result of an attempt to make the two fields into one. They differ in a number of their central constitutive characteristics, of which the most general",
    "source": "wikipedia",
    "title": "History and philosophy of science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_14285",
    "text": "The history of science and technology (HST) is a field of history that examines the development of the understanding of the natural world (science) and humans' ability to manipulate it (technology) at different points in time. This academic discipline also examines the cultural, economic, and political context and impacts of scientific practices; it likewise may study the consequences of new technologies on existing scientific fields.\n\nAcademic study of history of science\nHistory of science is an academic discipline with an international community of specialists. Main professional organizations for this field include the History of Science Society, the British Society for the History of Science, and the European Society for the History of Science.\nMuch of the study of the history of science has been devoted to answering questions about what science is, how it functions, and whether it exhibits large-scale patterns and trends.\nUniversities with history of science and technology programs\n\nProminent historians of the field\nSee also the list of George Sarton medalists.\nJournals and periodicals\nAnnals of Science\nThe British Journal for the History of Science\nCentaurus\nDynamis\nHistory and Technology (magazine)\nHistory of Science and Technology (journal)\nHistory of Technology (book series)\nHistorical Studies in the Physical and Biological Sciences (HSPS)\nHistorical Studies in the Natural Sciences (HSNS)\nHoST - Journal of History of Science and Technology\nICON\nIEEE Annals of the History of Computing\nIsis\nJournal of the History of Biology\nJournal of the History of Medicine and Allied Sciences\nNotes and Records of the Royal Society\nOsiris\nScience & Technology Studies\nScience in Context\nScience, Technology, & Human Values\nSocial History of Medicine\nSocial Studies of Science\nTechnology and Culture\nTransactions of the Newcomen Society\nHistoria Mathematica\nBulletin of the Scientific Instrument Society\nSee also\nHistory of science\nHistory of technology\nAncient Egyptian technology\nHistory of science and technology in China\nHistory of science and technology in Japan\nHistory of science and technology in France\nHistory of science and technology in the Indian subcontinent\nMesopotamian science\nProductivity improving technologies (historical)\nScience and technology in Argentina\nScience and technology in Canada\nScience and technology in Iran\nScience and technology in the United States\nScience in the medieval Islamic world\nScience tourism\nTechnological and industrial history of the United States\nTimeline of science and engineering in the Islamic world\n",
    "source": "wikipedia",
    "title": "History of science and technology",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_56498399",
    "text": "The Academic Family Tree, which began as Neurotree, is an online database for academic genealogy, containing numerous \"family trees\" of academic disciplines. Neurotree was established in 2005 as a family tree of neuroscientists. Later that year Academic Family Tree incorporated Neurotree and family trees of other scholarly disciplines.\nUnlike a conventional genealogy or family tree, in which connections among individuals are from kinship (e.g., parents to children), connections in Academic Family Tree are from mentoring relationships, usually among people working in academic settings (e.g., doctoral supervisors to students).\nAcademic Family Tree has been used as sources of information for the history and prospects of academic fields such as psychology, meteorology, organizational communication, and neuroscience. It has been used to address infometrics, to research issues of scientific methodology, and to examine mentor characteristics that predict mentee academic success.\n\nFunctioning and scope\nThe founders of the initial trees, including Neurotree, populated them from published sources, such as ProQuest. Later, they set up discipline-specific family trees of Academic Family Tree to be volunteer-run; accuracy is maintained by a group of volunteer editors. Hierarchical connections between mentors (\"parents\") and mentees (\"children\") are defined as any meaningful mentoring relationship (research assistant, graduate student, postdoctoral fellow, or research scientist). Continuous records extend well into the Middle Ages and earlier.\nAs of 29 September 2023, Academic Family Tree contained 871,361 people with 882,278 connections among them.\nAcademic Family Tree encompasses a broad range of discipline-specific trees. As of 29 September 2023, there were 73 trees spanning science (e.g., human genetics, microbiology, and psychology), mathematics and philosophy, engineering, the humanities (e.g., economics, law, theology, and music), and business (e.g., organizational communication and advertising).\nAll trees within Academic Family Tree are closely linked. A search for a person in one tree gives hits from all trees in Academic Family Tree.\nThe data in Academic Family Tree are owned by the nonprofit academictree.org, but they are shared under the Creative Commons License (CC-BY 3.0). This means a person may use the data in any tree for any purpose as long as the source is cited.\n",
    "source": "wikipedia",
    "title": "Academic Family Tree",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_4083089",
    "text": "An academic genealogy (or scientific genealogy) organizes a family tree of scientists and scholars according to mentoring relationships, often in the form of dissertation supervision relationships, and not according to genetic relationships as in conventional genealogy. Since the term academic genealogy has now developed this specific meaning, its additional use to describe a more academic approach to conventional genealogy would be ambiguous, so the description scholarly genealogy is now generally used in the latter context.\n\nOverview\nThe academic lineage or academic ancestry of someone is a chain of professors who have served as academic mentors or thesis advisors of each other, ending with the person in question. Many genealogical terms are often recast in terms of academic lineages, so one may speak of academic descendants, children,  siblings, etc. One method of developing an academic genealogy is to organize individuals by prioritizing their degree of relationship to a mentor/advisor as follows: (1). doctoral students, (2). post-doctoral researchers, (3). master's students and (4). current students, including undergraduate researchers.\nThrough the 19th century, particularly for graduates in sciences such as chemistry, it was common to have completed a degree in medicine or pharmacy before continuing with post-graduate or post-doctoral studies. Until the early 20th century, attaining professorial status or mentoring graduate students did not necessarily require a doctorate or graduate degree. For instance, the University of Cambridge did not require a formal doctoral thesis until 1919, and academic genealogies that include earlier Cambridge students tend to substitute an equivalent mentor. Academic genealogies are particularly easy to research in the case of Spain's doctoral degrees, because until 1954 only Complutense University had the power to grant doctorates. This means that all holders of a doctorates in Spain can trace back their academic lineage to a doctoral supervisor who was a member of Complutense's Faculty.\nWebsites such as the Mathematics Genealogy Project or the Chemical Genealogy document academic lineages for specific subject areas, while some other sites, such as Neurotree and Academic Family Tree aim to provide a complete academic genealogy across all fields of academia.\n",
    "source": "wikipedia",
    "title": "Academic genealogy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1949268",
    "text": "According to ancient and medieval science, aether (, alternative spellings include æther, aither, and ether), also known as the fifth element or quintessence, is the material that fills the region of the universe beyond the terrestrial sphere. The concept of aether was used in several theories to explain several natural phenomena, such as the propagation of light and gravity. In the late 19th century, physicists postulated that aether permeated space, providing a medium through which light could travel in a vacuum, but evidence for the presence of such a medium was not found in the Michelson–Morley experiment, and this result has been interpreted to mean that no luminiferous aether exists.\n\nMythological origins\nThe word αἰθήρ (aithḗr) in Homeric Greek means \"pure, fresh air\" or \"clear sky\". In Greek mythology, it was thought to be the pure essence that the gods breathed, filling the space where they lived, analogous to the air breathed by mortals. It is also personified as a deity, Aether, the son of Erebus and Nyx in traditional Greek mythology. Aether is related to αἴθω \"to incinerate\", and intransitive \"to burn, to shine\" (related is the name Aithiopes (Ethiopians; see Aethiopia), meaning \"people with a burnt (black) visage\").\nFifth element\nIn Plato's Timaeus (58d) speaking about air, Plato mentions that \"there is the most translucent kind which is called by the name of aether (αἰθήρ)\" but otherwise he adopted the classical system of four elements. Aristotle, who had been Plato's student at the Academy, agreed on this point with his former mentor, emphasizing additionally that fire has sometimes been mistaken for aether. However, in his Book On the Heavens he introduced a new \"first\" element to the system of the classical elements of Ionian philosophy. He noted that the four terrestrial classical elements were subject to change and naturally moved linearly. The first element however, located in the celestial regions and heavenly bodies, moved circularly and had none of the qualities the terrestrial classical elements had. It was neither hot nor cold, neither wet nor dry. With this addition the system of elements was extended to five and later commentators started referring to the new first one as the fifth and also called it aether, a word that Aristotle had used in On the Heavens and the Meteorology.\nAether differed from the four terrestrial elements; it was incapable of motion of quality or motion of quantity. Aether was only capable of local motion. Aether naturally moved in circles, and had no contrary, or unnatural, motion. Aristotle also stated that celestial spheres made of aether held the stars and planets. The idea of aethereal spheres moving with natural circular motion led to Aristotle's explanation of the observed orbits of stars and planets in perfectly circular motion.\nMedieval scholastic philosophers granted aether changes of density, in which the bodies of the planets were considered to be more dense than the medium which fille",
    "source": "wikipedia",
    "title": "Aether (classical element)",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_573",
    "text": "Alchemy (from the Arabic word al-kīmīā, الكیمیاء) is an ancient branch of natural philosophy, a philosophical and protoscientific tradition that was historically practised in China, India, the Muslim world, and Europe. In its Western form, alchemy is first attested in a number of pseudepigraphical texts written in Greco-Roman Egypt during the first few centuries AD. Greek-speaking alchemists often referred to their craft as \"the Art\" (τέχνη) or \"Knowledge\" (ἐπιστήμη), and it was often characterised as mystic (μυστική), sacred (ἱɛρά), or divine (θɛíα).\nAlchemists attempted to purify, mature, and perfect certain materials. Common aims were chrysopoeia, the transmutation of \"base metals\" (e.g., lead) into \"noble metals\" (particularly gold); the creation of an elixir of immortality; and the creation of panaceas able to cure any disease. The perfection of the human body and soul was thought to result from the alchemical magnum opus (\"Great Work\"). The concept of creating the philosophers' stone was variously connected with all of these projects.\nIslamic and European alchemists developed a basic set of laboratory techniques, theories, and terms, some of which are still in use today. They did not abandon the Ancient Greek philosophical idea that everything is composed of four elements, and they tended to guard their work in secrecy, often making use of cyphers and cryptic symbolism. In Europe, the 12th-century translations of medieval Islamic works on science and the rediscovery of Aristotelian philosophy gave birth to a flourishing tradition of Latin alchemy. This late medieval tradition of alchemy would go on to play a significant role in the development of early modern science (particularly chemistry and medicine).\nModern discussions of alchemy are generally split into an examination of its exoteric practical applications and its esoteric spiritual aspects, despite criticisms by scholars such as Eric J. Holmyard and Marie-Louise von Franz that they should be understood as complementary. The former is pursued by historians of the physical sciences, who examine the subject in terms of early chemistry, medicine, and charlatanism, and the philosophical and religious contexts in which these events occurred. The latter interests historians of esotericism, psychologists, and some philosophers and spiritualists. The subject has also made an ongoing impact on literature and the arts.\n\nEtymology\nThe word alchemy comes from Old French alkimie, used in Medieval Latin as alchymia. This name was itself adopted from the Arabic word al-kīmiyā (الكيمياء). The Arabic al-kīmiyā in turn was a borrowing of the Late Greek term khēmeía (χημεία), also spelled khumeia (χυμεία) and khēmía (χημία), with al- being the Arabic definite article 'the'. Together this association can be interpreted as 'the process of transmutation by which to fuse or reunite with the divine or original form'. Several etymologies have been proposed for the Greek term. The first was proposed by Zosimos",
    "source": "wikipedia",
    "title": "Alchemy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_39480604",
    "text": "The history of anthropometry includes its use as an early tool of anthropology, use for identification, use for the purposes of understanding human physical variation in paleoanthropology and in various attempts to correlate physical with racial and psychological traits. At various points in history, certain anthropometrics have been cited by advocates of discrimination and eugenics often as a part of some social movement or through pseudoscientific claims.\n\nCraniometry and paleoanthropology\nIn 1716 Louis-Jean-Marie Daubenton, who wrote many essays on comparative anatomy for the Académie française, published his Memoir on the Different Positions of the Occipital Foramen in Man and Animals (Mémoire sur les différences de la situation du grand trou occipital dans l'homme et dans les animaux). Six years later Pieter Camper (1722–1789), distinguished both as an artist and as an anatomist, published some lectures that laid the foundation of much work. Camper invented the \"facial angle,\" a measure meant to determine intelligence among various species. According to this technique, a \"facial angle\" was formed by drawing two lines: one horizontally from the nostril to the ear; and the other perpendicularly from the advancing part of the upper jawbone to the most prominent part of the forehead. Camper's measurements of facial angle were first made to compare the skulls of men with those of other animals. Camper claimed that antique statues presented an angle of 90°, Europeans of 80°, Central Africans of 70° and the orangutan of 58°.\nSwedish professor of anatomy Anders Retzius (1796–1860) first used the cephalic index in physical anthropology to classify ancient human remains found in Europe. He classed skulls in three main categories; \"dolichocephalic\" (from the Ancient Greek kephalê \"head\", and dolikhos \"long and thin\"), \"brachycephalic\" (short and broad) and \"mesocephalic\" (intermediate length and width). Scientific research was continued by Étienne Geoffroy Saint-Hilaire (1772–1844) and Paul Broca (1824–1880), founder of the Anthropological Society in France in 1859. Paleoanthropologists still rely upon craniofacial anthropometry to identify species in the study of fossilized hominid bones. Specimens of Homo erectus and athletic specimens of Homo sapiens, for example, are virtually identical from the neck down but their skulls can easily be told apart.\n\nSamuel George Morton (1799–1851), whose two major monographs were the Crania Americana (1839), An Inquiry into the Distinctive Characteristics of the Aboriginal Race of America and Crania Aegyptiaca (1844) concluded that the ancient Egyptians were not Negroid but Caucasoid and that Caucasians and Negroes were already distinct three thousand years ago. Since The Bible indicated that Noah's Ark had washed up on Mount Ararat only a thousand years before this Noah's sons could not account for every race on earth. According to Morton's theory of polygenism the races had been separate from the start. Josiah C.",
    "source": "wikipedia",
    "title": "History of anthropometry",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_89732",
    "text": "The Antikythera mechanism ( AN-tik-ih-THEER-ə, US also  AN-ty-kih-) is an ancient Greek hand-powered orrery (model of the Solar System). It is the oldest known example of an analogue computer. It could be used to predict astronomical positions and eclipses decades in advance. It could also be used to track the four-year cycle of athletic games similar to an olympiad, the cycle of the ancient Olympic Games.\nThe artefact was among wreckage retrieved from a shipwreck off the coast of the Greek island Antikythera in 1901. In 1902, during a visit to the National Archaeological Museum in Athens, it was noticed by Greek politician Spyridon Stais as containing a gear, prompting the first study of the fragment by his cousin, Valerios Stais, the museum director. The device, housed in the remains of a wooden-framed case of (uncertain) overall size 34 cm × 18 cm × 9 cm (13.4 in × 7.1 in × 3.5 in), was found as one lump, later separated into three main fragments which are now divided into 82 separate fragments after conservation efforts. Four of these fragments contain gears, while inscriptions are found on many others. The largest gear is about 13 cm (5 in) in diameter and originally had 223 teeth. All these fragments of the mechanism are kept at the National Archaeological Museum, along with reconstructions and replicas, to demonstrate how it may have looked and worked.\nIn 2005, a team from Cardiff University led by Mike Edmunds used computer X-ray tomography and high resolution scanning to image inside fragments of the crust-encased mechanism and read faint inscriptions that once covered the outer casing. These scans suggest that the mechanism had 37 meshing bronze gears enabling it to follow the movements of the Moon and the Sun through the zodiac, to predict eclipses and to model the irregular orbit of the Moon, where the Moon's velocity is higher in its perigee than in its apogee. This motion was studied in the 2nd century BC by astronomer Hipparchus of Rhodes, and he may have been consulted in the machine's construction. There is speculation that a portion of the mechanism is missing and it calculated the positions of the five classical planets. The inscriptions were further deciphered in 2016, revealing numbers connected with the synodic cycles of Venus and Saturn.\nThe instrument is believed to have been designed and constructed by Hellenistic scientists and been variously dated to about 87 BC, between 150 and 100 BC, or 205 BC. It must have been constructed before the shipwreck, which has been dated by multiple lines of evidence to approximately 70–60 BC. In 2022, researchers proposed its initial calibration date, not construction date, could have been 23 December 178 BC. Other experts propose 204 BC as a more likely calibration date. Machines with similar complexity did not appear again until the 14th century in western Europe.\n\nHistory\n\n",
    "source": "wikipedia",
    "title": "Antikythera mechanism",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_20860737",
    "text": "Antiquarian science books are original historical works (e.g., books or technical papers) concerning science, mathematics and sometimes engineering. These books are important primary references for the study of the history of science and technology, they can provide valuable insights into the historical development of the various fields of scientific inquiry (History of science, History of mathematics, etc.)\n\nThe landmark are significant first (or early) editions typically worth hundreds or thousands of dollars (prices may vary widely based on condition, etc.). \nReprints of these books are often available, for example from Great Books of the Western World, Dover Publications or Google Books.\nIncunabula are extremely rare and valuable, but as the Scientific Revolution is only taken to have started around the 1540s, such works of Renaissance literature (including alchemy, Renaissance magic, etc.) are not usually included under the notion of \"scientific\" literature. Printed originals of the beginning Scientific Revolution thus date to the 1540s or later, notably beginning with the original publication of Copernican heliocentrism. Nicolaus Copernicus' De revolutionibus orbium coelestium of 1543 sold for more than US$2 million at auctions.\n\nList of notable books\n\nReferences\n\nExternal links\nHeralds of Science - Smithsonian Libraries\nMilestones of Science Books - Antiquarian booksellers\n",
    "source": "wikipedia",
    "title": "Antiquarian science books",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1018868",
    "text": "Astrology and astronomy were archaically treated together (Latin: astrologia), but gradually distinguished through the Late Middle Ages into the Age of Reason. Developments in 17th century philosophy resulted in astrology and astronomy operating as independent pursuits by the 18th century.\nWhereas the academic discipline of astronomy studies observable phenomena beyond the Earth's atmosphere, astrology uses the apparent positions of celestial objects as the basis for divination.\n\nOverview\nIn pre-modern times, most cultures did not make a clear distinction between the two disciplines, putting them both together as one. In ancient Babylonia, famed for its astrology, there were not separate roles for the astronomer as predictor of celestial phenomena, and the astrologer as their interpreter; both functions were performed by the same person. In ancient Greece, pre-Socratic thinkers such as Anaximander, Xenophanes, Anaximenes, and Heraclides speculated about the nature and substance of the stars and planets. Astronomers such as Eudoxus (contemporary with Plato) observed planetary motions and cycles, and created a geocentric cosmological model that would be accepted by Aristotle. This model generally lasted until Ptolemy, who added epicycles to explain the retrograde motion of Mars. (Around 250 BC, Aristarchus of Samos postulated a proto-heliocentric theory, which would not be reconsidered for nearly two millennia (Copernicus), as Aristotle's geocentric model continued to be favored.) The Platonic school promoted the study of astronomy as a part of philosophy because the motions of the heavens demonstrate an orderly and harmonious cosmos. In the third century BC, Babylonian astrology began to make its presence felt in Greece. Astrology was criticized by Hellenistic philosophers such as the Academic Skeptic Carneades and Middle Stoic Panaetius. However, the notions of the Great Year (when all the planets complete a full cycle and return to their relative positions) and eternal recurrence were Stoic doctrines that made divination and fatalism possible.\nIn the Hellenistic world, the Greek words 'astrologia' and 'astronomia' were often used interchangeably, but they were conceptually not the same. Plato taught about 'astronomia'  and stipulated that planetary phenomena should be described by a geometrical model. The first solution was proposed by Eudoxus. Aristotle favored a physical approach and adopted the word 'astrologia'. Eccentrics and epicycles came to be thought of as useful fictions. For a more general public, the distinguishing principle was not evident and either word was acceptable. For the Babylonian horoscopic practice, the words specifically used were 'apotelesma' and 'katarche', but otherwise it was subsumed under the aristotelian term 'astrologia'.\nIn his compilatory work Etymologiae, Isidore of Seville noted explicitly the difference between the terms astronomy and astrology (Etymologiae, III, xxvii) and the same distinction appeared late",
    "source": "wikipedia",
    "title": "Astrology and astronomy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_21671434",
    "text": "Barlow's law is an incorrect physical law proposed by Peter Barlow in 1825 to describe the ability of wires to conduct electricity. It says that the strength of the effect of electricity passing through a wire varies inversely with the square root of its length and directly with the square root of its cross-sectional area, or, in modern terminology:\n\n  \n    \n      \n        I\n        ∝\n        \n          \n            \n              A\n              L\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle I\\propto {\\sqrt {\\frac {A}{L}}},}\n  \n\nwhere I is electric current, A is the cross-sectional area of the wire, and L is the length of the wire. Barlow formulated his law in terms of the diameter d of a cylindrical wire.  Since A is proportional to the square of d the law becomes \n  \n    \n      \n        I\n        ∝\n        d\n        \n          /\n        \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle I\\propto d/{\\sqrt {L}}}\n  \n for cylindrical wires.\nBarlow undertook his experiments with the aim of determining whether long-distance telegraphy was feasible and believed that he proved that it was not. The publication of Barlow's law delayed research into telegraphy for several years, until 1831 when Joseph Henry and Philip Ten Eyck constructed a circuit 1,060 feet long, which used a large battery to activate an electromagnet. Barlow did not investigate the dependence of the current strength on electric tension (that is, voltage). He endeavoured to keep this constant, but admitted there was some variation. Barlow was not entirely certain that he had found the correct law, writing \"the discrepancies are rather too great to enable us to say, with confidence, that such is the law in question.\"\nIn 1827, Georg Ohm published a different law, in which current varies inversely with the wire's length, not its square root; that is,\n\n  \n    \n      \n        I\n        ∝\n        \n          \n            1\n            \n              c\n              +\n              L\n              \n                /\n              \n              A\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle I\\propto {\\frac {1}{c+L/A}},}\n  \n\nwhere \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n is a constant dependent on the circuit setup. Ohm's law is now considered the correct law, and Barlow's false.\nThe law Barlow proposed was not in error due to poor measurement; in fact, it fits Barlow's careful measurements quite well.  Heinrich Lenz pointed out that Ohm took into account \"all the conducting resistances … of the circuit\", whereas Barlow did not. Ohm explicitly included a term for what we would now call the internal resistance of the battery. Barlow did not have this term and approximated the results with a power law instead. Ohm's law in modern usage is rarely stated with this explicit term, but nevertheless an awareness of it is necessary for a full understanding of the current in a circuit.\n\n\n==",
    "source": "wikipedia",
    "title": "Barlow's law",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_64310512",
    "text": "Bharat Ki Chhap (Identity of India) is a 13-episode Indian TV science documentary series chronicling the history of science and technology in India from pre-historic times until the present. It was directed by filmmaker Chandita Mukherjee and funded by the Department of Science and Technology's National Council for Science and Technology Communication (NCSTC) in 1987. It was telecasted on Doordarshan every Sunday Morning. It was introduced by Professor Yash Pal.\nIt projected in a pragmatic way alternative viewpoints on the subject of science as pioneered in India, in contrast with western scientific endeavours. It drew support from People's Science Movement.\nA companion book was later published by Comet Project titled Bhārat Ki Chhāp: A Companion Book to the Film Series by Chayanika Shah, Suhas Paranjape, Swatija Manorama.\n\nEpisodes\nA total of 13 episodes were released.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Bharat Ki Chhap",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_777174",
    "text": "Big science is a term used by scientists and historians of science to describe a series of changes in science which occurred in industrial nations during and after World War II, as scientific progress increasingly came to rely on large-scale projects usually funded by national governments or groups of governments. Individual or small group efforts, or small science, are still relevant today as theoretical results by individual authors may have a significant impact, but very often the empirical verification requires experiments using constructions, such as the Large Hadron Collider, costing between $5 and $10 billion.\n\nDevelopment\nWhile science and technology have always been important to and driven by warfare, the increase in military funding of science following the second World War was on a scale wholly unprecedented.  James Conant, in a 1941 letter to Chemical Engineering News, said that World War II \"is a physicist's war rather than a chemist's,\" a phrase that was cemented in the vernacular in post-war discussion of the role that those scientists played in the development of new weapons and tools, notably the proximity fuse, radar, and the atomic bomb.  The bulk of these last two activities took place in a new form of research facility: the government-sponsored laboratory, employing thousands of technicians and scientists, managed by universities (in this case, the University of California and the Massachusetts Institute of Technology).\nThe need of a strong scientific research establishment was obvious in the shadow of the first atomic weapons to any country seeking to play a prominent role in world affairs. After the success of the Manhattan Project, governments became the chief patron of science, and the character of the scientific establishment underwent several key changes. This was especially marked in the United States and the Soviet Union during the Cold War, but also to a lesser extent in many other countries.\nDefinitions\n\"Big science\" usually implies one or more of these specific characteristics:\n\nBig budgets: No longer required to rely on philanthropy or industry, scientists were able to use budgets on an unprecedented scale for basic research.\nBig staffs: Similarly, the number of practitioners of science on any one project grew as well, creating difficulty, and often controversy, in the assignment of credit for scientific discoveries (the Nobel Prize system, for example, allows awarding only three individuals in any one topic per year, based on a 19th-century model of the scientific enterprise).\nBig machines: Ernest Lawrence's cyclotron at his Radiation Laboratory in particular ushered in an era of massive machines (requiring massive staffs and budgets) as the tools of basic scientific research. The use of many machines, such as the many sequencers used during the Human Genome Project, might also fall under this definition.\n\nBig laboratories: Because of the increase in cost to do basic science (with the increase of large machines),",
    "source": "wikipedia",
    "title": "Big science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_31468613",
    "text": "The Book of Nature (Lat. liber naturae/liber mundi, Ar. kitāb takwīnī) is a religious and philosophical cosmological metaphor known from Antiquity in various cultures, and prominent in the Latin and Romance literature of the European Middle Ages. The idea of a cosmos formed by letters is already found in the fragments of Heraclitus, where it relates to the Greek concept of logos, in Plato’s Timaeus, and in Lucretius’ De rerum natura.\nThe metaphor of the Book of Nature straddles the divide between religion and science, viewing nature as a readable text open to knowledge and understanding. Early theologians, such as St. Paul, believed the Book of Nature was a source of God's revelation to humankind. He believed that when read alongside sacred scripture, the \"book\" and the study of God's creations would lead to a knowledge of God himself. This type of revelation is often referred to as a general revelation. The concept corresponds to the early Greek philosophical concept of logos, which implies that humans, as part of a coherent universe, are capable of understanding the design of the natural world through reason. The phrase liber naturae was famously used by Galileo when writing about how \"the book of nature [can become] readable and comprehensible\".\n\nHistory\nFrom the earliest times in known civilizations, events in the natural world were expressed through a collection of stories concerning everyday life. In ancient times, it was believed that the visible, mortal world existed alongside an upper world of spirits and gods acting through nature to create a unified and intersecting moral and natural cosmos. Humans, living in a world that was acted upon by free-acting and conspiring gods of nature, attempted to understand their world and the actions of the divine by observing and correctly interpreting natural phenomena, such as the motion and position of stars and planets. Efforts to analyze and understand divine intentions led mortals to believe that intervention and influence over godly acts were possible—either through religious persuasions, such as prayer and gifts, or through magic, which depended on sorcery and the manipulation of nature to bend the will of the gods. Humans believed they could discover divine intentions through observing or manipulating the natural world. Thus, mankind had a reason to learn more about nature.\nAround the sixth century BCE, humanity’s relationship with the deities and nature began to change. Greek philosophers, such as Thales of Miletus, no longer viewed natural phenomena as the result of omnipotent gods. Instead, natural forces resided within nature, an integral part of a created world, and appeared under certain conditions that had little to do with personal deities. The Greeks believed that natural phenomena occurred by \"necessity\" through intersecting chains of \"cause\" and \"effect\". Greek philosophers, however, lacked a new vocabulary to express such abstract concepts as \"necessity\" or \"cause\" and consequently",
    "source": "wikipedia",
    "title": "Book of Nature",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_7802702",
    "text": "The Bridgewater Treatises (1833–36) are a series of eight works that were written by leading scientific figures appointed by the President of the Royal Society in fulfilment of a bequest of £8000, made by Francis Henry Egerton, 8th Earl of Bridgewater, for work on \"the Power, Wisdom, and Goodness of God, as manifested in the Creation.\"\nDespite being voluminous and costly, the series was very widely read and discussed, becoming one of the most important contributions to Victorian literature on the relationship between religion and science.  They made such an impact that Charles Darwin began On the Origin of Species with a quotation from the Bridgewater Treatise of William Whewell.\n\nThe Bridgewater Bequest\nBefore unexpectedly becoming the 8th Earl of Bridgewater in 1823, Francis Henry Egerton spent most of his life as an absentee person. He published works of classical scholarship and issued others praising the historical achievements of his family, including those of his father's cousin, Francis Egerton, 3rd Duke of Bridgewater, the \"father of British inland navigation.\"  In 1781, he was elected a Fellow of the Royal Society; after 1802 he lived mostly in Paris, where he amassed a collection of manuscripts later donated to the British Museum and gained a reputation as an eccentric. He died in February 1829, leaving a will dated 25 February 1825, in which he directed that £8000 was to be used by the President of the Royal Society to appoint a \"person or persons\":...to write, print, and publish, one thousand copies of a work On the Power, Wisdom, and Goodness of God, as manifested in the Creation; illustrating such work by all reasonable arguments, as, for instance, the variety and formation of God's creatures in the animal, vegetable, and mineral kingdoms; the effect of digestion, and thereby of conversion; the construction of the hand of man, and an infinite variety of other arguments: as also by discoveries, ancient and modern, in arts, sciences, and the whole extent of literature.The President of the Royal Society at the time was Davies Gilbert, who sought the assistance of the Archbishop of Canterbury, William Howley, and the Bishop of London, Charles James Blomfield, in selecting authors. Those appointed, with the titles and dates of their treatises, were:\nThe Adaptation of External Nature to the Moral and Intellectual Condition of Man (1833), by Thomas Chalmers, D.D.\nOn The Adaptation of External Nature to the Physical Condition of Man (1833), by John Kidd, M.D.\nAstronomy and General Physics Considered with Reference to Natural Theology (1833), by William Whewell, D.D.\nThe Hand, its Mechanism and Vital Endowments as Evincing Design (1833), by Sir Charles Bell.\nAnimal and Vegetable Physiology Considered with Reference to Natural Theology (1834), by Peter Mark Roget.\nGeology and Mineralogy Considered with Reference to Natural Theology (1836), by William Buckland, D.D.\nOn the History, Habits and Instincts of Animals (1835), by William Kirby.\nChe",
    "source": "wikipedia",
    "title": "Bridgewater Treatises",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_75954090",
    "text": "Charles Babbage's Saturday night soirées were gatherings held by the mathematician and inventor Charles Babbage at his home in Dorset Street, Marylebone, London from 1828 and into the 1840s. The soirées were attended by the cultural elite of the time.\n\nScientific soirées\nBabbage left England when his wife and father died in 1827. On his return in 1828, now in possession of a considerable inheritance, he began to host Saturday evening parties. The science historian James A. Secord describes the parties as \"scientific soirées\". Secord writes that Babbage imported the idea from France, and once established, such soirées \"became one of the chief ways in which scientific discussion could take place on a more sustained basis within polite society.\"\nIn her autobiography, the English writer and sociologist Harriet Martineau wrote: \"All were eager to go to his glorious soirées and I always thought he appeared to great advantage as a host. His patience in explaining his machine in those days was really exemplary.\"\nAccording to biographers Bruce Collier and James H. MacLachlan, \"Babbage was a bon vivant with a love of dining out and socialising. He sparkled as a host and raconteur. His Saturday soirées were glittering events attended by the social and intellectual elite of London.\"\nGuests\nHundreds of prominent people attended the soirées, including Ada Lovelace, Lady Byron, Arthur Wellesley, 1st Duke of Wellington, Charles Darwin and Emma Darwin, Charles Dickens, Michael Faraday, Sophia Elizabeth De Morgan, Mary Somerville, Harriet Martineau, photographic inventor Henry Fox Talbot, the actor William Macready, the composer Felix Mendelssohn, the historian Thomas Babington Macaulay, telegraph inventor Charles Wheatstone, the French philosopher Alexis de Tocqueville, geologist Charles Lyell and his wife Mary Lyell, Mary's sister Frances, the Belgian ambassador Sylvain Van de Weyer, electrical inventor Andrew Crosse and many others. According to C. R. Keeler, up to 200-300 people might attend one evening event.\n",
    "source": "wikipedia",
    "title": "Charles Babbage's Saturday night soirées",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_31706469",
    "text": "This list of chemical elements named after people includes elements named for people both directly and indirectly. Of the 118 elements, 19 are connected with the names of 20 people. 15 elements were named to honor 16 scientists (as curium honours both Marie and Pierre Curie). Four others have indirect connection to the names of non-scientists. Only gadolinium and samarium occur in nature; the rest are man-made.\n\nList\nThese 19 elements are connected to the names of people. Seaborg and Oganessian were the only living persons honored by having elements named after them; Oganessian is the only one still alive. Names were proposed to honor Einstein and Fermi while they were still alive, but they had both died by the time those names became official.\nThe four elements associated with non-scientists were not named in their honor but named for something else bearing their name: samarium for the mineral samarskite from which it was isolated; and americium, berkelium and livermorium after places named for them. The cities of Berkeley, California and Livermore, California are the locations of the University of California Radiation Laboratory and Lawrence Livermore National Laboratory, respectively.\nOther connections\nOther element names connected with people (real or mythological) have been proposed but failed to gain official international recognition. The following such names received past significant use among scientists:\n\ncassiopeium after the constellation Cassiopeia, hence indirectly connected to the mythological Cassiopeia (now lutetium);\ncolumbium after Christopher Columbus (now niobium);\nhahnium after Otto Hahn (now dubnium, also later proposed for what is now hassium);\njoliotium after Irène Joliot-Curie (now nobelium, also later proposed for what is now dubnium);\nkurchatovium after Igor Kurchatov (now rutherfordium);\nNames had also been suggested (but not used) to honour Henri Becquerel (becquerelium) and Paul Langevin (langevinium). George Gamow, Lev Landau, and Vitalii Goldanski (who was alive at the time) were suggested for consideration for honoring with elements during the Transfermium Wars, but were not actually proposed.\n(See the article on element naming controversies and List of chemical elements named after places.)\nAlso, mythological entities have had a significant impact on the naming of elements. Helium, titanium, selenium, palladium, promethium, cerium, europium, tantalum, mercury, thorium, uranium, neptunium and plutonium are all given names connected to mythological characters. With some, that connection is indirect:\n\nhelium: named for the Sun where it was discovered by spectral analysis, being associated with the deity Helios,\niridium: named for the Greek goddess Iris,\ntellurium: named for the Roman goddess of the earth, Tellus Mater,\nniobium: named for Niobe, a character of Greek mythology,\nvanadium: named for Vanadis, another name for Norse goddess Freyja,\nselenium: named for the Moon being associated with the deity Selene,\npalla",
    "source": "wikipedia",
    "title": "List of chemical elements named after people",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1416046",
    "text": "The history of chemistry represents a time span from ancient history to the present. By 1000 BC, civilizations used technologies that would eventually form the basis of the various branches of chemistry. Examples include the discovery of fire, extracting metals from ores, making pottery and glazes, fermenting beer and wine, extracting chemicals from plants for medicine and perfume, rendering fat into soap, making glass,\nand making alloys like bronze.\nThe protoscience of chemistry, and alchemy, was unsuccessful in explaining the nature of matter and its transformations. However, by performing experiments and recording the results, alchemists set the stage for modern chemistry. The history of chemistry is intertwined with the history of thermodynamics, especially through the work of Willard Gibbs.\n\nAncient history\n\nMedieval alchemy\nThe elemental system used in medieval alchemy was developed primarily by Jābir ibn Hayyān and was rooted in the classical elements of Greek tradition. His system consisted of the four Aristotelian elements of air, earth, fire, and water in addition to two philosophical elements: sulphur, characterizing the principle of combustibility, \"the stone which burns\"; and mercury, characterizing the principle of metallic properties. They were seen by early alchemists as idealized expressions of irreducible components of the universe and are of larger consideration within philosophical alchemy.\nThe three metallic principles (sulphur to flammability or combustion, mercury to volatility and stability, and salt to solidity) became the tria prima of the Swiss alchemist Paracelsus. He reasoned that Aristotle's four-element theory appeared in bodies as three principles. Paracelsus saw these principles as fundamental and justified them by recourse to the description of how wood burns in fire. Mercury included the cohesive principle, so that when it left the wood (in smoke) the wood fell apart. Smoke described the volatility (the mercurial principle), the heat-giving flames described flammability (sulphur), and the remnant ash described solidity (salt).\n",
    "source": "wikipedia",
    "title": "History of chemistry",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_42657576",
    "text": "Most scientific and technical innovations prior to the Scientific Revolution were achieved by societies organized by religious traditions. Ancient Christian scholars pioneered individual elements of the scientific method. Historically, Christianity has been and still is a patron of sciences. It has been prolific in the foundation of schools, universities and hospitals, and many Christian clergy have been active in the sciences and have made significant contributions to the development of science.\nHistorians of science such as Pierre Duhem credit medieval Catholic mathematicians and philosophers such as John Buridan, Nicole Oresme and Roger Bacon as the founders of modern science. Duhem concluded that \"the mechanics and physics of which modern times are justifiably proud to proceed, by an uninterrupted series of scarcely perceptible improvements, from doctrines professed in the heart of the medieval schools\". Many of the most distinguished classical scholars in the Byzantine Empire held high office in the Eastern Orthodox Church. Protestantism has had an important influence on science, according to the Merton Thesis, there was a positive correlation between the rise of English Puritanism and German Pietism on the one hand, and early experimental science on the other.\nChristian scholars and scientists have made noted contributions to science and technology fields, as well as medicine, both historically and in modern times. Some scholars state that Christianity contributed to the rise of the Scientific Revolution. Between 1901 and 2001, about 56.5% of Nobel prize laureates in scientific fields were Christians, and 26% were of Jewish descent (including Jewish atheists).\nEvents in Christian Europe, such as the Galileo affair, that were associated with the Scientific Revolution and the Age of Enlightenment led some scholars such as John William Draper to postulate a conflict thesis, holding that religion and science have been in conflict throughout history. While the conflict thesis remains popular in atheistic and antireligious circles, it has lost favor among most contemporary historians of science. Most contemporary historians of science believe the Galileo affair is an exception in the overall relationship between science and Christianity and have also corrected numerous false interpretations of this event.\n\nOverview\nMost sources of knowledge available to the early Christians were connected to pagan worldviews as the early Christians largely lived among pagans. There were various opinions on how Christianity should regard pagan learning, which included its ideas about nature. For instance, among early Christian teachers, from Tertullian (c. 160–220) held a generally negative opinion of Greek philosophy, while Origen (c. 185–254) regarded it much more favourably and required his students to read nearly every work available to them.\nEarlier attempts at reconciliation of Christianity with Newtonian mechanics appear quite different from later attempts ",
    "source": "wikipedia",
    "title": "Christianity and science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1694427",
    "text": "Science in classical antiquity encompasses inquiries into the workings of the world or universe aimed at both practical goals (e.g., establishing a reliable calendar or determining how to cure a variety of illnesses) as well as more abstract investigations belonging to natural philosophy. Classical antiquity is traditionally defined as the period between the 8th century BC (beginning of Archaic Greece) and the 6th century AD (after which there was medieval science). It is typically limited geographically to the Greco-Roman West, Mediterranean basin, and Ancient Near East, thus excluding traditions of science in the ancient world in regions such as China and the Indian subcontinent.\nIdeas regarding nature that were theorized during classical antiquity were not limited to science but included myths as well as religion. Those who are now considered as the first scientists may have thought of themselves as natural philosophers, as practitioners of a skilled profession (e.g., physicians), or as followers of a religious tradition (e.g., temple healers). Some of the more widely known figures active in this period include Hippocrates, Aristotle, Euclid, Archimedes, Hipparchus, Galen, and Ptolemy. Their contributions and commentaries spread throughout the Eastern, Islamic, and Latin worlds and contributed to the birth of modern science. Their works covered many different categories including mathematics, cosmology, medicine, and physics.\n\nClassical Greece\n\nHellenistic age\nThe military campaigns of Alexander the Great spread Greek thought to Egypt, Asia Minor, Persia, up to the Indus River. The resulting migration of many Greek speaking populations across these territories provided the impetus for the foundation of several seats of learning, such as those in Alexandria, Antioch, and Pergamum. \nHellenistic science differed from Greek science in at least two respects: first, it benefited from the cross-fertilization of Greek ideas with those that had developed in other non-Hellenic civilizations; secondly, to some extent, it was supported by royal patrons in the kingdoms founded by Alexander's successors. The city of Alexandria, in particular, became a major center of scientific research in the 3rd century BC. Two institutions established there during the reigns of Ptolemy I Soter (367–282 BC) and Ptolemy II Philadelphus (309–246 BC) were the Library and the Museum. Unlike Plato's Academy and Aristotle's Lyceum, these institutions were officially supported by the Ptolemies, although the extent of patronage could be precarious depending on the policies of the current ruler.\nHellenistic scholars often employed the principles developed in earlier Greek thought in their scientific investigations, such as the application of mathematics to phenomena or the deliberate collection of empirical data. The assessment of Hellenistic science, however, varies widely. At one extreme is the view of English classical scholar Cornford, who believed that \"all the most important",
    "source": "wikipedia",
    "title": "Science in classical antiquity",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_64433437",
    "text": "A conversazione is a \"social gathering [predominantly] held by [a] learned or art society\" for conversation and discussion, especially about the arts, literature, medicine, and science.\n\nIt would not be easy to devise a happier way [than the conversazione] of bringing novelties at once under practical criticism—of making the outliers of science acquainted with the centre, of enabling investigators to compare operations and discuss facts and speculations, and of giving occasion for renewal of intercourse and removal of misunderstandings. …[The] tangible gain to science [from the conversazione is that] inventors and experimentalists … hear [directly] what contemporaries say of their schemes and experiments, and much can be said and done with advantage amid the free talk of a general gathering which could not be permitted in the formal meeting of a scientific society. (Nature, 5 May 1870.)\n\nOrigin\nThe writer Horace Walpole is credited with the first recorded English use of conversazione in a letter written (from Italy) on 11 November 1739 to Richard West (1716–1742) in which he writes, \"After the play we were introduced to the assembly, which they [viz., the Italians] call the conversazione\".\nHistorical usage in Britain\nIn Italy, the term generally refers to a gathering for conversation; and was first used in English to identify the sort of private social gathering more generally known today as an \"At Home\".\nIn England, however, it soon came to be far more widely used to denote the gatherings of a far more intellectual character and was applied in the more specific sense of a scientific, artistic, or literary assembly/soirée, generally held at night.\n\nA conversazione like everything else has undergone conspicuous development in these days.Formerly the word was applicable only to a meeting of cognoscenti, who were themselves proficient in some art or science which might be the immediate subject of learned interest.At the present time the materials for discussion are supplied by the proficients, and the general public are invited to provide the talk or the criticism.Moreover a \"conversation\" of this kind is not limited to a specific subject, but may comprise topics incidental to any branch of science and art whatever. (New Zealand Herald, 17 September 1880.)\nIn its report on the first conversazione ever conducted by the Lambeth Literary Institution (on 22 June 1836), The Gentleman's Magazine noted that,\n\nthe principal object [of the Lambeth Literary Institution's inaugural conversazione] has been—by the collection of articles of virtù, antiquity, science, or art, and by the reading of original papers, conversation, and music,— to unite its members, at stated periods, into one focus of neighbourly community; where all may be on a footing of social equality,—the aristocracy of mind, united with urbanity of manners, alone maintaining its ascendancy here; where the high attainments of the classical scholar,—the lofty imaginings of the poet,—the deep resea",
    "source": "wikipedia",
    "title": "Conversazione",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_75707236",
    "text": "Darwin's Ghosts: The Secret History of Evolution is a nonfiction history of science book by British author Rebecca Stott. It was published in the United States in 2012 by Spiegel & Grau, the international version is subtitled differently: Darwin's Ghosts: In Search of the First Evolutionists. It is written in 12 distinct chapters that highlight persons that contributed to the pre-history of evolution by natural selection, published by Charles Darwin in On the Origin of Species in 1859. The book contains biographical sketches of 12 persons spanning from 344 BC to 19th century contemporaries of Darwin.\nThe book has received reviews from notable reviewers and was included on the New York Times Book Review 100 Notable Books 2012 list.\n\nSynopsis\nTho focus of each chapter is summarized in the table below.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Darwin's Ghosts: The Secret History of Evolution",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_28712146",
    "text": "The Department of History and Philosophy of Science (HPS), of the University of Cambridge is the largest department of history and philosophy of science in the United Kingdom. A majority of its submissions received maximum ratings of 4* and 3* in the 2014 REF (Research Excellence Framework). Located in the historic buildings of the Old Physical Chemistry Laboratories on Free School Lane, Cambridge, the department teaches undergraduate courses towards the Cambridge Tripos and graduate courses including a taught Masters and PhD supervision in the field of HPS.  The department shares its premises with the Whipple Museum and Whipple Library which provide important resources for its teaching and research.\n\nAcademic staff\nThe Department of HPS at Cambridge employs fifteen full-time teaching staff, approximately thirty research staff, numerous supervisors and research associates from departments and colleges across the University of Cambridge, in addition to external supervisors and examiners. A long-standing head of department was the noted Professor Peter Lipton, who served until his unexpected death in 2007. He was followed as head of department by the late Professor John Forrester, an international authority in the History of Mind, and a leading figure on Sigmund Freud and the history of psychoanalysis. Professor Jim Secord became head of the department in 2013 and was succeeded in 2016 by Professor Liba Taub. The current head is Professor Hasok Chang. Other senior   staff include Professor Tim Lewens, Professor Lauren Kassell, Professor Nick Hopwood and retired Professor Simon Schaffer.\nDegree courses\nThe department offers a nine-month MPhil course in history, philosophy and sociology of science, medicine and technology. It also supervises graduate students for the Cambridge PhD in HPS and provides advisors in the related fields of research in history, philosophy and social science.  Together with the Departments of Sociology and Social Anthropology, it also sponsors a nine-month MPhil in health, medicine and society.\nUndergraduate teaching and supervision is provided for students who have completed their first year at Cambridge.  Due to the interdisciplinary nature of the Cambridge Tripos system, undergraduates from a wide range of fields may study HPS, although entry is predominantly through the Natural Sciences Tripos. The resources of the Whipple Museum provide for first-hand study of scientific instruments which often provide topics for student dissertations.\n",
    "source": "wikipedia",
    "title": "Department of History and Philosophy of Science, University of Cambridge",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_29647899",
    "text": "The Dibner Institute for the History of Science and Technology (1992–2006) was a research institute established at MIT, and housed in a renovated building (E56) on campus at 38 Memorial Drive, overlooking the Charles River.\n\nDescription\nAt the heart of the Institute was the Burndy Library on the ground floor, initially containing 37,000 volumes on the history of science and technology  collected by the Dibner Fund.  The Library also possessed a large collection of antique scientific instruments, such as astrolabes, telescopes, microscopes, early spectrometers, and a Wimshurst machine, which were on public display in a dedicated gallery outside the library.  Also on display was a large collection of antique incandescent light bulbs, gas discharge tubes, electronic vacuum tubes, and other early examples of electrical and electronic technology.  The Library would mount occasional special exhibits, such as The Afterlife of Immortality: Obelisks Outside Egypt.\nThe building was a modest Art Deco structure, fronting on Memorial Drive and the Charles River. Above the Library and display space, on the second and third floor were offices and lecture and seminar rooms.  The Institute held regular lectures, seminars, study programs, and an annual symposium in the history of science and technology.  Over the period of its existence, the Institute supported over 340 short- and longer-term fellowships.\nHistory and development\nThe Institute was named in honor of Bern Dibner (1897–1988), who had conceived of it before his death. The Institute was developed and supported by the Dibner Fund he had established in 1957, directed by his son David Dibner. The institute, from its inception, was run by executive director Evelyn Simha. On the academic side, the Institute was supported by a consortium of MIT, Boston University, Brandeis University and Harvard University.\nIn 1995, the 600-volume Babson Collection of historical material related to Isaac Newton was placed on permanent deposit with the Burndy Library.  The collection had been assembled by Roger Babson, founder of Babson College in Wellesley, Massachusetts, and was previously housed at the College. In 1999, the addition of the 7,000-volume Volterra Collection from Italy increased the Burndy Library collection by more than a third.\nIn 2004 MIT decided not to renew its affiliation, and the Dibner family began looking for a new location to house the collection.  David Dibner died unexpectedly in 2005.   The Dibner Institute closed in 2006, and the Burndy Library and associated collections were transferred to The Huntington Library in San Marino, California, which now offers a Dibner History of Science Program to fund fellowships, a lecture series and annual conference. The acquisition of the Burndy Library (by then numbering 67,000 volumes) transformed the Huntington Library's collections in the history of science and technology into one of the world's largest in that field.\nThe Huntington houses a permanent exhib",
    "source": "wikipedia",
    "title": "Dibner Institute for the History of Science and Technology",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_38596702",
    "text": "Below are discoveries in science that involve chance circumstances in a particularly salient way. This page should not list all chance involved in all discoveries (i.e. it should focus on discoveries reported for their notable circumstances).\n\nOverview\nRoyston Roberts says that various discoveries required a degree of genius, but also some lucky element for that genius to act on. Richard Gaughan writes that accidental discoveries result from the convergence of preparation, opportunity, and desire.\nMajor everyday discoveries that were helped by luck in some way include products like vulcanized rubber, teflon, nylon, penicillin, cyanoacrylate (Super Glue), the implantable pacemaker, the microwave oven, Scotchgard, Saran wrap, Silly Putty, Slinky, safety glass, propeller, snowmaking, stainless steel, Perkin's mauve, and popsicles. Most artificial sweeteners have been discovered when accidentally tasted, including aspartame and saccharin.\nIdeas include the theory of the Big Bang, tissue culture, radio astronomy, and the discovery of DNA.\nSuch archeological discoveries as the Rosetta Stone, the Dead Sea Scrolls and the ruins of Pompeii also emerged partly out of serendipity.\nMany relevant and well known scientific theories were developed by chance at some degree along history. According to a legend, Archimedes realized his principle on hydrostatics when he entered in a bath full of water, which overflows (he then shouted out his famous \"Eureka!\"). And the unexpected, negative results of the Michelson–Morley experiment in their search of the luminiferous aether ultimately led to the special theory of relativity by Albert Einstein.\nThe optical illusion called the \"flashed face distortion effect\" suggests a new area of research in the neurology of face perception.\nDetailed examples\n\n",
    "source": "wikipedia",
    "title": "List of discoveries influenced by chance circumstances",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_33809974",
    "text": "The discovery of human antiquity was a major achievement of science in the middle of the 19th century, and the foundation of scientific paleoanthropology. The antiquity of man, human antiquity, or in simpler language the age of the human race, are names given to the series of scientific debates it involved, which with modifications continue in the 21st century. These debates have clarified and given scientific evidence, from a number of disciplines, towards solving the basic question of dating the first human being.\nControversy was very active in this area in parts of the 19th century, with some dormant periods also. A key date was the 1859 re-evaluation of archaeological evidence that had been published 12 years earlier by Boucher de Perthes. It was then widely accepted, as validating the suggestion that man was much older than had previously been believed, for example than the 6,000 years implied by some traditional chronologies.\nIn 1863 T. H. Huxley argued that man was an evolved species; and in 1864 Alfred Russel Wallace combined natural selection with the issue of antiquity. The arguments from science for what was then called the \"great antiquity of man\" became convincing to most scientists, over the following decade. The separate debate on the antiquity of man had in effect merged into the larger one on evolution, being simply a chronological aspect. It has not ended as a discussion, however, since the current science of human antiquity is still in flux.\n\nContemporary formulations\nModern science has no single answer to the question of how old humanity is. What the question now means indeed depends on choosing genus or species in the required answer. It is thought that the genus of man has been around for ten times as long as our species. Currently, fresh examples of (extinct) species of the genus Homo are still being discovered, so that definitive answers are not available. The consensus view is that human beings are one species, the only existing species of the genus. With the rejection of polygenism for human origins, it is asserted that this species had a definite and single origin in the past. (That assertion leaves aside the point whether the origin meant is of the current species, however. The multiregional hypothesis allows the origin to be otherwise.) The hypothesis of recent African origin of modern humans is now widely accepted, and states that anatomically modern humans had a single origin, in Africa.\nThe genus Homo is now estimated to be about 2.3 to 2.4 million years old, with the appearance of H. habilis; meaning that the existence of all types of humans has been within the Quaternary.\n\nOnce the question is reformulated as dating the transition of the evolution of H. sapiens from a precursor species, the issue can be refined into two further questions. These are: the analysis and dating of the evolution of Archaic Homo sapiens, and of the evolution from \"archaic\" forms of the species H. sapiens sapiens. The second question is ",
    "source": "wikipedia",
    "title": "Discovery of human antiquity",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_480936",
    "text": "The doctrine of signatures, also known as the doctrine of correspondences, is a biomedicinal theory of pseudoscience. It states that herbs or animals have physical or behavioral traits that mirror the ailment it can successfully treat. Theological justifications, such as that of botanist William Cole, were that God would want to show men what plants would be useful for. The doctrine of signatures has a debated origin. Many historians believe it begins with primitive thinking methods, while other historians believe it originated with Dioscorides and was popularized in the 16th and 17th centuries after Jakob Böhme coined the doctrine of signatures in his book The Signature of All Things. \nThis theory is a possible explanation for the ancient discovery of medicinal properties; however, there is no definitive proof as to whether the medicinal property or the connection in physical/behavioral traits was realized first. The theory later became a scientific basis for trying new remedies solely based upon their qualities in an attempt to find new medicines. While there are some homeopathic remedies that are still used today which have been connected to this theory, there are also remedies from this theory which have been found harmful. For instance, birthwort (so-called because of its resemblance to the uterus) was once used widely for pregnancies, but is carcinogenic and very damaging to the kidneys, owing to its aristolochic acid content. As a defense against predation, many plants contain toxic chemicals, the action of which is not immediately apparent or easily tied to the plant rather than other factors.\n\nHistory\nThe origins of the doctrine of signatures are debated by historians. The concept of the doctrine of signatures dates back to Hippocratic medicine and the belief that \"cures for human ills were divinely revealed in nature, often through plants.\" The concept would be further developed by Dioscorides. Dioscorides would provide ample descriptions of plant medications through various drawings, detailing the importance of their look, name, shelf life, how to tell when plants have gone bad, and how to properly harvest the crop for medical use. Paracelsus (1493–1541) developed the concept further, writing that \"nature marks each growth ... according to its curative benefit\", and it was further developed by Giambattista della Porta in his Phytognomonica (1588).\nThe writings of Jakob Böhme (1575–1624) coined the term \"doctrine of signatures\" within his book The Signature of All Things (or Signatura Rerum), published in 1621. He suggested that God marked objects with a sign, or \"signature\", for their purpose, specifically that \"to that Signature, his inward form is noted in the form of his face; and thus also is a beast, an herb, and the trees; every thing as it is inwardly [in its innate virtue and quality] so it is outwardly signed\". Plants bearing parts that resembled human body parts, animals, or other objects were thought to have useful relevance",
    "source": "wikipedia",
    "title": "Doctrine of signatures",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_8586",
    "text": "A Dyson sphere is a hypothetical megastructure that encompasses a star and captures a large percentage of its power output. The concept is a thought experiment that attempts to imagine how a spacefaring civilization would meet its energy requirements once those requirements exceed what can be generated from the home planet's resources alone. Because only a tiny fraction of a star's energy emissions reaches the surface of any orbiting planet, building structures encircling a star would enable a civilization to harvest far more energy.\nThe earliest modern imagining of such a structure was by Olaf Stapledon in his science fiction novel Star Maker (1937). The same concept was later used by physicist Freeman Dyson in his 1960 satirical paper \"Search for Artificial Stellar Sources of Infrared Radiation\". Dyson speculated that such structures would be the logical consequence of the escalating energy needs of a technological civilization and would be a necessity for its long-term survival. A signature of such spheres detected in astronomical searches would be an indicator of extraterrestrial intelligence.\nSince Dyson's paper, many variant designs involving an artificial structure or series of structures to encompass a star have been proposed in exploratory engineering or described in science fiction, often under the name \"Dyson sphere\". Fictional depictions often describe a solid shell of matter enclosing a star – an arrangement considered by Dyson himself to be impossible.\n\nOrigins\nInspired by the 1937 science fiction novel Star Maker by Olaf Stapledon, the physicist and mathematician Freeman Dyson was the first to formalize the concept of what became known as the \"Dyson sphere\" in his 1960 Science paper \"Search for Artificial Stellar Sources of Infra-Red Radiation\". Dyson theorized that as the energy requirements of an advanced technological civilization increased, there would come a time when it would need to systematically harvest the energy from its local star on a large scale. He speculated that this could be done via a system of structures orbiting the star, designed to intercept and collect its energy. He argued that as the structure would result in the large-scale conversion of starlight into far-infrared radiation, an earth-based search for sources of infrared radiation could identify stars supporting intelligent life.\nDyson did not detail how such a system could be constructed, simply referring to it in the paper as a \"shell\" or \"biosphere\". He later clarified that he did not have in mind a solid structure, saying: \"A solid shell or ring surrounding a star is mechanically impossible. The form of 'biosphere' which I envisaged consists of a loose collection or swarm of objects traveling on independent orbits around the star.\" Such a concept has often been referred to as a Dyson swarm; however, in 2013, Dyson said he had come to regret that the concept had been named after him. In an interview with Robert Wright in 2003, Dyson referred to his pap",
    "source": "wikipedia",
    "title": "Dyson sphere",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_15732918",
    "text": "Julian Huxley used the phrase \"the eclipse of Darwinism\" to describe the state of affairs prior to what he called the \"modern synthesis\". During the \"eclipse\", evolution was widely accepted in scientific circles but relatively few biologists believed that natural selection was its primary mechanism. Historians of science such as Peter J. Bowler have used the same phrase as a label for the period within the history of evolutionary thought from the 1880s to around 1920, when alternatives to natural selection were developed and explored—as many biologists considered natural selection to have been a wrong guess on Charles Darwin's part, or at least to be of relatively minor importance. \nFour major alternatives to natural selection were in play in the 19th century:\n\nTheistic evolution, the belief that God directly guided evolution\nNeo-Lamarckism, the idea that evolution was driven by the inheritance of characteristics acquired during the life of the organism\nOrthogenesis, the belief that organisms were affected by internal forces or laws of development that drove evolution in particular directions\nMutationism, the idea that evolution was largely the product of mutations that created new forms or species in a single step.\nTheistic evolution had largely disappeared from the scientific literature by the end of the 19th century as direct appeals to supernatural causes came to be seen as unscientific. The other alternatives had significant followings well into the 20th century; mainstream biology largely abandoned them only when developments in genetics made them seem increasingly untenable, and when the development of population genetics and the modern synthesis demonstrated the explanatory power of natural selection. Ernst Mayr wrote that as late as 1930 most textbooks still emphasized such non-Darwinian mechanisms.\n\nContext\nEvolution was widely accepted in scientific circles within a few years after the publication of On the Origin of Species, but there was much less acceptance of natural selection as its driving mechanism. Six objections were raised to the theory in the 19th century:\n\nThe fossil record was discontinuous, suggesting gaps in evolution.\nThe physicist Lord Kelvin calculated in 1862 that the Earth would have cooled in 100 million years or less from its formation, too little time for evolution.\nIt was argued that many structures were nonadaptive (functionless), so they could not have evolved under natural selection.\nSome structures seemed to have evolved on a regular pattern, like the eyes of unrelated animals such as the squid and mammals.\nNatural selection was argued not to be creative, while variation was admitted to be mostly not of value.\nThe engineer Fleeming Jenkin correctly noted in 1868, reviewing The Origin of Species, that the blending inheritance favoured by Charles Darwin would oppose the action of natural selection.\nBoth Darwin and his close supporter Thomas Henry Huxley freely admitted, too, that selection might not be the who",
    "source": "wikipedia",
    "title": "The eclipse of Darwinism",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_60880859",
    "text": "The Eddington experiment was an observational test of general relativity, organised by the British astronomers Frank Watson Dyson and Arthur Stanley Eddington in 1919. Observations of the total solar eclipse of 29 May 1919 were carried out by two expeditions, one to the West African island of Príncipe, and the other to the Brazilian town of Sobral. The aim of the expeditions was to measure the gravitational deflection of starlight passing near the Sun. The amount of deflection was predicted by Albert Einstein in a 1911 paper; however, his initial prediction proved inaccurate because it was based on an incomplete theory of general relativity. Einstein improved his prediction after finalizing his theory in 1915 and obtaining the solution to his equations by Karl Schwarzschild. Following the return of the expeditions, the results were presented by Eddington to the Royal Society of London and, after some deliberation, were accepted. Widespread newspaper coverage of the results led to worldwide fame for Einstein and his theories.\n\nBackground\nOne of the first considerations of gravitational deflection of light was published in 1801, when Johann Georg von Soldner pointed out that Newtonian gravity predicts that starlight will be deflected when it passes near a massive object. Initially, in a paper published in 1911, Einstein had incorrectly calculated that the amount of light deflection was the same as the Newtonian value, that is 0.83 seconds of arc for a star that would be just on the limb of the Sun in the absence of gravity.\nIn October 1911, responding to Einstein's encouragement, German astronomer Erwin Freundlich contacted solar eclipse expert Charles D. Perrine in Berlin to inquire as to the suitability of existing solar eclipse photographs to prove Einstein's prediction of light deflection. Perrine, the director of the Argentine National Observatory at Cordoba, had participated in four solar eclipse expeditions while at the Lick Observatory in 1900, 1901, 1905, and 1908. He did not believe existing eclipse photos would be useful. In 1912 Freundlich asked if Perrine would include observation of light deflection as part of the Argentine Observatory's program for the solar eclipse of 10 October 1912 in Brazil. W. W. Campbell, director of the Lick Observatory, loaned Perrine its intramercurial camera lenses. Perrine and the Cordoba team were the only eclipse expedition to construct specialized equipment dedicated to observe light deflection. Unfortunately all the expeditions suffered from torrential rains which prevented any observations. Nevertheless, Perrine was the first astronomer to make a dedicated attempt to observe light deflection to test Einstein's prediction. Eddington had taken part in a British expedition to Brazil to observe the 1912 eclipse but was interested in different measurements. Eddington and Perrine spent several days together in Brazil and may have discussed their observation programs including Einstein's prediction of light ",
    "source": "wikipedia",
    "title": "Eddington experiment",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_10174",
    "text": "In philosophy, empiricism is an epistemological view which holds that true knowledge or justification comes either only or primarily from sensory experience and empirical evidence. It is one of several competing views within epistemology, along with rationalism and skepticism. Empiricists argue that empiricism is a more reliable method of finding the truth than relying purely on logical reasoning, because humans have cognitive biases and limitations which lead to errors of judgement. Empiricism emphasizes the central role of empirical evidence in the formation of ideas, rather than innate ideas or traditions. Empiricists may argue that traditions (or customs) arise due to relations of previous sensory experiences. \nHistorically, empiricism was associated with the \"blank slate\" concept (tabula rasa), according to which the human mind is \"blank\" at birth and develops its thoughts only through later experience. \nEmpiricism in the philosophy of science emphasizes evidence, especially as discovered in experiments. It is a fundamental part of the scientific method that all hypotheses and theories must be tested against observations of the natural world rather than resting solely on a priori reasoning, intuition, or revelation.\nEmpiricism, often used by natural scientists, holds that \"knowledge is based on experience\" and that \"knowledge is tentative and probabilistic, subject to continued revision and falsification\". Empirical research, including experiments and validated measurement tools, guides the scientific method.\n\nEtymology\nThe English term empirical derives from the Ancient Greek word ἐμπειρία, empeiria, which is cognate with and translates to the Latin experientia, from which the words experience and experiment are derived.\nBackground\nA central concept in science and the scientific method is that conclusions must be empirically based on the evidence of the senses. Both natural and social sciences use working hypotheses that are testable by observation and experiment. The term semi-empirical is sometimes used to describe theoretical methods that make use of basic axioms, established scientific laws, and previous experimental results to engage in reasoned model building and theoretical inquiry.\nPhilosophical empiricists hold no knowledge to be properly inferred or deduced unless it is derived from one's sense-based experience. In epistemology (theory of knowledge) empiricism is typically contrasted with rationalism, which holds that knowledge may be derived from reason independently of the senses, and in the philosophy of mind it is often contrasted with innatism, which holds that some knowledge and ideas are already present in the mind at birth. However, many Enlightenment rationalists and empiricists still made concessions to each other. For example, the empiricist John Locke admitted that some knowledge (e.g. knowledge of God's existence) could be arrived at through intuition and reasoning alone. Similarly, Robert Boyle, a prominent advocate ",
    "source": "wikipedia",
    "title": "Empiricism",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_11470059",
    "text": "In the history of physics, the history of energy examines the gradual development of energy as a central scientific concept. Classical mechanics was initially understood through the study of motion and force by thinkers like Galileo Galilei and Isaac Newton, the importance of the concept of energy was made clear in the 19th century with the principles of thermodynamics, particularly the conservation of energy which established that energy cannot be created or destroyed, only transformed. In the 20th century Albert Einstein's mass–energy equivalence expanded this understanding by linking mass and energy, and quantum mechanics introduced quantized energy levels. Today, energy is recognized as a fundamental conserved quantity across all domains of physics, underlying both classical and quantum phenomena.\n\nAntiquity\nThe word energy derives from Greek word \"energeia\" (Greek: ἐνέργεια) meaning actuality, which appears for the first time in the 4th century BCE in various works of Aristotle when discussing potentiality and actuality including Physics, Metaphysics, Nicomachean Ethics and On the Soul.\nKinetic energy\nThe modern concept of kinetic energy emerged from the idea of vis viva (living force), which Gottfried Wilhelm Leibniz defined over the period 1676–1689 as the product of the mass of an object and its velocity squared; he believed that total vis viva was conserved. To account for slowing due to friction, Leibniz claimed that heat consisted of the random motion of the constituent parts of matter — a view described by Francis Bacon in Novum Organon to illustrate inductive reasoning and shared by Isaac Newton, although it would be more than a century until this was generally accepted.\nÉmilie du Châtelet in her book Institutions de Physique (\"Lessons in Physics\"), published in 1740, incorporated the idea of Leibniz with practical observations of Willem 's Gravesande to show that the \"quantity of motion\" of a moving object is proportional to its mass and its velocity squared (not the velocity itself as Newton taught—what was later called momentum).\nDaniel Bernoulli extended the vis viva principle into the Bernoulli's principle for fluids in his book in his work Hydrodynamica of 1738.\nIn 1802 lectures to the Royal Society, Thomas Young was the first to use the term energy to refer to kinetic energy in its modern sense, instead of vis viva. In the 1807 publication of those lectures, he wrote,\n\nThe product of the mass of a body into the square of its velocity may properly be termed its energy.\nGustave-Gaspard Coriolis described \"kinetic energy\" in 1829 in its modern sense,\n",
    "source": "wikipedia",
    "title": "History of energy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_59691369",
    "text": "Epistemic cultures (often used in plural form) is a concept developed in the nineties by anthropologist Karin Knorr Cetina in her book Epistemic Cultures: How the Sciences Make Knowledge. Opposed to a monist vision of scientific activity (according to which, would exist  a unique scientific method), Knorr Cetina defines the concept of epistemic cultures as a diversity of scientific activities according to different scientific fields, not only in methods and tools, but also in types of reasonings, ways to establish evidence, and relationships between theory and empiry. Knorr Cetina's work is seminal in questioning the so-called unity of science.\n\nKnorr Cetina's anthropology\nIn practice, Knorr Cetina compares two contemporary important scientific fields: High energy physics and molecular biology. She worked as an anthropologist within two laboratories, along the line of the laboratory anthropology work by Latour and Woolgar. Her anthropological work is comparative and the two chosen scientific fields are highly mediaticized and easily distinguishable.\nEpistemic cultures as a philosophical concept has been perused by numerous philosophical, anthropological or historical studies of science.\nTwo distinct publication regimes\nHigh energy physics and molecular biology are very different as scientific fields belonging to two different epistemic cultures. They also are very different in terms of academic authorship. Biagioli describes this difference in terms of publications culture regarding number of authors per paper, distribution of contributorship within authors, preprint policy and he precisely chooses to oppose the very same domains.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Epistemic cultures",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_31642145",
    "text": "The era of European and American voyages of scientific exploration followed the Age of Discovery and were inspired by a new confidence in science and reason that arose in the Age of Enlightenment. Maritime expeditions in the Age of Discovery were a means of expanding colonial empires, establishing new trade routes and extending diplomatic and trade relations to new territories, but with the Enlightenment scientific curiosity became a new motive for exploration to add to the commercial and political ambitions of the past. See also List of Arctic expeditions and List of Antarctic expeditions.\n\nMaritime exploration in the Age of Discovery\nFrom the early 15th century to the early 17th century the Age of Discovery had, through Portuguese seafarers, and later, Spanish, Dutch, French and English, opened up southern Africa, the Americas (New World), Asia and Oceania to European eyes: Bartholomew Dias had sailed around the Cape of southern Africa in search of a trade route to India; Christopher Columbus, on four journeys across the Atlantic, had prepared the way for European colonisation of the New World; Ferdinand Magellan had commanded the first expedition to sail across the Atlantic and Pacific oceans to reach the Maluku Islands and was continued by Juan Sebastián Elcano, completing the first circumnavigation of the Earth.\nThe Francisco Hernández expedition (1570–1577) (Spanish: Comisión de Francisco Hernández a Nueva España) is considered to be the first scientific expedition to the New World, led by Francisco Hernández de Toledo, a naturalist and physician of the Court of King Philip II, who was highly regarded in Spain because of his works on herbal medicine.\nAmong some of the most important achievements of the expedition were the discovery and subsequent introduction in Europe of a number of new plants that did not exist in the Old World, but that quickly gained acceptance and become very popular among European consumers, such as pineapples, cocoa, corn, and many others.\nDuring the 17th century the naval hegemony started to shift from the Portuguese and Spanish to the Dutch and then the British and French. The new era of scientific exploration began in the late 17th century as scientists, and in particular natural historians, established scientific societies that published their researches in specialist journals. The British Royal Society was founded in 1660 and encouraged the scientific rigour of empiricism with its principles of careful observation and deduction. Activities of early members of the Royal Society served as models for later maritime exploration. Hans Sloane (1650–1753) was elected a member in 1685 and travelled to Jamaica from 1687 to 1689 as physician to the Duke of Albemarle (1653–1688) who had been appointed Governor of Jamaica. In Jamaica Sloane collected numerous specimens which were carefully described and illustrated in a published account of his stay. Sloane bequeathed his vast collection of natural history 'curiosities' and",
    "source": "wikipedia",
    "title": "European and American voyages of scientific exploration",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_36920393",
    "text": "The history of experimental research is long and varied. Indeed, the definition of an experiment itself has changed in responses to changing norms and practices within particular fields of study. This article documents the history and development of experimental research from its origins in Galileo's study of gravity into the diversely applied method in use today.\n\nIbn al-Haytham\nThe Arab physicist Ibn al-Haytham (Alhazen) used experimentation to obtain the results in his Book of Optics (1021). He combined observations, experiments and rational arguments to support his intromission theory of vision, in which rays of light are emitted from objects rather than from the eyes. He used similar arguments to show that the ancient emission theory of vision supported by Ptolemy and Euclid (in which the eyes emit the rays of light used for seeing), and the ancient intromission theory supported by Aristotle (where objects emit physical particles to the eyes), were both wrong.\nExperimental evidence supported most of the propositions in his Book of Optics and grounded his theories of vision, light and colour, as well as his research in catoptrics and dioptrics.  His legacy was elaborated through the 'reforming' of his Optics by Kamal al-Din al-Farisi (d. c. 1320) in the latter's Kitab Tanqih al-Manazir (The Revision of [Ibn al-Haytham's] Optics).\nAlhazen viewed his scientific studies as a search for truth: \"Truth is sought for its own sake. And those who are engaged upon the quest for anything for its own sake are not interested in other things. Finding the truth is difficult, and the road to it is rough. ...\nAlhazen's work included the conjecture that \"Light travels through transparent bodies in straight lines only\", which he was able to corroborate only after years of effort. He stated, \"[This] is clearly observed in the lights which enter into dark rooms through holes. ... the entering light will be clearly observable in the dust which fills the air.\" He also demonstrated the conjecture by placing a straight stick or a taut thread next to the light beam.\nIbn al-Haytham employed scientific skepticism, emphasizing the role of empiricism and explaining the role of induction in syllogism. He went so far as to criticize Aristotle for his lack of contribution to the method of induction, which Ibn al-Haytham regarded as being not only superior to syllogism but the basic requirement for true scientific research.\nSomething like Occam's razor is also present in the Book of Optics. For example, after demonstrating that light is generated by luminous objects and emitted or reflected into the eyes, he states that therefore \"the extramission of [visual] rays is superfluous and useless.\" He may also have been the first scientist to adopt a form of positivism in his approach. He wrote that \"we do not go beyond experience, and we cannot be content to use pure concepts in investigating natural phenomena\", and that the understanding of these cannot be acquired without mathem",
    "source": "wikipedia",
    "title": "History of experiments",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_318374",
    "text": "The following is a list of historically important scientific experiments and observations demonstrating something of great scientific interest, typically in an elegant or clever manner.\n\nAstronomy\nOle Rømer makes the first quantitative estimate of the speed of light in 1676 by timing the motions of Jupiter's satellite Io with a telescope\nArno Penzias and Robert Wilson detect the cosmic microwave background radiation, giving support to the theory of the Big Bang (1964)\nKerim Kerimov launches Kosmos 186 and Kosmos 188 as experiments on automatic docking eventually leading to the development of space stations (1967)\nThe Supernova Cosmology Project and the High-Z Supernova Search Team discover, by observing Type Ia supernovae, that the expansion of the Universe is accelerating (1998)\nGalileo Galilei uses a telescope to observe that the moons of Jupiter appear to circle Jupiter. This evidence supports the heliocentric model, and weakens the geocentric model of the cosmos (1609)\nBiology\nRobert Hooke, using a microscope, observes cells (1665).\nAnton van Leeuwenhoek discovers microorganisms (1674–1676).\nJames Lind, publishes 'A Treatise of the Scurvy' which describes a controlled shipboard experiment using two identical populations but with only one variable, the consumption of citrus fruit (1753).\nEdward Jenner tests his hypothesis for the protective action of mild cowpox infection for smallpox, the first vaccine (1796).\nGregor Mendel's experiments with the garden pea led him to surmise many of the fundamental laws of genetics (dominant vs recessive genes, the 1–2–1 ratio, see Mendelian inheritance) (1856–1863).\nCharles Darwin demonstrates evolution by natural selection using many examples (1859).\nLouis Pasteur uses S-shaped flasks to prevent spores from contaminating broth. This disproves the theory of Spontaneous generation (1861) extending the rancid meat experiment of Francesco Redi (1668) to the micro scale.\nCharles Darwin and his son Francis, using dark-grown oat seedlings, discover the stimulus for phototropism is detected at the tip of the shoot (the coleoptile tip), but the bending takes place in the region below the tip (1880).\nEmil von Behring and Kitasato Shibasaburō demonstrate passive immunity, protection of animals from infection by injection of immune serum (1890).\nThomas Hunt Morgan identifies a sex chromosome linked gene in Drosophila melanogaster (1910) and his student Alfred Sturtevant develops the first genetic map (1913).\nAlexander Fleming demonstrates that the zone of inhibition around a growth of penicillin mould on a culture dish of bacteria is caused by a diffusible substance secreted by the mould (1928).\nFrederick Griffith demonstrates (Griffith's experiment) that living cells can be transformed via a transforming principle, later discovered to be DNA (1928).\nKarl von Frisch decodes the waggle dance honey bees use to communicate the location of flowers (1940).\nGeorge Wells Beadle and Edward Lawrie Tatum moot the \"one gene-one ",
    "source": "wikipedia",
    "title": "List of experiments",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_59616",
    "text": "In biochemistry, fermentation theory refers to the historical study of models of natural fermentation processes, especially alcoholic and lactic acid fermentation. Notable contributors to the theory include Justus Von Liebig and Louis Pasteur, the latter of whom developed a purely microbial basis for the fermentation process based on his experiments. Pasteur's work on fermentation later led to his development of the germ theory of disease, which put the concept of spontaneous generation to rest. Although the fermentation process had been used extensively throughout history prior to the origin of Pasteur's prevailing theories, the underlying biological and chemical processes were not fully understood. In the contemporary, fermentation is used in the production of various alcoholic beverages, foodstuffs, and medications.\n\nOverview of fermentation\nFermentation is the anaerobic metabolic process that converts sugar into acids, gases, or alcohols in oxygen starved environments. Yeast and many other microbes commonly use fermentation to carry out anaerobic respiration necessary for survival. Even the human body carries out fermentation processes from time to time, such as during long-distance running; lactic acid will build up in muscles over the course of long-term exertion. Within the human body, lactic acid is the by-product of ATP-producing fermentation, which produces energy so the body can continue to exercise in situations where oxygen intake cannot be processed fast enough. Although fermentation yields less ATP than aerobic respiration, it can occur at a much higher rate. Fermentation has been used by humans consciously since around 5000 BCE, evidenced by jars recovered in the Iran Zagros Mountains area containing remnants of microbes similar those present in the wine-making process.\nHistory\nPrior to Pasteur's research on fermentation, there existed some preliminary competing notions of it. One scientist who had a substantial degree of influence on the theory of fermentation was Justus von Liebig. Liebig believed that fermentation was largely a process of decomposition as a consequence of the exposure of yeast to air and water. This theory was corroborated by Liebig's observation that other decomposing matter, such as rotten plant and animal parts, interacted with sugar in a similar manner as yeast. That is, the decomposition of albuminous matter (i.e. water-soluble proteins) caused sugar to transform to alcohol. Liebig held this view until his death in 1873. A different theory was supported by Charles Cagniard de la Tour and cell theorist Theodor Schwann, who claimed that alcoholic fermentation depended on the biological processes carried out by brewer's yeast.\nLouis Pasteur's interest in fermentation began when he noticed some remarkable properties of amyl alcohol—a by-product of lactic acid and alcohol fermentation—during his biochemical studies. In particular, Pasteur noted its ability to “rotate the plane of polarized light”, and its “unsy",
    "source": "wikipedia",
    "title": "Fermentation theory",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_18134112",
    "text": "The golden age of cosmology is a term often used to describe the period from 1992 to the present in which important advances in observational cosmology have been made. Prior to the golden age of cosmology, the understanding of the universe was limited to what scientists could observe through telescopes and other instruments. Theories and models were developed based on limited data and observations, and there was much speculation and debate regarding the true nature of the universe.\nThe golden age of cosmology has also seen the development of new observational techniques and technologies. For example, the use of telescopes in space has revolutionized our ability to observe the universe. Space-based observatories such as the Hubble Space Telescope (launched in 1990) and the James Webb Space Telescope (launched in 2021) have provided stunning images and data that have expanded our understanding of the universe. \nIn addition, ground-based telescopes have also undergone significant improvements in recent years. For example, the Atacama Large Millimeter Array (ALMA) in Chile is a revolutionary new telescope that is able to observe the universe in unprecedented detail. It has already made significant contributions to our understanding of star formation and the early universe.\n\nLambda-CDM model\nIn 1992, however, the situation changed dramatically with the launch of the Cosmic Background Explorer (COBE) satellite. This mission was designed to study the cosmic microwave background (CMB) radiation, which is the leftover radiation from the Big Bang. The COBE mission made the first precise measurements of the CMB, and these measurements provided evidence in support of the Big Bang theory. The COBE mission also discovered small fluctuations in the CMB radiation, which were believed to be the seeds of galaxy formation. This discovery was a major breakthrough in our understanding of the early universe, as it provided evidence for the inflationary universe model. This model suggests that the universe underwent a rapid expansion in the first few moments after the Big Bang, which would have caused the tiny fluctuations in the CMB.\nIn the years following the COBE mission, there were several other important discoveries in observational cosmology. One of the most significant was the discovery of dark matter. This mysterious substance makes up approximately 27% of the universe, yet it cannot be observed directly. Its existence was inferred from its gravitational effects on visible matter.\nThe discovery of dark matter was followed by the discovery of dark energy, which makes up approximately 68% of the universe. Dark energy is believed to be responsible for the accelerated expansion of the universe, which was first observed in 1998 by two independent teams of astronomers.\nThe discovery of dark matter and dark energy, along with the observations of the CMB and the large-scale structure of the universe, have led to the development of the Lambda-CDM model of the universe. ",
    "source": "wikipedia",
    "title": "Golden age of cosmology",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_38078804",
    "text": "Oktōēchos (here transcribed \"Octoechos\"; Greek: ὁ Ὀκτώηχος pronounced in koine: Ancient Greek pronunciation: [okˈtóixos]; from ὀκτώ \"eight\" and ἦχος \"sound, mode\" called echos; Slavonic: Осмогласие, Osmoglasie from о́смь \"eight\" and гласъ \"voice, sound\") is the name of the eight mode system used for the composition of religious chant in most Christian churches during the Middle Ages. In a modified form, the octoechos is still regarded as the foundation of the tradition of monodic Orthodox chant today (Neobyzantine Octoechos).\nThe octoechos as a liturgical concept which established an organization of the calendar into eight-week cycles, was the invention of monastic hymnographers at Mar Saba in Palestine, at the Patriarchates of Antiochia and of Constantinople. It was officially announced as the modal system of hymnography at the Quinisext Council in 692.\nA similar eight-mode system was established in Western Europe during the Carolingian reform, and particularly at the Second Council of Nicaea in 787 AD which decanonised the former iconoclastic council in 754 and confirmed earlier ones. Quite possibly this was an attempt to follow the example of the Eastern Church by an octoechos reform, even if it was rather a transfer of knowledge with an introduction of a new book called \"tonary\" which introduced into a Western octoechos of its own design.\nIt had a list of incipits of chants ordered according to the intonation formula of each tone in its psalmody. Later on, fully notated and theoretical tonaries were also written. The Byzantine book octoechos (9th century) was one of the first hymn books with musical notation and its earliest surviving copies date from the 10th century.\n\nOrigins\nStudents of Orthodox chant today often study the history of Byzantine chant in three periods, identified by the names John of Damascus (675/676-749) as the \"beginning\", John Koukouzeles (c. 1280–1360) as the \"flower\" (Papadic Octoechos), and Chrysanthos of Madytos (c. 1770-c. 1840) as the master of the living tradition today (Neobyzantine Octoechos). The latter has the reputation that he once connected in his time the current tradition with the past of Byzantine chant, which was in fact the work of at least four generations of teachers at the New Music School of the Patriarchate.\nThis division of the history into three periods begins quite late with the 8th century, despite the fact that the octoechos reform was already accepted some decades earlier, before John and Cosmas entered the monastery Mar Saba in Palestine. The earliest sources which gave evidence of the octoechos' use in Byzantine chant, can be dated back to the 6th century.\n",
    "source": "wikipedia",
    "title": "Hagiopolitan Octoechos",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1653438",
    "text": "Jennifer Michael Hecht (born November 23, 1965) is a teacher, author, poet, historian, and philosopher. She was an associate professor of history at Nassau Community College (1994–2007) and most recently taught at The New School in New York City.\nHecht has seven published books, her scholarly articles have been published in many journals and magazines, and her poetry has appeared in The New Yorker, The New Republic, Ms. Magazine, and Poetry Magazine, among others. She has also written essays and book reviews for The New York Times, The Washington Post, The Philadelphia Inquirer, The American Scholar, The Boston Globe and other publications. She has written several columns for The New York Times online \"Times Select.\" In 2010 Hecht was one of the five nonfiction judges for the National Book Award.\nHecht is a longtime blogger for The Best American Poetry series web site and maintains a personal blog on her website. She resides in Brooklyn, New York.\n\nBackground\nBorn in Glen Cove, New York on Long Island, Hecht attended Adelphi University, where she earned a BA in history, for a time studying at the Université de Caen, and the Université d'Angers. She earned her PhD in the history of science from Columbia University in 1995 and taught at Nassau Community College from 1994 to 2007, finally as a tenured associate professor of history. Hecht has taught in the MFA programs at The New School and Columbia University, and is a fellow of the New York Institute for the Humanities.\nHecht is married and has two children.\nShe has appeared on television on the Discovery Channel, The Morning Show with Marcus Smith, Road to Reason and MSNBC's Hardball, and on radio on The Brian Lehrer Show, The Leonard Lopate Show, On Being (formerly known as Speaking of Faith), All Things Considered, The Joy Cardin Show, and others.\nIntellectual interests and writings\nOf her three major intellectual interests, she ranks them, \"Poetry came first, then historical scholarship, then public atheism, and they probably remain in that order in my dedication to them.\"\nOriginally intending to be a poet, she was drawn to the history of science. Her first book, The End of the Soul: Scientific Modernity, Atheism, and Anthropology in France, 1876-1936, grew out of her dissertation on some late 19th-century anthropologists who formed the Society of Mutual Autopsy. The members would dissect each other's brains after death, and Hecht, having noticed their atheism, came to understand that this was being done not only for the sake of scientific finds, but perhaps to prove to the Catholic Church that the soul does not exist.\nWhile researching her first book, she came to realize that there was no sufficient history of atheism, and that led to her second book, Doubt: A History.\nWhile writing Doubt, she found that many atheists went beyond simply stating that there are no gods and also made profound suggestions about how people should think of life and how we should live. That led to her third book, Th",
    "source": "wikipedia",
    "title": "Jennifer Michael Hecht",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_42748548",
    "text": "Hellenophilia is the idea that all western science began in Greek tradition. This is discussed in length by David Pingree in his address to colleagues. Hellenophilia is a way of thought that allows those who look into the history of science to be blinded to science born in other cultures. Pingree states, in explanation of the term that \"a Hellenophile suffers from a form of madness that blinds him or her to historical truth\" (Pingree, 1992, p. 554) He continues by explaining the main symptoms of Hellenophilia \"the first of these is that the Greeks invented science; the second is that they discovered a way to truth, the scientific method, that we are now successfully following; the third is that the only real sciences are those that began in Greece; and the fourth (and last?) is that the true definition of science is just that which scientists happen to be doing now, following a method or methods adumbrated by the Greeks, but never fully understood or utilized by them\" (Pingree, 1992, p. 555).\nAn anthropological etiology of Greek innovation in natural science is advanced by sociologist Michael G. Horowitz in \"The Scientific Dialectic of Ancient Greece and the Cultural Tradition of Indo-European Speakers\" (Journal of Indo-European Studies, 24(3-4):409-19 [1996]).\nAlthough Hellenophilia relates directly to the history of science, it is important to look at it through aspects of history that lend to the habit, other than the symptoms listed by Pingree. One of these habits, as described by David C. Lindberg is looking at the history of science as starting with writing in fully syllabic systems. According to Lindberg the beginning of syllabic writing was around 1500 B.C. However, fully alphabetic writing was apparent in Greece in 800 B.C. (Linberg, 2007, p. 10).\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Hellenophilia",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_81581326",
    "text": "Information System \"History of Geology and Mining\" (Russian: Информационная система «История геологии и горного дела») is a scientific information system (knowledge base, bibliographic database and non-commercial website) containing biographical, bibliographical data and documents about scientists, scientific organizations, and publications related to geological and mining sciences. It is a joint project of the Geological Institute and the Library for Natural Sciences of the Russian Academy of Sciences. It represents the first attempt to systematize and provide access to a large array of data in the field of the history of geology and mining. It was created to support and facilitate scientific research in the history of science.\n\nHistory\nThe information system is a continuation of the printed publications in the series “Materials on the History of Geology in the USSR”.\nVersions of the \"History of Geology and Mining\" information system:\n\nTest version on the Library for Natural Sciences of the Russian Academy of Sciences website (2011—2015 — scirus.benran.ru/higeo.\nAt the Geological Institute RAS: 2015–2025 at higeo.ginras.ru. New test version at https://higeo.ru\nDescription and Structure\nThe system was created based on the customizable network software complex \"SciRus\" developed at the Library for Natural Sciences of the Russian Academy of Sciences (part of the Information and Library Council of the RAS). Software development by: N. Kalenov, A. Senko, and M. Yakshin.\nThe information system on the history of geology includes core data about scientists:\n\nData for scientific biographies of scientists\nOrganizations (academies, educational institutions, institutes, and scientific societies), geography and research directions\nPrinted sources (journals, newspapers, and other serial publications)\nBrief scientific biographies of scientists (under development)\nMajor scientific works and references to them (URLs and DOIs are provided where available)\nLiterature about scientists and references to it (URLs are provided where available)\nDocuments related to scientists (questionnaires, manuscripts, event programs, correspondence, and other documents)\nLinks to portraits of scientists, group photographs, engravings, and other images.\nThe data array is centered around the scientist's profile. A large portion of the documents are still being processed or are currently in closed access: about 10,000 folders on scientists, >15,000 photographs and other images.\nInternal integrated search across many parameters is the primary functionality of the Information System.\n",
    "source": "wikipedia",
    "title": "History of Geology and Mining (Information System)",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_33561186",
    "text": "The history of scholarship is the historical study of fields of study which are not covered by the English term \"science\" (cf., history of science), but are covered by, for example, the German term \"Wissenschaft\" (i.e., all kinds of academic studies). Examples include the history of classical studies, philosophy, religion, biblical studies, historiography, music, art, and literature. \nIt is a field which has recently undergone a complete renewal and is now a major branch of research. In 2015, the Society for the History of the Humanities was established, coinciding with the launch of the journal History of Humanities in 2016. Both developments reflect the field’s growing institutional presence and international scholarly collaboration.\n\nClassical scholarship\nRudolph Pfeiffer (1968) describes the history of classical scholarship from its revival inspired by Petrarch to the achievements of the Italian humanists and the independent movement in Holland (including Erasmus) and the German scholar-reformers. Pfeiffer traces the development of classical scholarship in the countries of Western Europe through the next two centuries, with particular attention to sixteenth-century France and eighteenth-century England. Finally he provides an account of the new approach made by Winckelmann and his successors in Germany.\nPhilosophers, scholars, polymaths, and scientists\nThe word scientist was coined by the English philosopher and historian of science William Whewell in 1833. Until then there was no differentiation between the history of science, the history of philosophy, and the history of scholarship.\nBefore 1700 the fields of scholarship were not of a size that made academic specialisation necessary. Academic disciplines as we know them today did not exist. Scholars were generally active in both the sciences and what are now called the Arts and Humanities.\nSee also\nArt history\nCultural history\nHistoric recurrence\nHistory of archaeology\nHistory of books\nHistory of education\nHistory of European universities\nHistory of knowledge\nHistory of mathematics\nHistory of writing\nHuman science\nIntellectual history\nMedieval university\nScholarly method\n",
    "source": "wikipedia",
    "title": "History of scholarship",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_48395726",
    "text": "The presence of women in science spans the earliest times of the history of science wherein they have made substantial contributions. Historians with an interest in gender and science have researched the scientific endeavors and accomplishments of women, the barriers they have faced, and the strategies implemented to have their work peer-reviewed and accepted in major scientific journals and other publications. The historical, critical, and sociological study of these issues has become an academic discipline in its own right.\nThe involvement of women in medicine occurred in several early Western civilizations, and the study of natural philosophy in ancient Greece was open to women. Women contributed to the proto-science of alchemy in the first or second centuries CE During the Middle Ages, religious convents were an important place of education for women, and some of these communities provided opportunities for women to contribute to scholarly research. The 11th century saw the emergence of the first universities; women were, for the most part, excluded from university education. Outside academia, botany was the science that benefitted most from the contributions of women in early modern times. The attitude toward educating women in medical fields appears to have been more liberal in Italy than elsewhere. The first known woman to earn a university chair in a scientific field of studies was eighteenth-century Italian scientist Laura Bassi.\nGender roles were largely deterministic in the eighteenth century and women made substantial advances in science.  During the nineteenth century, women were excluded from most formal scientific education, but they began to be admitted into learned societies during this period. In the later nineteenth century, the rise of the women's college provided jobs for women scientists and opportunities for education. Marie Curie paved the way for scientists to study radioactive decay and discovered the elements radium and polonium. Working as a physicist and chemist, she conducted pioneering research on radioactive decay and was the first woman to receive a Nobel Prize in Physics and became the first person to receive a second Nobel Prize in Chemistry. Sixty women have been awarded the Nobel Prize between 1901 and 2022. Twenty-four women have been awarded the Nobel Prize in physics, chemistry, physiology or medicine.\n\nCross-cultural perspectives\nIn the 1970s and 1980s, many books and articles about women scientists were appearing; virtually all of the published sources ignored women of color and women outside of Europe and North America. The formation of the Kovalevskaia Fund in 1985 and the Organization for Women in Science for the Developing World in 1993 gave more visibility to previously marginalized women scientists, but even today there is a dearth of information about current and historical women in science in developing countries. According to academic Ann Hibner Koblitz:\n\nMost work on women scientists has focused",
    "source": "wikipedia",
    "title": "Women in science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_57975652",
    "text": "The human body has been subject of much debate. How people are defined, and what defined them – be it their anatomy or their energy or both – depends on culture and time. Culture not only defines how sex is perceived but also how gender is defined.  Today gender, sex, and identity continue to be of much debate and change based on what place and people are being examined.\nThe early modern idea of the body was a cultural ideal, an understanding and approach to how the body works and what place that body has in the world.  All cultural ideals of the body in the early modern period deal with deficiencies and disorders within a body, commonly told through a male ideal.  Ideas of the body in the early modern period form the history of how bodies should be and how to correct the body when something has gone wrong.  Therefore, early modern conceptions of the body were not biological as there was not a restrictive biological view of the human body as established by modern science.\nConceptions of the body are primarily either eastern, based in China and involving practices such as Traditional chinese medicine, or western, which follows the Greek traditions of science and is more closely related to modern science despite original anatomists and ideas of the body being just as unscientific as Chinese practices.\n\nHistoriography\nIn Western historical research, scholars began investigating the cultural history of the human body in detail in the 1980s. The movement is particularly associated with the historian of medicine Roy Porter, whose 1991 article 'History of the Body' was a seminal study. 1995 saw the foundation of the journal Body and Society, by which time the field of the history of the body was already extensive and diverse.\nPorter pointed out that Western historiography had previously assumed mind–body dualism (i.e. that the body is fundamentally separate from the mind or soul) and therefore that the cultural history of bodies as material objects had been overlooked: 'given the abundance of evidence available, we remain remarkably ignorant about how individuals and social groups have experienced, controlled, and projected their embodied selves. How have people made sense of the mysterious link between \"self\" and its extensions? How have they managed the body as an intermediary between self and society?' He emphasised that the history of the body is important to understanding histories of coercion and control, sex and gender, and other important but culturally varied aspects of human experience.\nAnother prominent voice in the field at the same time was Caroline Walker Bynum, whose 1988 Holy Feast and Holy Fast became a landmark study. Both Bynum and Porter noted that during the 1980s Western history of the body research drew on post-structuralist thought, such as Michel Foucault's ideas of biopolitics and biopower, which emphasised that state power is not abstract, but exercised through and over human bodies. But both expressed a concern that research ",
    "source": "wikipedia",
    "title": "History of beliefs about the human body",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_15361",
    "text": "An ice age is a term describing periods of time when the reduction in the temperature of Earth's surface and atmosphere results in the presence or expansion of continental and polar ice sheets and alpine glaciers. The term is applied in several different senses to very long and comparatively short periods of cooling. Colder periods are called glacials or ice ages, and warmer periods are called interglacials.\nEarth's climate alternates between icehouse and greenhouse periods based on whether there are glaciers on the planet, and for most of Earth's history it has been in a greenhouse period with little or no permanent ice. Over the very long term, Earth is currently in an icehouse period called the Late Cenozoic Ice Age, which started 34 million years ago. There have been colder and warmer periods within this ice age, and the term is also applied to the Quaternary glaciation, which started 2.58 million years ago. Within this period, the Last Interglacial ended 115,000 years ago, and was followed by the Last Glacial Period (LGP), which gave way to the current warm Holocene, which started 11,700 years ago. The most severe cold period of the LGP was the Last Glacial Maximum, which reached its maximum between 26,000 and 20,000 years ago. The most recent glaciation was the Younger Dryas between 12,800 and 11,700 years ago.\n\nHistory of research\nIn 1742, Pierre Martel (1706–1767), an engineer and geographer living in Geneva, visited the valley of Chamonix in the Alps of Savoy. Two years later he published an account of his journey. He reported that the inhabitants of that valley attributed the dispersal of erratic boulders to the glaciers, saying that they had once extended much farther. Later similar explanations were reported from other regions of the Alps. In 1815 the carpenter and chamois hunter Jean-Pierre Perraudin (1767–1858) explained erratic boulders in the Val de Bagnes in the Swiss canton of Valais as being due to glaciers previously extending further. An unknown woodcutter from Meiringen in the Bernese Oberland advocated a similar idea in a discussion with the Swiss-German geologist Jean de Charpentier (1786–1855) in 1834. Comparable explanations are also known from the Val de Ferret in the Valais and the Seeland in western Switzerland and in Goethe's scientific work. Such explanations could also be found in other parts of the world. When the Bavarian naturalist Ernst von Bibra (1806–1878) visited the Chilean Andes in 1849–1850, the natives attributed fossil moraines to the former action of glaciers.\nMeanwhile, European scholars had begun to wonder what had caused the dispersal of erratic material. From the middle of the 18th century, some discussed ice as a means of transport. The Swedish mining expert Daniel Tilas (1712–1772) was, in 1742, the first person to suggest drifting sea ice was a cause of the presence of erratic boulders in the Scandinavian and Baltic regions. In 1795, the Scottish philosopher and gentleman naturalist, James Hutto",
    "source": "wikipedia",
    "title": "Ice age",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_71465383",
    "text": "Indigenous science is the application and intersection of Indigenous knowledge and science. This field is based on careful observation of the environment, and through experimentation. It is a holistic field, informed by physical, social, mental and cultural knowledge. When applied to ecology and the environment, it can be sometimes termed traditional ecological knowledge. Indigenous science involves the knowledge systems and practices of Indigenous peoples, which are rooted in their cultural traditions and relationships to their indigenous context. There are some similar methods of Western science including (but not limited to): observation, prediction, interpretation, and questioning. There are also some areas in which Western science and Indigenous science differ. Indigenous knowledge is place and case-specific and does not attempt to label or generalize natural processes. Western science strives to find commonalities and theories that can be applied to all areas, such as Newton's Laws of Physics. This is because most Indigenous knowledge stems from the relationship humans have with their environment, which is passed down through stories or is discovered through observation. Western knowledge takes a different approach by isolating targets to study, splitting them from their surroundings and making sets of assumptions and theories. Community is a larger aspect of Indigenous science, and conclusions are shared through oral tradition and family knowledge, whereas most Western science research is published in a journal specific to that scientific field, and may restrict access to various papers.\nThere is a history of oppression against Native Americans beginning when settlers came to America, and this has carried into the field of Indigenous science as American scientists and academics have overlooked the findings and knowledge of Indigenous people. Multiple studies found that Indigenous perspectives are rarely represented in empirical studies, and has led to the underrepresentation of Native people in research fields. In addition, Western researchers have benefitted from the research they do about Indigenous nations, while the tribes do not receive compensation for their work and information.\nHigher recognition and advocacy of Indigenous people in the 21st century has increased the visibility of this field. There has been a growing recognition of the potential benefits of incorporating Indigenous perspectives and knowledge, particularly in fields such as ecology and environmental management.\n\nOral traditions in Indigenous science\nIndigenous knowledge and experiences are often passed down orally from generation to generation. Indigenous knowledge has an empirical basis and has traditionally been used to predict and understand the world. Such knowledge has informed studies of human management of natural processes.\nThis oral knowledge is embedded in songs and dances, which allows for accurate information to be passed down for centuries as songs and ",
    "source": "wikipedia",
    "title": "Indigenous science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_82060664",
    "text": "The Institute for the History of Natural Sciences (IHNS CAS; Chinese: 中国科学院自然科学史研究所; pinyin: Zhōngguó Kēxuéyuàn Zìrán Kēxuéshǐ Yánjiūsuǒ) is a leading research institution in China dedicated to the study of the history of science, technology, and medicine. It is affiliated with the Chinese Academy of Sciences (CAS).\n\nHistory\nThe Institute for the History of Natural Sciences was founded in 1957 with the active involvement of the renowned British biochemist and historian of science Joseph Needham (Chinese: 李约瑟, Lǐ Yuēsè) and a group of Chinese scholars. Its establishment was initiated to systematically research China's rich scientific and technological heritage and its place in world history. Initially, the institute was located in the Gulou district of Beijing.\nResearch Focus\nThe institute's primary mission is to conduct fundamental and applied research on the history of science and technology in China and the world, and to promote the development of this discipline.\nKey research areas include:\n\nHistory of Science and Technology in China: Studying traditional Chinese science, technology, medicine, astronomy, mathematics, and their interaction with society and culture.\nHistory of World Science: Comparative studies, history of scientific exchanges, study of the Scientific Revolution and the development of modern science.\nTheoretical Studies in History of Science: Methodology, philosophy, and sociology of science.\nPreservation of Scientific and Technological Heritage: Identifying and researching tangible and intangible objects of scientific and technological heritage.\nScience, Technology and Society (STS): Investigating the interrelations between scientific/technological progress and social development.\nThe IHNS plays a central role in developing and institutionalizing the history of science as an academic discipline in China. Its research contributes significantly to the global understanding of the history of science and highlights China's contributions to scientific and technological development.\n",
    "source": "wikipedia",
    "title": "Institute for the History of Natural Sciences",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_37092860",
    "text": "The International Conference on Cold Fusion (ICCF) (also referred to as Annual Conference on Cold Fusion in 1990-1991 and mostly as International Conference on Condensed Matter Nuclear Science since 2007) is an annual or biennial conference on the topic of cold fusion. An international conference on cold fusion was held in Santa Fe, New Mexico, USA in 1989. However, the first ICCF conference (ICCF1) took place in 1990 in Salt Lake City, Utah, USA, under the title \"First Annual Conference on Cold Fusion\". Its location has since rotated between Russia, the USA, Europe, and Asia. It was held in India for the first time in 2011. The conferences have been criticized as events which attract \"crackpots\" and \"pseudo-scientists\".\n\nReception\nThe First Annual Conference on Cold Fusion was held in March 1990 in Salt Lake City, Utah, United States. Robert L. Park of the American Physical Society derisively referred to it as a \"seance of true believers.\" The conference was attended by more than 200 researchers from the United States, Italy, Japan, India and Taiwan and dozens of reporters from all over the U.S. and abroad.\nThe Third International Conference on Cold Fusion was held in 1992 in Nagoya, Japan. It was described by The New York Times, \"depending on one's point of view\" as \"either a turning point in which evidence was presented that will convince the skeptics that cold fusion exists or a religious revival where claims of miracles were lapped up by ardent believers.\" The conference was sponsored by seven Japanese scientific societies, it was attended by 200 Japanese scientists and more than 100 from abroad. Tomohiro Taniguchi, then director of the Electric Power Technology Division at Japan's Ministry of International Trade and Industry, reportedly said that the Ministry of International Trade and Industry was willing to finance research in the field in view of \"encouraging evidence, especially after the conference.\" The conference was also covered by the Associated Press.\nA journalist for the Wired magazine attended the 1998 conference in Vancouver—apparently the only mainstream journalist who attended—and reported that he found there \"about 200 extremely conventional-looking scientists, almost all of them male and over 50\" with some apparently over 70. He then inferred that \"[the] younger ones had bailed years ago, fearing career damage from the cold fusion stigma.\" He reported seeing \"highly technical presentations\" and \"was amazed by the quantity of the work, its quality, and the credentials of the people pursuing it\", whereas \"[a] few obvious pseudoscientists, promoting their ideas in an adjoining room used for poster sessions, were politely ignored.\"\nBy 1999, attendance by researchers at the ICCF meetings drew comment from the field of science studies. Although scientific debate over cold fusion had effectively ended in 1990, attendance at the ICCF meetings for the next 8 years had been relatively stable at between 100 and 300. Sociologist Bart S",
    "source": "wikipedia",
    "title": "International Conference on Cold Fusion",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_57241168",
    "text": "The International Scientific Committee on Price History was created in 1929 by William Beveridge and Edwin Francis Gay after receiving a five-year grant from the Rockefeller Foundation. The national representatives were William Beveridge for Great Britain, Moritz John Elsas for Germany, Edwin Francis Gay for the United States, Earl J. Hamilton for Spain, Henri Hauser for France and Alfred Francis Pribram for Austria; later, Franciszek Bujak for Poland and Nicolaas Wilhelmus Posthumus for the Netherlands also joined; Arthur H. Cole was in charge of finances for the whole project.\n\nBooks by the committee\nHamilton (Earl J.), American Treasure and the Price Revolution in Spain (1501–1650), 1934.\nHamilton (Earl J.), Money, Prices and Wages in Valencia, Aragon and Navarre (1351–1500), 1936.\nHauser (Henri), Recherches et documents sur l’histoire des prix en France de 1500 à 1800, 1936.\nElsas (Moritz John), Umriß einer Geschichte der Preise und Löhne in Deutschland vom ausgehenden Mittelalter bis zum Beginn des 19. Jarhunderts, 3 vol., 1936–1949.\nPřibram (Alfred Francis), Materialien zur Geschichte der Preise und Löhne in Österreich, 1938.\nCole (Arthur Harrison), Wholesale Commodity Prices in the United States 1700–1861, 1938.\nBeveridge (William H.), Prices and Wages in England from the 12th to the 19th Century, 1939.\nPosthumus (Nicolaas), Nederlandsche Prijsgeschiedenis, 1943–1964.\nHamilton (Earl J.), War and Prices in Spain (1651–1800), 1947.\nReferences\n\nOlivier Dumoulin, \"Aux origines de l'histoire des prix\", Annales. Économies, sociétés, civilisations, 45/2, 1990, p. 507-522[1].\nJulien Demade, Produire un fait scientifique. Beveridge et le Comité international d'histoire des prix, Paris, Publications de la Sorbonne, 2018.\n",
    "source": "wikipedia",
    "title": "International scientific committee on price history",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_76635267",
    "text": "Isis was an encyclopedic journal that focused on articles on natural science, medicine, technology, economics as well as art and history. It also published important articles on science policy and the organization of science. Edited by Lorenz Oken and published by Friedrich Arnold Brockhaus, Isis was the first interdisciplinary journal in the German-speaking world.\nThe 41 volumes of the journal named after the Egyptian goddess Isis were nominally published from 1817 to 1848. However, the first issue appeared on August 1, 1816, while the printing of the last issue was delayed until February 1850. Until 1832, Isis bore the title Encyclopädische Zeitung. After the focus of the articles published in it had changed, Oken changed the title to Encyclopädische Zeitschrift, vorzüglich für Naturgeschichte, vergleichende Anatomie und Physiologie in 1833. Initially printed in Jena, the journal was banned in the Grand Duchy of Saxe-Weimar-Eisenach and from the summer of 1819 was produced in nearby Rudolstadt in the court printing works of the Principality of Schwarzburg-Rudolstadt. The magazine's original print run of 1,500 copies fell rapidly in the first few years of its existence and amounted to around 200 copies in the last few years.\nOriginally conceived as a non-political journal, Oken was forced to vehemently defend the freedom of the press in the first years of Isis' existence. This resulted in numerous lawsuits against Oken, some of which overlapped in time, which led to temporary bans on Isis in the Grand Duchy of Saxe-Weimar-Eisenach. In the run-up to the Carlsbad Decrees, this led to Oken's dismissal as a professor at the University of Jena at the end of June 1819 under pressure from the states of the Holy Alliance.\nFrom 2006 to 2013, a project funded by the German Research Foundation at the Friedrich Schiller University Jena studied the significance of Isis for scientific communication and the popularization of the natural sciences in the first half of the 19th century.\n\nOrigin\nIn a letter dated April 11, 1814, Lorenz Oken contacted the publisher Friedrich Arnold Brockhaus for the first time and offered him his publication Neue Bewaffnung, neues Frankreich, neues Theutschland for printing. Brockhaus did not print it, but Oken subsequently contributed to Brockhaus' Conversations-Lexikon and, from June 1815 at the latest, was a contributor to the Deutsche Blätter, which Brockhaus had been publishing since October 1813 following the Battle of Leipzig and which became the most important journal in central Germany in 1813/1814. Presumably at the end of June/beginning of July 1815, Oken took over the editorship of the Tagesgeschichte, a supplement to the Deutsche Blätter, which was dedicated to daily politics and for which he wrote and edited numbers 1 to 16. With the end of the Wars of Liberation, the focus of the Deutsche Blätter shifted from war reporting to general daily politics, which was associated with a considerable decline in circulation from",
    "source": "wikipedia",
    "title": "Isis (journal, 1816)",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_78066172",
    "text": "The Islamic Scientific Manuscripts Initiative (ISMI) (Arabic: مبادرة المخطوطات العلمية الإسلامية) is an online database that supports research on mathematics history in the Islamic world to 1350 CE. The initiative aims to provide accessible information on all Islamic manuscripts in the exact sciences, including astronomy, mathematics, theories, mathematical geography, music, mechanics, and related subjects.\nIt is an initiative of the Max Planck Institute for the History of Science (MPIWG), which is dedicated to advancing scientific knowledge and research.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Islamic Scientific Manuscripts Initiative",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_69664295",
    "text": "Languages of science are vehicular languages used by one or several scientific communities for international communication. According to the science historian Michael Gordin, scientific languages are \"either specific forms of a given language that are used in conducting science, or they are the set of distinct languages in which science is done.\" These two meanings are different, since the first describes a distinct prose in a given language (i.e., scientific writing), while the second describes which languages are used in mainstream science.\n\nUntil the 19th century, classical languages—such as Latin, Classical Arabic, Sanskrit, Classical Malay and Classical Chinese—were commonly used across Afro-Eurasia for international scientific communication. A combination of structural factors, the emergence of nation-states in Europe, the Industrial Revolution, and the expansion of colonization entailed the global use of three European national languages: French, German, and English. Yet new languages of science, such as Russian and Italian, had started to emerge by the end of the 19th century—to the point that international scientific organizations began promoting the use of constructed languages such as Esperanto as a non-national global standard.\nAfter the First World War, English gradually outpaced French and German; it became the leading language of science, but not the only international standard. Research in the Soviet Union (USSR) rapidly expanded in the years after the Second World War, and access to Russian journals became a major policy issue in the United States, prompting the early development of machine translation. In the last decades of the 20th century, an increasing number of scientific publications were written primarily in English, in part due to the preeminence of English-speaking scientific infrastructure, indexes, and metrics such as the Science Citation Index. Local languages remain largely relevant for science in major countries and world regions such as China, Latin America, and Indonesia. Disciplines and fields of study with a significant degree of public engagement—such as social sciences, environmental studies, and medicine—have also maintained the relevance of local languages.\n\nThe development of open science has revived the debate over linguistic diversity in science, as social and local impact has become an important objective of open science infrastructure and platforms. In 2019, 120 international research organizations cosigned the Helsinki Initiative on Multilingualism in Scholarly Communication; they also called for supporting multilingualism and the development of an \"infrastructure of scholarly communication in national languages\". In 2021, UNESCO's Recommendation for Open Science included \"linguistic diversity\" as one of the core features of open science, since this diversity aims to \"make multilingual scientific knowledge openly available, accessible and reusable for everyone.\" In 2022, the Council of the European Un",
    "source": "wikipedia",
    "title": "Languages of science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_73378834",
    "text": "Leith AGCM is a climate model that was developed by Cecil Leith beginning in 1958; it is likely the oldest atmospheric general circulation model. Leith published videos of its model output, inspiring other scientists to do the same. Today it has been superseded by climate models developed from different base codes; as such, it is little known.\n\nHistory and development\nEfforts to calculate the behaviour of the weather system commenced in the 1920s with a seminal paper by Lewis Fry Richardson. By the 1950s and 1960s several groups were involved in making climate models, with major efforts taking place at several US universities that eventually gave rise to the well-known GFDL, UCLA, and NCAR models. Today climate models are an important enterprise with significant impact on public policy, where hundreds of scientists and institutions participate worldwide.\nThe researcher Cecil E. “Chuck” Leith (1923–2016) is well-known for his research on fluid mechanics. After an initial career on the Manhattan Project, which resulted in the invention of nuclear bombs, he joined the Lawrence Radiation Laboratory after 1946 and in 1968 the National Center for Atmospheric Research. Beginning in 1958, he began to work on a climate model that was later named the \"Leith atmospheric model\" or \"Livermore atmospheric model\". Its existence was barely reported at that time, with only several contemporary journal articles mentioning it. According to interviews with Leith, he was inspired to work on climate modelling by the noted scientist Edward Teller and by the idea to put his knowledge on nuclear explosions to use in a field that wouldn't be hindered by nuclear test bans. The model was written in assembly language, which may have given it a headstart compared to other climate model projects that were undergoing in Livermore at the time and which relied on compiler language. There appear to have been four versions, based on reports of improvement work on the code, and Leith publicized numerous videos (at the time called \"movies\") of the output of his model. Today, the readable presentation of the often enormous quantities of data output by climate models is a major problem in climate modelling; and Leith's example inspired other scientists to make videos as well. Leith apparently relied on a private company, Pacific Title, which worked in the entertainment industry at Hollywood, and one video displaying the output of hid model.\n",
    "source": "wikipedia",
    "title": "Leith AGCM",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_18403",
    "text": "Logical positivism, also known as logical empiricism or neo-positivism, was a philosophical movement, in the empiricist tradition, that sought to formulate a scientific philosophy in which philosophical discourse would be, in the perception of its proponents, as authoritative and meaningful as empirical science.\nLogical positivism's central thesis was the verification principle, also known as the \"verifiability criterion of meaning\", according to which a statement is cognitively meaningful only if it can be verified through empirical observation or if it is a tautology (true by virtue of its own meaning or its own logical form). The verifiability criterion thus rejected statements of metaphysics, theology, ethics and aesthetics as cognitively meaningless in terms of truth value or factual content. Despite its ambition to overhaul philosophy by mimicking the structure and process of empirical science, logical positivism became erroneously stereotyped as an agenda to regulate the scientific process and to place strict standards on it.\nThe movement emerged in the late 1920s among philosophers, scientists and mathematicians congregated within the Vienna Circle and Berlin Circle and flourished in several European centres through the 1930s. By the end of World War II, many of its members had settled in the English-speaking world and the project shifted to less radical goals within the philosophy of science.\nBy the 1950s, problems identified within logical positivism's central tenets became seen as intractable, drawing escalating criticism among leading philosophers, notably from Willard Van Orman Quine and Karl Popper, and even from within the movement, from Carl Hempel. These problems would remain unresolved, precipitating the movement's eventual decline and abandonment by the 1960s. In 1967, philosopher John Passmore pronounced logical positivism \"dead, or as dead as a philosophical movement ever becomes\".\n\nOrigins\nLogical positivism emerged in Germany and Austria amid a cultural background characterised by the dominance of Hegelian metaphysics and the work of Hegelian successors such as F. H. Bradley, whose metaphysics portrayed the world without reference to empirical observation. The late 19th century also saw the emergence of neo-Kantianism as a philosophical movement, in the rationalist tradition.\nThe logical positivist program established its theoretical foundations in the empiricism of David Hume, Auguste Comte and Ernst Mach, along with the positivism of Comte and Mach, defining its exemplar of science in Einstein's general theory of relativity. In accordance with Mach's phenomenalism, whereby material objects exist only as sensory stimuli rather than as observable entities in the real world, logical positivists took all scientific knowledge to be only sensory experience. Further influence came from Percy Bridgman's operationalism—whereby a concept is not knowable unless it can be measured experimentally—as well as Immanuel Kant's perspective",
    "source": "wikipedia",
    "title": "Logical positivism",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_76589675",
    "text": "The materialism controversy (German: Materialismusstreit) was a public debate in the mid-19th century about how new developments in the natural sciences might affect existing worldviews. During the 1840s, a new form of materialism emerged, shaped by advances in biology and the decline of idealistic philosophy. This form of materialism sought to explain human beings and their behavior through scientific methods. The central question of the debate was whether scientific discoveries were compatible with traditional ideas such as the existence of an immaterial soul, a personal God, and human free will. The discussion also touched on deeper philosophical issues, such as what kind of knowledge a materialist or mechanical view of the world could offer.\nIn his Physiologische Briefe from 1846, zoologist Carl Vogt argued that mental processes were entirely physical, famously stating that \"thoughts stand in the same relation to the brain as bile does to the liver or urine to the kidneys.\" In 1854, the physiologist Rudolf Wagner criticized this view in a speech to the Göttingen Naturalists' Assembly. He argued that religious belief and science belonged to separate areas of understanding, and that natural science could not answer questions about God, the soul, or free will.\nWagner’s comments were strongly worded, accusing materialists of trying to undermine spiritual values. His attacks sparked sharp responses from Vogt and others. The materialist position was later defended by figures such as physiologist Jakob Moleschott and physician Ludwig Büchner, brother of writer Georg Büchner. Supporters of materialism saw themselves as opposing what they viewed as outdated philosophical, religious, and political ideas. While their approaches varied, they found growing support among the middle classes. The idea of a scientific worldview became an important feature in the broader cultural debates of the late 19th and early 20th centuries.\n\nDevelopment of natural scientific materialism\n\nCarl Vogt and the political opposition\nThe materialism controversy was sparked in part by the writings of physiologist Carl Vogt, beginning in 1847. His commitment to materialism was shaped by the scientific and political reform movements of the time, as well as his own personal and political development. Vogt was born in Giessen in 1817, into a family with both scientific and revolutionary traditions. His father, Philipp Friedrich Wilhelm Vogt, was a professor of medicine who moved to Bern in 1834 after facing political persecution. On his mother’s side, political activism was also a strong influence: Louise Follen’s three brothers—Adolf, Karl, and Paul Follen—were all involved in nationalist and democratic causes and eventually went into exile.\nIn 1817 Adolf Follen drafted a proposal for a future German constitution and was later arrested for his political activities. He avoided a 10-year prison sentence by fleeing to Switzerland. Karl Follen was suspected of encouraging the assassinat",
    "source": "wikipedia",
    "title": "Materialism controversy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_26692522",
    "text": "The history of metamaterials begins with artificial dielectrics in microwave engineering as it developed just after World War II. Yet, there are seminal explorations of artificial materials for manipulating electromagnetic waves at the end of the 19th century.\nHence, the history of metamaterials is essentially a history of developing certain types of manufactured materials, which interact at radio frequency, microwave, and later optical frequencies.\nAs the science of materials has advanced, photonic materials have been developed which use the photon of light as the fundamental carrier of information. This has led to photonic crystals, and at the beginning of the new millennium, the proof of principle for functioning metamaterials with a negative index of refraction in the microwave- (at 10.5 Gigahertz) and optical  range. This was followed by the first proof of principle for metamaterial cloaking (shielding an object from view), also in the microwave range, about six years later. However, a cloak that can conceal objects across the entire electromagnetic spectrum is still decades away. Many physics and engineering problems need to be solved.\nNevertheless, negative refractive materials have led to the development of metamaterial antennas and metamaterial microwave lenses for miniature wireless system antennas which are more efficient than their conventional counterparts. Also, metamaterial antennas are now commercially available. Meanwhile, subwavelength focusing with the superlens is also a part of present-day metamaterials research.\n\nEarly wave studies\nClassical waves transfer energy without transporting matter through the medium (material). For example, waves in a pond do not carry the water molecules from place to place; rather the wave's energy travels through the water, leaving the water molecules in place. Additionally, charged particles, such as electrons and protons create electromagnetic fields when they move, and these fields transport the type of energy known as electromagnetic radiation, or light. A changing magnetic field will induce a changing electric field and vice versa—the two are linked. These changing fields form electromagnetic waves. Electromagnetic waves differ from mechanical waves in that they do not require a medium to propagate. This means that electromagnetic waves can travel not only through air and solid materials, but also through the vacuum of space.\nThe \"history of metamaterials\" can have a variety starting points depending on the properties of interest. Related early wave studies started in 1904 and progressed through more than half of the first part of the twentieth century. This early research included the relationship of the phase velocity to group velocity and the relationship of the wave vector and Poynting vector.\nIn 1904 the possibility of negative phase velocity accompanied by an anti-parallel  group velocity were noted by Horace Lamb (book: Hydrodynamics) and Arthur Schuster (Book: Intro to Optics). Howe",
    "source": "wikipedia",
    "title": "History of metamaterials",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_31743909",
    "text": "The history of the metric system began during the Age of Enlightenment with measures of length and weight derived from nature, along with their decimal multiples and fractions. The system became the standard of France and Europe within half a century.  Other measures with unity ratios were added, and the system went on to be adopted across the world.\nThe first practical realisation of the metric system came in 1799, during the French Revolution, after the existing system of measures had become impractical for trade, and was replaced by a decimal system based on the kilogram and the metre.  The basic units were taken from the natural world. The unit of length, the metre, was based on the dimensions of the Earth, and the unit of mass, the kilogram, was based on the mass of a volume of water of one litre (a cubic decimetre). Reference copies for both units were manufactured in platinum and remained the standards of measure for the next 90 years. After a period of reversion to the mesures usuelles due to unpopularity of the metric system, the metrication of France and much of Europe was complete by the 1850s.\nIn the middle of the 19th century, James Clerk Maxwell conceived a coherent system where a small number of units of measure were defined as base units, and all other units of measure, called derived units, were defined in terms of the base units. Maxwell proposed three base units for length, mass and time. Advances in electromagnetism in the 19th century necessitated additional units to be defined, and multiple incompatible systems of such units came into use; none could be reconciled with the existing dimensional system. The impasse was resolved by Giovanni Giorgi, who in 1901 proved that a coherent system that incorporated electromagnetic units required a fourth base unit, of electromagnetism.\nThe seminal 1875 Treaty of the Metre resulted in the fashioning and distribution of metre and kilogram artefacts, the standards of the future coherent system that became the SI, and the creation of an international body Conférence générale des poids et mesures or CGPM to oversee systems of weights and measures based on them.\nIn 1960, the CGPM launched the International System of Units (in French the Système international d'unités or SI) with six \"base units\": the metre, kilogram, second, ampere, degree Kelvin (subsequently renamed the \"kelvin\") and candela, plus 16 more units derived from the base units. A seventh base unit, the mole, and six other derived units were added later in the 20th century. During this period, the metre was redefined in terms of the speed of light, and the second was redefined based on the microwave frequency of a caesium atomic clock.\nDue to the instability of the international prototype of the kilogram, a series of initiatives were undertaken, starting in the late 20th century, to redefine the ampere, kilogram, mole and kelvin in terms of invariant constants of physics, ultimately resulting in the 2019 revision of the SI, whic",
    "source": "wikipedia",
    "title": "History of the metric system",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_74980602",
    "text": "Mihi itch or Mihisucht is the ambition to describe new species (or other taxa: subspecies, hybrids, genera, etc.) as a means to immortalize one's name. Mihi is the dative form of the Latin word ego, thus \"mihi itch\" means to satisfy one's egotistical  impulses. The expression appeared in print as early as 1884.\nA consequence of the Mihi itch may be the unwarranted description of new taxa, differing only slightly from already established taxa, leading to taxonomic inflation. A more extreme case may be termed taxonomic vandalism when a large number of species are described with limited scientific evidence.\n\nExamples\nLa \"nouvelle école\" in malacology, led by Jules René Bourguignat, was responsible for the description of hundreds of new species of molluscs in Europe at the end of the nineteen century.\nHarold St. John published 440 names in the genus Pandanus, which encompasses c. 600 accepted species, and 283 names in the genus Cyrtandra, which encompasses c. 700 accepted species.\nBetween 2000 and 2011, Raymond Hoser published 582 species names, and 340 generic names of animals (mostly reptiles).\nSee also\nTaxonomic vandalism\nTaxonomic inflation\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Mihi itch",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_166380",
    "text": "Natural history is a domain of inquiry involving organisms, including animals, fungi, and plants, in their natural environment, leaning more towards observational than experimental methods of study. A person who studies natural history is called a naturalist or natural historian.\nNatural history encompasses scientific research but is not limited to it. It involves the systematic study of any category of natural objects or organisms, so while it dates from studies in the ancient Greco-Roman world and the mediaeval Arabic world, through to European Renaissance naturalists working in near isolation, today's natural history is a cross-discipline umbrella of many specialty sciences; e.g., geobiology has a strong multidisciplinary nature.\n\nDefinitions\n\nHistory\n\nMuseums\nNatural history museums, which evolved from cabinets of curiosities, played an important role in the emergence of professional biological disciplines and research programs. Particularly in the 19th century, scientists began to use their natural history collections as teaching tools for advanced students and the basis for their own morphological research.\nSocieties\nThe term \"natural history\" alone, or sometimes together with archaeology, forms the name of many national, regional, and local natural history societies that maintain records for animals—including birds (ornithology), insects (entomology) and mammals (mammalogy)—fungi (mycology), plants (botany), and other organisms. They may also have geological and microscopical sections.\nExamples of these societies in Britain include the Natural History Society of Northumbria founded in 1829, London Natural History Society (1858), Birmingham Natural History Society (1859), British Entomological and Natural History Society founded in 1872, Glasgow Natural History Society, Manchester Microscopical and Natural History Society established in 1880, Whitby Naturalists' Club founded in 1913, Scarborough Field Naturalists' Society and the Sorby Natural History Society, Sheffield, founded in 1918. The growth of natural history societies was also spurred due to the growth of British colonies in tropical regions with numerous new species to be discovered. Many civil servants took an interest in their new surroundings, sending specimens back to museums in the Britain. (See also: Indian natural history)\nSocieties in other countries include the American Society of Naturalists and Polish Copernicus Society of Naturalists. The Ecological Society of America launched its \"Natural History Section\" in 2010, using the tagline \"the heart and soul of ecology.\"\nProfessional societies have recognized the importance of natural history and have initiated new sections in their journals specifically for natural history observations to support the discipline. These include \"Natural History Field Notes\" of Biotropica, \"The Scientific Naturalist\" of Ecology, \"From the Field\" of Waterbirds, and the \"Natural History Miscellany section\" of the American Naturalist.\n",
    "source": "wikipedia",
    "title": "Natural history",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_10804088",
    "text": "Natural magic in the context of Renaissance magic is that part of the occult which deals with natural forces directly, as opposed to ceremonial magic which deals with the summoning of spirits. Natural magic sometimes makes use of physical substances from the natural world such as stones or herbs.\nNatural magic so defined includes astrology, alchemy, and certain disciplines that would today be considered fields of natural science, such as astronomy and chemistry (divergently evolved from astrology and alchemy, respectively) or botany (from herbology). Jesuit scholar Athanasius Kircher wrote that \"there are as many types of natural magic as there are subjects of applied sciences\".\nHeinrich Cornelius Agrippa discusses natural magic in his Three Books of Occult Philosophy (1533), where he calls it \"nothing else but the highest power of natural sciences\". The Italian Renaissance philosopher Giovanni Pico della Mirandola, who founded the tradition of Christian Kabbalah, argued that natural magic was \"the practical part of natural science\" and was lawful rather than heretical.\n\nSee also\nKitāb al-nawāmīs – Arabic book of magic\nGiambattista della Porta – Italian polymath (1535–1615)\nMagia Naturalis – Book by Giambattista della Porta\nProtoscience – Research field that may become a science\nThomas Vaughan – Welsh philosopher (1621–1666)\nWhite magic – Magic used for selfless purposes\nReferences\n\nFurther reading\nNauert, Charles G. (1957). \"Magic and Skepticism in Agrippa's Thought\". Journal of the History of Ideas: 176.\nStark, Ryan J. (2009). Rhetoric, Science, and Magic in Seventeenth-Century England. Washington, DC: The Catholic University of America Press.\nExternal links\n The dictionary definition of natural magic at Wiktionary.\n",
    "source": "wikipedia",
    "title": "Natural magic",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_382251",
    "text": "Natural philosophy, philosophy of nature (from Latin philosophia naturalis), or experimental philosophy is the philosophical study of nature and the physical universe, while ignoring any supernatural influence. Until the late modern period, the term natural philosophy referred to the branch of philosophy (a broader term then, meaning all rational fields of study and contemplation) that explored topics now considered scientific, such as physics, biology, and astronomy. Thus, natural philosophy served as the precursor to, and has been mostly supplanted by, modern science.\nFrom the ancient world (at least since Aristotle) until the 19th century, natural philosophy was the common term for the study of physics (nature), a broad term that included botany, zoology, anthropology, and chemistry as well as what is now called physics. It was in the 19th century that the concept of science received its modern shape, with different subjects within science emerging, such as astronomy, biology, and physics. Institutions and communities devoted to science were founded. Isaac Newton's book Philosophiæ Naturalis Principia Mathematica (1687) (English: Mathematical Principles of Natural Philosophy) reflects the use of the term natural philosophy in the 17th century. Even in the 19th century, the work that helped define much of modern physics bore the title Treatise on Natural Philosophy (1867).\nIn the German tradition, Naturphilosophie (philosophy of nature) persisted into the 18th and 19th centuries as an attempt to achieve a speculative unity of nature and spirit, after rejecting the scholastic tradition and replacing Aristotelian metaphysics, along with those of the dogmatic churchmen, with Kantian rationalism. Some of the greatest names in German philosophy are associated with this movement, including Goethe, Hegel, and Schelling. Naturphilosophie was associated with Romanticism and a view that regarded the natural world as a kind of giant organism, as opposed to the philosophical approach of figures such as John Locke and others espousing a more mechanical philosophy of the world, regarding it as being like a machine.\n\nOrigin and evolution of the term\nThe term natural philosophy preceded current usage of natural science (i.e. empirical science). Empirical science historically developed out of philosophy or, more specifically, natural philosophy. Natural philosophy was distinguished from the other precursor of modern science, natural history, in that natural philosophy involved reasoning and explanations about nature (and after Galileo, quantitative reasoning), whereas natural history was essentially qualitative and descriptive.\nGreek philosophers defined natural philosophy as the combination of beings living in the universe, ignoring things made by humans. The other definition refers to human nature.\nIn the 14th and 15th centuries, natural philosophy was one of many branches of philosophy, but was not a specialized field of study. The first person appointed as ",
    "source": "wikipedia",
    "title": "Natural philosophy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_17902329",
    "text": "The one-sex and two-sex models are historiographical concepts introduced by historian Thomas Laqueur in his 1990 book Making Sex: Body and Gender from the Greeks to Freud. Laqueur proposed that Western medical and philosophical thought underwent a fundamental shift in the 18th century: from a \"one-sex model\" in which female anatomy was understood as an inverted, inferior version of male anatomy, to a \"two-sex model\" treating men and women as anatomically distinct and opposite. While Making Sex has been highly influential across academic disciplines, Laqueur's thesis has attracted substantial criticism from historians of medicine and science who argue that it oversimplifies the historical record, misreads primary sources, and imposes an artificial chronological divide.\n\nLaqueur's thesis\n\nCriticism\nLaqueur's thesis has been subject to extensive criticism from historians of medicine and science. Critics have challenged both his reading of primary sources and the accuracy of his proposed chronology.\nInfluence and legacy\nDespite scholarly criticisms, Making Sex has remained highly influential, particularly in gender studies, literary criticism, and cultural history. The book helped establish the broader argument that sex, like gender, is historically and culturally constructed rather than a timeless biological given. This insight has proven productive for scholars even when they reject Laqueur's specific historical claims.\nThe ongoing debate over Making Sex also illustrates broader methodological questions in the history of science: how to interpret historical texts without imposing modern categories, how to balance sweeping narratives against the complexity of historical evidence, and how disciplinary popularity can sustain a thesis despite sustained criticism from specialists.\nSee also\nHistory of biology\nHistory of medicine\nHistory of sexuality\nSex and gender distinction\nSocial construction of gender\nThomas Laqueur\nReferences\n\nFurther reading\nFletcher, Anthony (1995). Gender, Sex and Subordination in England 1500–1800. New Haven: Yale University Press. ISBN 978-0-300-06531-2.\nHarvey, Karen (2002). \"The Century of Sex? Gender, Bodies, and Sexuality in the Long Eighteenth Century\". The Historical Journal. 45 (4): 899–916. doi:10.1017/S0018246X02002728.\nSchiebinger, Londa (1993). Nature's Body: Gender in the Making of Modern Science. Boston: Beacon Press. ISBN 978-0813535319.\n",
    "source": "wikipedia",
    "title": "One-sex and two-sex theories",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_31657115",
    "text": "An Oral History of British Science is an oral history project conducted by National Life Stories at the British Library.  The project began in 2009 with funding from the Arcadia Fund, the Royal Commission for the Exhibition of 1851 and a number of other private donors and focuses on audio interviews with British science and engineering figures.\n\nProject background\nThe project focused on 200 video interviews lasting 8–15 hours, with four themes: Made in Britain, A Changing Planet, Cosmologies and Biomedicine.   The project Advisory Committee included Jon Agar, Alec Broers, Tilly Blyth, Georgina Ferry, Dame Julia Higgins, Maja Kominko, Sir Harry Kroto, John Lynch, Chris Rapley and Simone Turchetti.\nAn Oral History of British Science was conducted by National Life Stories (NLS) at the British Library, and formed part of a wider institutional initiative to better document contemporary history of science and technology through the addition of audio visual sources as well as written sources.\nMethodology\nThe oral history of British science follows the biographical, or life story, oral history approach with each audio interview averaging 8 to 15 hours in length. The interviews cover the individual’s career history, education, background and family.\nAccess to interviews\nAll interviews are catalogued on the Sound and Moving Image Catalogue.  Interviews which are complete and open are accessible onsite at the Library in St Pancras, London and in Boston Spa, Yorkshire via the Library’s Listening & Viewing Service.   Interviews which are open are also made accessible via the Archival Sound Recordings website under the ‘Oral history of British science’ content package.\nPeople interviewed\nInterviewed for ‘A Changing Planet’:\n\nBarbara Bowen (Geophysics technician/ research assistant)\nJoe Farman (Geophysicist)\nJohn Glen (Glaciologist)\nA.T. (Dick) Grove (Geographer/ geomorphologist)\nDavid Jenkinson (Soil Scientist)\nDesmond King-Hele (Physicist)\nJohn Kington (Meteorologist and climatologist)\nJames Lovelock (Geochemist)\nMelvyn Mason (Technician in seismic refraction)\nDan McKenzie (geophysicist)\nStephen Moorbath (Geologist and Geochronologist)\nJohn Nye (scientist) (Physicist, Theoretical glaciologist)\nCharles Swithinbank (Glaciologist)\nJanet Thomson (Geologist)\nSue Vine (Geophysicist technician/ research assistant)\nRichard West (Botanist and Quaternary Geologist)\nInterviewed for ‘Made in Britain’:\n\nRaymond Bird (Computer Engineer)\nTony Brooker (Computer Scientist)\nMary Coombs (Computer Programmer)\nSir Alan Cottrell (Metallurgist and Physicist)\nDai Edwards (Computer Engineer);\nRoy Gibson (Aerospace Engineer)\nAndy Hopper (Computer Engineer)\nFrank Land (Computer Scientist)\nBob Parkinson (Aerospace Engineer)\nDame Stephanie Shirley (Computer Scientist)\nGeoff Tootill (Computer Engineer)\nMaurice Wilkes (Computer Engineer)\nInterviewed under ‘Biomedicine’:\n\nSammy Lee (scientist) (Clinical embryologist)\n",
    "source": "wikipedia",
    "title": "Oral History of British Science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_36506774",
    "text": "\"The Parable of the Sunfish\" is an anecdote with which Ezra Pound opens ABC of Reading, a 1934 work of literary criticism. Pound uses this anecdote to emphasize an empirical approach for learning about art, in contrast to relying on commentary rooted in abstraction. While the parable is based on students' recollections of Louis Agassiz's teaching style, Pound's retelling diverges from these sources in several respects. The parable has been used to illustrate the benefits of scientific thinking, but more recent literary criticism has split on whether the parable accurately reflects the scientific process and calls into question Pound's empirical approach to literature.\n\nThe Parable\nThe text of the parable below is excerpted from Pound's ABC of Reading.\nContext\n\nSources\nLouis Agassiz was a Swiss-born scientist at Harvard University who, by 1896, had established a reputation for \"lock[ing] a student up in a room full of turtle-shells, or lobster-shells, or oyster-shells, without a book or a word to help him, and not let[ting] him out till he had discovered all the truths which the objects contained.\" Several students of Agassiz who went on to prominence recorded this rite of passage, including Henry Blake, David Starr Jordan, Addison Emery Verrill, and Burt Green Wilder. American literary critic Robert Scholes traces the parable's source to two narratives in particular: those of former students Nathaniel Southgate Shaler and Samuel Hubbard Scudder. Their anecdotes were reprinted in Lane Cooper's Louis Agassiz as a Teacher: Illustrative Extracts on his Method of Instruction. Their separate accounts differ markedly from Pound's: both students provide oral reports with a wealth of detail after being initially forbidden from consulting outside sources.\nInterpretation and criticism\n\nNotes\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Parable of the Sunfish",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_35659147",
    "text": "Patterns in nature are visible regularities of form found in the natural world. These patterns recur in different contexts and can sometimes be modelled mathematically. Natural patterns include symmetries, trees, spirals, meanders, waves, foams, tessellations, cracks and stripes. Early Greek philosophers studied pattern, with Plato, Pythagoras and Empedocles attempting to explain order in nature. The modern understanding of visible patterns developed gradually over time.\nIn the 19th century, the Belgian physicist Joseph Plateau examined soap films, leading him to formulate the concept of a minimal surface. The German biologist and artist Ernst Haeckel painted hundreds of marine organisms to emphasise their symmetry. Scottish biologist D'Arcy Thompson pioneered the study of growth patterns in both plants and animals, showing that simple equations could explain spiral growth. In the 20th century, the English mathematician Alan Turing predicted mechanisms of morphogenesis which give rise to patterns of spots and stripes. The Hungarian biologist Aristid Lindenmayer and the French American mathematician Benoît Mandelbrot showed how the mathematics of fractals could create plant growth patterns.\nMathematics, physics and chemistry can explain patterns in nature at different levels and scales. Patterns in living things are explained by the biological processes of natural selection and sexual selection. Studies of pattern formation make use of computer models to simulate a wide range of patterns.\n\nHistory\nEarly Greek philosophers attempted to explain order in nature, anticipating modern concepts. Pythagoras (c. 570–c. 495 BC) explained patterns in nature like the harmonies of music as arising from number, which he took to be the basic constituent of existence. Empedocles (c. 494–c. 434 BC) to an extent anticipated Darwin's evolutionary explanation for the structures of organisms. Plato (c. 427–c. 347 BC) argued for the existence of natural universals. He considered these to consist of ideal forms (εἶδος eidos: \"form\") of which physical objects are never more than imperfect copies. Thus, a flower may be roughly circular, but it is never a perfect circle. Theophrastus (c. 372–c. 287 BC) noted that plants \"that have flat leaves have them in a regular series\"; Pliny the Elder (23–79 AD) noted their patterned circular arrangement. Centuries later, Leonardo da Vinci (1452–1519) noted the spiral arrangement of leaf patterns, that tree trunks gain successive rings as they age, and proposed a rule purportedly satisfied by the cross-sectional areas of tree-branches.\nIn 1202, Leonardo Fibonacci introduced the Fibonacci sequence to the western world with his book Liber Abaci. Fibonacci presented a thought experiment on the growth of an idealized rabbit population. Johannes Kepler (1571–1630) pointed out the presence of the Fibonacci sequence in nature, using it to explain the pentagonal form of some flowers. In 1658, the English physician and philosopher Sir Thomas B",
    "source": "wikipedia",
    "title": "Patterns in nature",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_48799229",
    "text": "Physica speculatio is a text of scientific character written by Alonso de la Vera Cruz in 1557 in the capital of New Spain. It was the first published work in the American continent that specifically addressed the study of physics, and was written to teach the students of the Real University of Mexico.\nIt introduced the main theoretical concepts of geocentric astronomy and references the heliocentric model.\nFray Alonso de la Vera Cruz published in the capital of New Spain a Course of Arts, constituted in three volumes in Latin. The first form in 1553 under the title of Recognitio Summularum, that had like purpose help to the students of the Real University of Mexico to understand the philosophy by means of the understanding of the formal logic. A year afterwards appeared the second called Dialectica Resolutio, that was a continuation of the previous. The last was Physica speculatio.\nThey did  four editions, the last 3 of which were for use of the salmantino students and were abbreviated versions of the Mexican one.\n\nSubjects\nThe Physica speculatio has by object the study or \"investigation\" -speculatio- and the exhibition, in general, of subjects of physics  on the nature -Physica-, treated by fray Alonso de la Vera Cruz basically from the philosophical perspective, characteristic of Aristotle and traditional in the Half Age.\nIt talks about, in what can be considered like the first part, the subjects treated by Aristotle in the Eight books of physics, as they are the essence of the physical or natural being, the movement and the infinite, the extension, the continuous, the space, the time, the first engine, etc. The second part treats of the subjects of the generation and the corruption of the living beings, of the mixed and composed being, of the primary qualities and of the elements and their properties. In the third part it exposes the doctrines on the meteors, it talks about the stars and their influence on humans, of the three regions of the air or atmosphere, of the comets, of the tides, of the ray and of a lot of other atmospheric phenomena. The fourth part devotes fray Alonso to comment the books De Anima by Aristotle. To end the Physica speculatio, there are some reflections on the treatise De Caelo by Aristotle.\n",
    "source": "wikipedia",
    "title": "Physica speculatio",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1692795",
    "text": "The Physics (Ancient Greek: Φυσικής ἀκρόασις, romanized: Physikḗs akróasis, or: Φυσικής ακροάσεως, Physikḗs akroáseōs; Latin: Physica or Naturales Auscultationes, possibly meaning \"Lectures on nature\") is a named text, written in ancient Greek, collated from a collection of surviving manuscripts known as the Corpus Aristotelicum, attributed to the 4th-century BC philosopher Aristotle.\n\nThe meaning of physics in Aristotle\nIt is a collection of treatises or lessons that deals with the most general (philosophical) principles of natural or moving things, both living and non-living, rather than physical theories (in the modern sense) or investigations of the particular contents of the universe. The chief purpose of the work is to discover the principles and causes of (and not merely to describe) change, or movement, or motion (κίνησις kinesis), especially that of natural wholes (mostly living things, but also inanimate wholes like the cosmos). In the conventional Andronicean ordering of Aristotle's works, it stands at the head of, as well as being foundational to, the long series of physical, cosmological and biological treatises, whose ancient Greek title, τὰ φυσικά, means \"the [writings] on nature\" or \"natural philosophy\".\nDescription of the content\nThe Physics is composed of eight books, which are further divided into chapters. This system is of ancient origin, now obscure. In modern languages, books are referenced with Roman numerals, standing for ancient Greek capital letters (the Greeks represented numbers with letters, e.g. A for 1). Chapters are identified by Arabic numerals, but the use of the English word \"chapter\" is strictly conventional. Ancient \"chapters\" (capita) are generally very short, often less than a page. Additionally, the Bekker numbers give the page and column (a or b) used in the Prussian Academy of Sciences' edition of Aristotle's works, instigated and managed by Bekker himself. These are evident in the 1831 2-volume edition. Bekker's line numbers may be given. These are often given, but unless the edition is the Academy's, they do match any line counts.\n",
    "source": "wikipedia",
    "title": "Physics (Aristotle)",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_75138286",
    "text": "Pictet's experiment is the demonstration of the reflection of heat and the apparent reflection of cold in a series of experiments performed in 1790 (reported in English in 1791 in An Essay on Fire) by Marc-Auguste Pictet—ten years before the discovery of infrared heating of the Earth by the Sun. The apparatus for most of the experiments used two concave mirrors facing one another at a distance. An object placed at the focus of one mirror would have heat and light reflected by the mirror and focused. An object at the focus of the counterpart mirror would do the same. Placing a hot object at one focus and a thermometer at the other would register an increase in temperature on the thermometer. This was sometimes demonstrated with the explosion of a flammable mix of gasses in a blackened balloon, as described and depicted by John Tyndall in 1863.\nAfter \"demonstrating that radiant heat, even when it was not accompanied by any light, could be reflected and focused like light\", Pictet used the same apparatus to demonstrate the apparent reflection of cold in a similar manner. This demonstration was important to Benjamin Thompson, Count Rumford who argued for the existence of \"frigorific rays\" conveying cold. Rumford's continuation of the experiments and promotion of the topic caused the name to be attached to the experiment.\nThe apparent reflection of cold if a cold object is placed in one focus surprised Pictet and two scholars writing about the experiment in 1985 noted \"most physicists, on seeing it demonstrated for the first time, find it surprising and even puzzling.\" The confusion may be resolved by understanding that all objects in the system—including the thermometer—are constantly radiating heat. Pictet described this as \"the thermometer acts the same part relatively to the snow as the bullet [heat source] in relation to the thermometer.\" Addition of a very cold object adds an effective heat sink versus a room temperature object which would not, in the net, cool or warm a thermometer in the other focus.\n\nModern replications and demonstrations\nThere are relatively few published examples of demonstrations or recreation of the experiment. Two physicists in the University of Washington system reported on demonstrations to students and colleagues and produced directions for re-creating the experiment in 1985 as part of an investigation into the role of the experiment in the history of physics. Physicists at Sofia University in Bulgaria reported on reproducing the experiment for high school students in 2017.\n",
    "source": "wikipedia",
    "title": "Pictet's experiment",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_31732740",
    "text": "Of the 118 chemical elements, 41 are named after, or have names associated with, places around the world or among astronomical objects. 32 of these have names tied to the Earth and the other 10 have names connected to bodies in the Solar System.\nThe first table below lists terrestrial locations (excluding the entire Earth taken as a whole) and the last table lists astronomical objects which the chemical elements are named after.\n\nTerrestrial locations\n\nAstronomical objects\n* - The element mercury was named directly for the deity, with only indirect naming connection to the planet (see etymology of mercury).\n** - Phosphorus was the Ancient Greek name for the planet Venus. (see history of phosphorus).\nSee also\nList of chemical elements named after people\nList of chemical element name etymologies\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "List of chemical elements named after places",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_5788461",
    "text": "Through history, the systems of economic support for scientists and their work have been important determinants of the character and pace of scientific research.  The ancient foundations of the sciences were driven by practical and religious concerns and or the pursuit of philosophy more generally.  From the Middle Ages until the Age of Enlightenment, scholars sought various forms of noble and religious patronage or funded their own work through medical practice.  In the 18th and 19th centuries, many disciplines began to professionalize, and both government-sponsored \"prizes\" and the first research professorships at universities drove scientific investigation.  In the 20th century, a variety of sources, including government organizations, military funding, patent profits, corporate sponsorship, and private philanthropies, have shaped scientific research.\n\nAncient science\nMost early advances in mathematics, astronomy and engineering were byproducts of more immediate and practical goals.  Surveying and accounting needs drove ancient Egyptian, Babylonian, Chinese, and Indian mathematics, while calendars created for religious and agricultural purposes drove early astronomy.\nModern science owes much of its heritage to ancient Greek philosophers; influential work in astronomy, mechanics, geometry, medicine, and natural history was part of the general pursuit of philosophy.  Architectural knowledge, especially in ancient Greece and Rome, also contributed to the development of mathematics, though the extent of the connection between architectural knowledge and more abstract mathematics and mechanics is unclear.\nState policy has influenced the funding of public works and science for thousands of years, dating at least from the time of the Mohists, who inspired the study of logic  during the period of the Hundred Schools of Thought, and the study of defensive fortifications during the Warring States period in China. General levies of labor and grain were collected to fund great public works in China, including the accumulation of grain for distribution in times of famine, for the building of levees to control flooding by the great rivers of China, for the building of canals and locks to connect rivers of China, some of which flowed in opposite directions to each other, and for  the building of bridges across these rivers. These projects required a civil service, the scholars, some of whom demonstrated great mastery of hydraulics.\n",
    "source": "wikipedia",
    "title": "History of science policy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_24241",
    "text": "In the philosophy of science, protoscience (adj. protoscientific) is a research field that has the characteristics of an undeveloped science that may ultimately develop into an established science.  Philosophers use protoscience to understand the history of science and distinguish protoscience from science and pseudoscience.\nThe word \"protoscience\" is a hybrid Greek-Latin compound of the roots proto- + scientia, meaning a first or primeval rational knowledge.\nExamples of protoscience include alchemy, Wegener's original theory of continental drift and political economy (the predecessor to the modern economic sciences).\n\nHistory\nProtoscience as a research field with the characteristics of an undeveloped science appeared in the early 20th century. In 1910, Jones described the field of political economy as it began the transition to the modern field of economics:\n\nI confess to a personal predilection for some term such as proto-science, pre-science, or nas-science, to give expression to what I conceive to be the true state of affairs, which I take to be this, that economics and kindred subjects are not sciences, but are on the way to become sciences.\nThomas Kuhn later provided a more precise description, protoscience as a field that generates testable conclusions, faces \"incessant criticism and continually strive for a fresh start,\" but currently, like art and philosophy, appears to have failed to progress in a way similar to the progress seen in the established sciences.  He applies protoscience to the fields of natural philosophy, medicine and the crafts in the past that ultimately became established sciences.  Philosophers later developed more precise criteria to identify protoscience using the cognitive field concept.\nThe historian Scott Hendrix argued that the English word \"science\" as it is used by 21st century English speakers means modern science and that the use of the word to describe pre-modern scholars is misleading. \"[E]ven an astute reader is prompted to classify intellectual exercises of the past as 'scientific'...based upon how closely those activities appear to mirror the activities of a modern scientist.\" Noting that natural philosophy was a far more neutral term than \"science\", Hendrix recommended that term be used instead when discussing pre-modern scholars of the natural world. \"[T]here are sound reasons for a return to the use of the term natural philosophy that, for all its imprecision, reveals rather than imposes meaning on the past.\"\n",
    "source": "wikipedia",
    "title": "Protoscience",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1674353",
    "text": "The history of pseudoscience is the study of pseudoscientific theories over time. A pseudoscience is a set of ideas that presents itself as science, while it does not meet the criteria to properly be called such.\nDistinguishing between proper science and pseudoscience is sometimes difficult. One popular proposal for demarcation between the two is the falsification criterion, most notably contributed to by the philosopher Karl Popper. In the history of pseudoscience it can be especially hard to separate the two, because some sciences developed from pseudosciences. An example of this is the science chemistry, which traces its origins from the protoscience of alchemy.\nThe vast diversity in pseudosciences further complicates the history of pseudoscience. Some pseudosciences originated in the pre-scientific era, such as astrology and acupuncture. Others developed as part of an ideology, such as Lysenkoism, or as a response to perceived threats to an ideology. An example of this is creationism, which was developed as a response to the scientific theory of evolution.\nDespite failing to meet proper scientific standards, many pseudosciences survive. This is usually due to a persistent core of devotees who refuse to accept scientific criticism of their beliefs, or due to popular misconceptions. Sheer popularity is also a factor, as is attested by astrology which remains popular despite being rejected by a large majority of scientists.\n\n19th century\nAmong the most notable developments in the history of pseudoscience in the 19th century are the rise of Spiritualism (traced in America to 1848), homeopathy (first formulated in 1796), and phrenology (developed around 1800). Another popular pseudoscientific belief that arose during the 19th century was the idea that there were canals visible on Mars. A relatively mild Christian fundamentalist backlash against the scientific theory of evolution foreshadowed subsequent events in the 20th century.\nThe study of bumps and fissures in people's skulls to determine their character, phrenology, was originally considered a science. It influenced psychiatry and early studies into neuroscience. As science advanced, phrenology was increasingly viewed as a pseudoscience. Halfway through the 19th century, the scientific community had prevailingly abandoned it, although it was not comprehensively tested until much later.\nHalfway through the century, iridology was invented by the Hungarian physician Ignaz von Peczely. The theory would remain popular throughout the 20th century as well.\n\nSpiritualism (sometimes referred to as \"Modern Spiritualism\" or \"Spiritism\")  or \"Modern American Spiritualism\" grew phenomenally during the period. The American version of this movement has been traced to the Fox sisters who in 1848 began claiming the ability to communicate with the dead. The religious movement would remain popular until the 1920s, when renowned magician Harry Houdini began exposing famous mediums and other performers as frauds ",
    "source": "wikipedia",
    "title": "History of pseudoscience",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_26476831",
    "text": "In ancient history, the concepts of chance and randomness were intertwined with that of fate. Many ancient peoples threw dice to determine fate, and this later evolved into games of chance. At the same time, most ancient cultures used various methods of divination to attempt to circumvent randomness and fate.  Beyond religion and games of chance, randomness has been attested for sortition since at least ancient Athenian democracy in the form of a kleroterion.\nThe formalization of odds and chance was perhaps earliest done by the Chinese 3,000 years ago. The Greek philosophers discussed randomness at length, but only in non-quantitative forms. It was only in the sixteenth century that Italian mathematicians began to formalize the odds associated with various games of chance. The invention of modern calculus had a positive impact on the formal study of randomness. In the 19th century the concept of entropy was introduced in physics.\nThe early part of the twentieth century saw a rapid growth in the formal analysis of randomness, and mathematical foundations for probability were introduced, leading to its axiomatization in 1933. At the same time, the advent of quantum mechanics changed the scientific perspective on determinacy. In the mid to late 20th-century, ideas of algorithmic information theory introduced new dimensions to the field via the concept of algorithmic randomness.\nAlthough randomness had often been viewed as an obstacle and a nuisance for many centuries, in the twentieth century computer scientists began to realize that the deliberate introduction of randomness into computations can be an effective tool for designing better algorithms. In some cases, such randomized algorithms are able to outperform the best deterministic methods.\n\nAntiquity to the Middle Ages\nPre-Christian people along the Mediterranean threw dice to determine fate, and this later evolved into games of chance. There is also evidence of games of chance played by ancient Egyptians, Hindus and\nChinese, dating back to 2100 BC. The Chinese used dice before the Europeans, and have a long history of playing games of chance.\nOver 3,000 years ago, the problems concerned with the tossing of several coins were considered in the I Ching, one of the oldest Chinese mathematical texts, that probably dates to 1150 BC. The two principal elements yin and yang were combined in the I Ching in various forms to produce Heads and Tails permutations of the type HH, TH, HT, etc. and the Chinese seem to have been aware of Pascal's triangle long before the Europeans formalized it in the 17th century. However, Western philosophy focused on the non-mathematical aspects of chance and randomness until the 16th century.\nThe development of the concept of chance throughout history has been very gradual. Historians have wondered why progress in the field of randomness was so slow, given that humans have encountered chance since antiquity. Deborah J. Bennett suggests that ordinary people face an inheren",
    "source": "wikipedia",
    "title": "History of randomness",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_42877569",
    "text": "The relationship between mathematics and physics has been a subject of study of philosophers, mathematicians and physicists since antiquity, and more recently also by historians and educators. Generally considered a relationship of great intimacy, mathematics has been described as \"an essential tool for physics\" and physics has been described as \"a rich source of inspiration and insight in mathematics\".\nSome of the oldest and most discussed themes are about the main differences between the two subjects, their mutual influence, the role of mathematical rigor in physics, and the problem of explaining the effectiveness of mathematics in physics.\nIn his work Physics, one of the topics treated by Aristotle is about how the study carried out by mathematicians differs from that carried out by physicists. Considerations about mathematics being the language of nature can be found in the ideas of the Pythagoreans: the convictions that \"Numbers rule the world\" and \"All is number\", and two millennia later were also expressed by Galileo Galilei: \"The book of nature is written in the language of mathematics\".\n\nHistorical interplay\nBefore giving a mathematical proof for the formula for the volume of a sphere, Archimedes used physical reasoning to discover the solution (imagining the balancing of bodies on a scale). Aristotle classified physics and mathematics as theoretical sciences, in contrast to practical sciences (like ethics or politics) and to productive sciences (like medicine or botany).\nFrom the seventeenth century, many of the most important advances in mathematics appeared motivated by the study of physics, and this continued in the following centuries (although in the nineteenth century mathematics started to become increasingly independent from physics). The creation and development of calculus were strongly linked to the needs of physics: There was a need for a new mathematical language to deal with the new dynamics that had arisen from the work of scholars such as Galileo Galilei and Isaac Newton. The concept of derivative was needed, Newton did not have the modern concept of limits, and instead employed infinitesimals, which lacked a rigorous foundation at that time. During this period there was little distinction between physics and mathematics; as an example, Newton regarded geometry as a branch of mechanics. \nNon-Euclidean geometry, as formulated by Carl Friedrich Gauss, János Bolyai, Nikolai Lobachevsky, and Bernhard Riemann, freed physics from the limitation of a single Euclidean geometry. A version of non-Euclidean geometry, called Riemannian geometry, enabled Albert Einstein to develop general relativity by providing the key mathematical framework on which he fit his physical ideas of gravity.\nIn the 19th century Auguste Comte in his hierarchy of the sciences, placed physics and astronomy as less general and more complex than mathematics, as both depend on it. In 1900, David Hilbert in his 23 problems for the advancement of mathematical s",
    "source": "wikipedia",
    "title": "Relationship between mathematics and physics",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_29266",
    "text": "The relationship between science and religion involves discussions that interconnect the study of the natural world, history, philosophy, and theology. Even though the ancient and medieval worlds did not have conceptions resembling the modern understandings of \"science\" or of \"religion\", certain elements of modern ideas on the subject recur throughout history. The pair-structured phrases \"religion and science\" and \"science and religion\" first emerged in the literature during  the 19th century. This coincided with the refining of \"science\" (from the studies of \"natural philosophy\") and of \"religion\" as distinct concepts in the preceding few centuries—partly due to professionalization of the sciences, the Protestant Reformation, colonization, and globalization. Since then the relationship between science and religion has been characterized in terms of \"conflict\", \"harmony\", \"complexity\", and \"mutual independence\", among others.\nBoth science and religion are complex social and cultural endeavors that may vary across cultures and change over time. Most scientific and technical innovations until the scientific revolution were achieved by societies organized by religious traditions. Ancient pagan, Islamic, and Christian scholars pioneered individual elements of the scientific method. Roger Bacon, often credited with formalizing the scientific method, was a Franciscan friar and medieval Christians who studied nature emphasized natural explanations. Confucian thought, whether religious or non-religious in nature, has held different views of science over time. Many 21st-century Buddhists view science as complementary to their beliefs, although the philosophical integrity of such Buddhist modernism has been challenged. While the classification of the material world by the ancient Indians and Greeks into air, earth, fire, and water was more metaphysical, and figures like Anaxagoras questioned certain popular views of Greek divinities, medieval Middle Eastern scholars empirically classified materials.\nEvents in Europe such as the Galileo affair of the early 17th century, associated with the scientific revolution and the Age of Enlightenment, led scholars such as John William Draper to postulate (c. 1874) a conflict thesis, suggesting that religion and science have been in conflict methodologically, factually, and politically throughout history. Some contemporary philosophers and scientists, such as Richard Dawkins, Lawrence Krauss, Peter Atkins, and Donald Prothero subscribe to this thesis; however, such views have not been held by historians of science for a very long time.\nMany scientists, philosophers, and theologians throughout history, from Augustine of Hippo to Thomas Aquinas to Francisco Ayala, Kenneth R. Miller, and Francis Collins, have seen compatibility or interdependence between religion and science. Biologist Stephen Jay Gould regarded religion and science as \"non-overlapping magisteria\", addressing fundamentally separate forms of knowledge and ",
    "source": "wikipedia",
    "title": "Relationship between science and religion",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_4175709",
    "text": "During the Renaissance, great advances occurred in geography, astronomy, chemistry, physics, mathematics, manufacturing, anatomy and engineering. The collection of ancient scientific texts began in earnest at the start of the 15th century and continued up to the Fall of Constantinople in 1453, and the invention of printing allowed a faster propagation of new ideas. Nevertheless, some have seen the Renaissance, at least in its initial period, as one of scientific backwardness. Historians like George Sarton and Lynn Thorndike criticized how the Renaissance affected science, arguing that progress was slowed for some amount of time. Humanists favored human-centered subjects like politics and history over study of natural philosophy or applied mathematics. More recently, however, scholars have acknowledged the positive influence of the Renaissance on mathematics and science, pointing to factors like the rediscovery of lost or obscure texts and the increased emphasis on the study of language and the correct reading of texts.\nMarie Boas Hall coined the term Scientific Renaissance to designate the period leading up to the Scientific Revolution. More recently, Peter Dear has argued for a two-phase model of early modern science: a Scientific Renaissance of the 15th and 16th centuries, focused on the restoration of the natural knowledge of the ancients; and a Scientific Revolution of the 17th century, when scientists shifted from recovery to innovation.\n\nContext\nDuring and after the Renaissance of the 12th century, Europe experienced an intellectual revitalization, especially with regard to the investigation of the natural world. In the 14th century, however, a series of events that would come to be known as the Crisis of the Late Middle Ages was underway. When the Black Death came, it wiped out so many lives it affected the entire system. It brought a sudden end to the previous period of massive scientific change. The plague killed 25–50% of the people in Europe, especially in the crowded conditions of the towns, where the heart of innovations lay. Recurrences of the plague and other disasters caused a continuing decline of population for a century.\n",
    "source": "wikipedia",
    "title": "Science in the Renaissance",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1107345",
    "text": "Many fields of scientific research in the Soviet Union were banned or suppressed with various justifications. All humanities and social sciences were tested for strict accordance with dialectical materialism. These tests served as a cover for political suppression of scientists who engaged in research labeled as \"idealistic\" or \"bourgeois\". Many scientists were fired, others were arrested and sent to Gulags. The suppression of scientific research began during the Stalin era and continued after his death.\nThe ideologically motivated persecution damaged many fields of Soviet science.\n\nExamples\n\nTheme in literature\nVladimir Dudintsev, White Garments (1987), a fictionalized story about Soviet geneticists working during the Lysenkoism era\nSee also\nAcademic freedom\nAntiscience\nAnti-intellectualism\nBourgeois pseudoscience\nCensorship in the Soviet Union\nDeutsche Physik\nFirst Department\nHistorical negationism\nPolitical correctness\nPoliticization of science\nScience and technology in the Soviet Union\nSoviet historiography\nAlexander Veselovsky, a case of suppressed literary research\nStalin and the Scientists\nReferences\n\nЯ. В. Васильков, М. Ю. Сорокина (eds.), Люди и судьбы. Биобиблиографический словарь востоковедов  жертв политического террора в советский период (1917–1991) (\"People and Destiny. Bio-Bibliographic Dictionary of Orientalists – Victims of the political terror during the Soviet period (1917–1991)\"),   Петербургское Востоковедение (2003). online edition\n",
    "source": "wikipedia",
    "title": "Repression of science in the Soviet Union",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_35114916",
    "text": "The role of chance, or \"luck\", in science comprises all ways in which unexpected discoveries are made.\nMany domains, especially psychology, are concerned with the way science interacts with chance –particularly \"serendipity\" (accidents that, through sagacity, are transformed into opportunity). Psychologist Kevin Dunbar and colleagues estimate that between 30% and 50% of all scientific discoveries are accidental in some sense (see examples below). Scientists themselves in the 19th and 20th century acknowledged the role of fortunate luck or serendipity in discoveries.\nPsychologist Alan A. Baumeister says a scientist must be \"sagacious\" (attentive and clever) to benefit from an accident. Dunbar quotes Louis Pasteur's saying that \"Chance favors only the prepared mind\". The prepared mind, Dunbar suggests, is one trained for observational rigor. Dunbar adds that there is a great deal of writing about the role that serendipity (\"happy accidents\") plays in the scientific method.\nResearch suggests that scientists are taught various heuristics and practices that allow their investigations to benefit, and not suffer, from accidents. First, careful control conditions allow scientists to properly identify something as \"unexpected\". Once a finding is recognized as legitimately unexpected and in need of explaining, researchers can attempt to explain it: They work across various disciplines, with various colleagues, trying various analogies in order to understand the first curious finding.\n\nPreparing to make discoveries\nAccidental discoveries have been a topic of discussion especially from the 20th century onwards. Kevin Dunbar and Jonathan Fugelsang say that somewhere between 33% and 50% of all scientific discoveries are unexpected. This helps explain why scientists often call their discoveries \"lucky\", and yet scientists themselves may not be able to detail exactly what role luck played (see also introspection illusion). Dunbar and Fugelsang believe scientific discoveries are the result of carefully prepared experiments, but also \"prepared minds\".\nThe author Nassim Nicholas Taleb calls science \"anti-fragile\". That is, science can actually use—and benefit from—the chaos of the real world. While some methods of investigation are fragile in the face of human error and randomness, the scientific method relies on randomness in many ways. Taleb believes that the more anti-fragile the system, the more it will flourish in the real world. According to  M. K. Stoskopf, it is in this way that serendipity is often the \"foundation for important intellectual leaps of understanding\" in science.\nThe word \"Serendipity\" is frequently understood as simply \"a happy accident\", but Horace Walpole used the word 'serendipity' to refer to a certain kind of happy accident: the kind that can only be exploited by a \"sagacious\" or clever person. Thus Dunbar and Fugelsang talk about, not just luck or chance in science, but specifically \"serendipity\" in science.\nDunbar and Fugelsang suggest",
    "source": "wikipedia",
    "title": "Role of chance in scientific discoveries",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_76454060",
    "text": "Roman Science: Origins, Development, and Influence to the Later Middle Ages is a book by science historian William Harris Stahl, published in 1962 by University of Wisconsin Press.\n\nSynopsis\nThis book covers the history of science in the Latin-speaking  West from its Greek origins to the time of the Graeco-Arabic revival, focusing on the influence of Greek science in the Latin world, and on how this influence shaped both scientific education and scientific culture all the way to the Middle Ages. The volume follows what the author calls \"the handbooks movement\", the production of encyclopedic material originating with Greek authors, such as Posidonius and Theon of Smyrna, and follows this tradition among the Romans. Stahl devotes specific chapters to Pliny, Solinus, Chalcidius, Macrobius, Capella, Boethius, Cassiodorus, Isidore, Bede, and other authors till about 1250, and discusses the genesis and subsequent development of the liberal arts in the Quadrivium and Trivium from the age of Plato (428–424 BC) and Isocrates (436–338 BC) till the Middle Ages and Renaissance.\nContent\nThe initial section on \"Classical Greek Origins\" treats the discoveries of Aristarchus of Samos, Pythagora, the Sophists Hippias of Elis and  Isocrates, Plato, the mathematician Eudoxus – credited with the invention of the Method of exhaustion – \nand Aristotle. The mathematicians Euclid, Archimedes, Apollonius of Perga and Hipparchus are described in the section of the early Hellenistic tradition, together with the early botanist Theophrastus who headed the Peripatetic school after Aristotle, and Eratosthenes of Cyrenes. The first section of the work of Stahl ends with a chapter entitled \"The Posidonian Age\", from Posidonius (c. 135-51 BC) that marks the period when a Greek, mostly Stoic tradition, opens a \"lengthy period of mutual admiration\"  between the Greek and Roman intellectuals. The chapter tells how the historian Polybius and the Stoic philosopher Panaetius were invited to the Scipionic Circle and of the friendship between Posidonius and Cicero. Though no work of Posidonius has reached us, his writings were used extensively by Cicero in his works, and influenced later authors such as Marcus Aurelius and  Seneca.  \nAuthors treated in the central section of Roman science, beside Pliny, include Cato the Elder, Cicero, Varro, Lucretius, Pomponius Mela, Vitruvius, Celsus, and Lucius Annaeus Seneca. Marcus Agrippa has a special mention for his approach of measuring the  length and breadth of each province of the Roman Empire by computing distances recorded on the milestones on the imperial highways. \nNicomachus and Apuleius are treated in the chapter on the second century AD, while Latin neoplatonist encyclopedists Solinus, Calcidius, Macrobius, and Martianus Capella are treated in the chapter on Third- and Fourth-Century Cosmography.\nThe last part of the volume describes the short period of Ostrogothic renaissance, with Boethius and Cassiodorus, then moves to Isidore of S",
    "source": "wikipedia",
    "title": "Roman Science: Origins, Development, and Influence to the Later Middle Ages",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_8255091",
    "text": "19th-century science was greatly influenced by Romanticism (or the Age of Reflection, c. 1800–1840), an intellectual movement that originated in Western Europe as a counter-movement to the late-18th-century Enlightenment.  Romanticism incorporated many fields of study, including politics, the arts, and the humanities.\n\nIn contrast to the Enlightenment's mechanistic natural philosophy, European scientists of the Romantic period held that observing nature implied understanding the self and that knowledge of nature \"should not be obtained by force\".  They felt that the Enlightenment had encouraged the abuse of the sciences, and they sought to advance a new way to increase scientific knowledge, one that they felt would be more beneficial not only to mankind but to nature as well.\nRomanticism advanced a number of themes: it promoted anti-reductionism (that the whole is more valuable than the parts alone) and epistemological optimism (man was connected to nature), and encouraged creativity, experience, and genius. It also emphasized the scientist's role in scientific discovery, holding that acquiring knowledge of nature meant understanding man as well; therefore, these scientists placed a high importance on respect for nature.\nRomanticism declined beginning around 1840 as a new movement, positivism, took hold of intellectuals, and lasted until about 1880.  As with the intellectuals who earlier had become disenchanted with the Enlightenment and had sought a new approach to science, people now lost interest in Romanticism and sought to study science using a stricter process.\n\nRomantic science vs. Enlightenment science\nAs the Enlightenment had a firm hold in France during the last decades of the 18th century, the Romantic view on science was a movement that flourished in Great Britain and especially Germany in the first half of the 19th century.  Both sought to increase individual and cultural self-understanding by recognizing the limits in human knowledge through the study of nature and the intellectual capacities of man.  The Romantic movement, however, resulted as an increasing dislike by many intellectuals for the tenets promoted by the Enlightenment; it was felt by some that Enlightened thinkers' emphasis on rational thought through deductive reasoning and the mathematization of natural philosophy had created an approach to science that was too cold and that attempted to control nature, rather than to peacefully co-exist with nature.\nAccording to the philosophes of the Enlightenment, the path to complete knowledge required dissection of information on any given subject and a division of knowledge into subcategories of subcategories, known as reductionism.  This was considered necessary in order to build upon the knowledge of the ancients, such as Ptolemy, and Renaissance thinkers, such as Copernicus, Kepler, and Galileo. It was widely believed that man's sheer intellectual power alone was sufficient to understanding every aspect of nature. Examples o",
    "source": "wikipedia",
    "title": "Romanticism in science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_66569125",
    "text": "The Royal Commission on Animal Magnetism involved two entirely separate and independent French Royal Commissions, each appointed by Louis XVI in 1784, that were conducted simultaneously by a committee composed of four physicians from the Paris Faculty of Medicine (Faculté de médecine de Paris) and five scientists from the Royal Academy of Sciences (Académie des sciences) (the \"Franklin Commission\", named for Benjamin Franklin), and a second committee composed of five physicians from the Royal Society of Medicine (Société Royale de Médecine) (the \"Society Commission\").\nEach Commission took five months to complete its investigations. The \"Franklin\" Report was presented to the King on 11 August 1784 – and was immediately published and very widely circulated throughout France and neighbouring countries – and the \"Society\" Report was presented to the King five days later on 16 August 1784.\nThe \"Franklin Commission's\" investigations are notable as a very early \"classic\" example of a systematic controlled trial, which not only applied \"sham\" and \"genuine\" procedures to patients with \"sham\" and \"genuine\" disorders, but, significantly, was the first to use the \"blindfolding\" of both the investigators and their subjects.\n\nThe report of the [\"Franklin\"] Royal Commission of 1784 ... is a masterpiece of its genre, and enduring testimony to the power and beauty of reason. ... Never in history has such an extraordinary and luminous group [as the \"Franklin Commission\"] been gathered together in the service of rational inquiry by  the methods of experimental science. For this reason alone the [Report of the \"Franklin Commission\"] ... is a key document in the history of human reason. It should be rescued from obscurity, translated into all languages, and reprinted by organizations dedicated to the unmasking of quackery and the defense of rational thought.\nBoth sets of Commissioners were specifically charged with investigating the claims made by Charles-Nicolas d’Eslon (1750–1786) for the existence of a substantial (rather than metaphorical) \"animal magnetism\", le magnétisme animal, and of a similarly (non-metaphorical) physical \"magnetic fluid\", le fluide magnétique. Further, having completed their investigations into the claims of d'Eslon – that is, they did not examine Franz Mesmer, Mesmer's theories, Mesmer's principles, Mesmer's practices, Mesmer's techniques, Mesmer's apparatus, Mesmer's claims, Mesmer's \"cures\" or, even, \"mesmerism\" itself – they were each required to make \"a separate and distinct report\".\n\nBefore the [\"Franklin\" Commission's] investigations began, [Antoine Lavoisier] had studied the writings of d'Eslon and [had] drawn up a plan for the conduct of the inquiry. He decided that the commissioners should not study any of the alleged cures, but [that] they should determine whether animal magnetism existed by trying to magnetize a person without his knowledge or making him think that he had been magnetized when in fact he had not. This plan was ad",
    "source": "wikipedia",
    "title": "Royal Commission on Animal Magnetism",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_80680176",
    "text": "The Sergey Vavilov Institute for the History of Science and Technology of the Russian Academy of Sciences (or Institute for the History of Science and Technology named after S. I. Vavilov RAS (IHIT or IIET RAS)) is the only research institute in Russia for the study of the history of science and technology. It managed by the Presidium of the Russian Academy of Sciences.\n\nHistory\nIn 1921, the Russian Academy of Sciences established the \"Commission for the Study of the History, Philosophy and Technology\" under the chairmanship of Vladimir Vernadsky (later renamed the Commission on the History of Knowledge).\nFrom 1930, the commission was chaired by N. I. Bukharin.\nStructure\nSt. Petersburg Branch (SPbF IHST RAS; heads: Boris Fedorenko (1953–1955), D.Biol.Sc. P. P. Perfiliev (1956–1962), D.Hist.Sc. A. V. Koltcov (1963–1966, 1972–1973), D.Phil.Sc. Yu. S. Meleshchenko (1967–1972), D.Med.Sc. N. A. Tolokontsev (1973–1975), D.Phil.Sc. B. I. Ivanov (1975–1978), Cand.Eng.Sc. E. P. Karpeev (1978–1987), D.Phil.Sc A. I. Melua (1987–1995), D.Phil.Sc. E. I. Kolchinsky (1995–2015), since 2015 — Cand.Soc.Sc. N. A. Ashcheulova).\nSince 2010, the Exhibition Center of the RAS has been a branch. The center organizes exhibitions of completed works by RAS institutions and the results of the most interesting fundamental research at Russian and international exhibitions in Russia, as well as exhibitions of works by the Russian Academy of Sciences at foreign exhibitions organized by Russian ministries and agencies, foreign companies, and organizations.\nJournals:\n\nStudies in the History of Science and Technology (VIET)\nStudies in the History of Biology (since 2009, quarterly)\nYearbooks:\n\nHistorico-Mathematical Research (since 1948)\nResearch on the History of Physics and Mechanics (since 1986)\nHistorico-Astronomical Research (since 1955).\nCouncils\nThe institute has several dissertation councils in the specialty \"history of science and technology\".\nIHST holds annual conferences on the history of science and technology in Moscow and St. Petersburg. The institute hosts several regular Moscow-wide seminars—on the history of astronomy, the history of physics and mechanics, and the history of the Soviet atomic project.\nIn 2004, the Academic Council and the Council of Young Scientists of IHST RAS established the \"Alexey Karimov Memorial Prize\", awarded to young scientists of the institute for significant contributions to the study of the history of science and technology.\n\nStructure\nDepartment of the History of Technology and Technical Sciences\nDepartment of the History of Physical and Mathematical Sciences\nCenter for the History of the Organization of Science and Science Studies (CHONS)\nEcological Center\nCenter for the History of Socio-Cultural Problems of Science and Technology\nDepartment of Historiography and Source Studies of the History of Science and Technology\nDepartment of Methodological and Social Problems of the Development of Science\nDepartment of the History of Chemical a",
    "source": "wikipedia",
    "title": "S.I. Vavilov Institute for the History of Science and Technology RAS",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_22734942",
    "text": "Africa has the world's oldest record of human technological achievement: the oldest surviving stone tools in the world have been found in eastern Africa, and later evidence for tool production by humans' hominin ancestors has been found across West, Central, Eastern and Southern Africa. The history of science and technology in Africa since then has, however, received relatively little attention compared to other regions of the world, despite notable African developments in mathematics, metallurgy, architecture, and other fields.\n\nEarly humans\nThe Great Rift Valley of Africa provides critical evidence for the evolution of early hominins. The earliest tools in the world can be found there as well:\n\nAn unidentified hominin, possibly Australopithecus afarensis or Kenyanthropus platyops, created stone tools dating to 3.3 million years ago at Lomekwi in the Turkana Basin, eastern Africa.\nHomo habilis, residing in eastern Africa, developed another early toolmaking industry, the Oldowan, around 2.3 million years ago.\nHomo erectus developed the Acheulean stone tool industry, specifically hand-axes, at 1.5 million years ago. This tool industry spread to the Middle East and Europe around 800,000 to 600,000 years ago. Homo erectus also begins using fire.\nHomo sapiens, or modern humans, created bone tools and backed blades around 90,000 to 60,000 years ago, in southern and eastern Africa. The use of bone tools and backed blades eventually became characteristic of Later Stone Age tool industries. The first appearance of abstract art is during the Middle Stone Age, however. The oldest abstract art in the world is a shell necklace dated to 82,000 years ago from the Cave of Pigeons in Taforalt, eastern Morocco. The second oldest abstract art and the oldest rock art is found at Blombos Cave in South Africa, dated to 77,000 years ago. There are evidences that Stone Age humans around 100,000 years ago had an elementary knowledge of chemistry in Southern Africa, and that they used a specific recipe to create a liquefied ochre-rich mixture. According to Henshilwood, \"This isn't just a chance mixture, it is early chemistry. It suggests conceptual and probably cognitive abilities which are the equivalent of modern humans\".\n",
    "source": "wikipedia",
    "title": "History of science and technology in Africa",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_8759532",
    "text": "Science and technology in Germany has a long and illustrious history, and research and development efforts form an integral part of the country's economy. Germany has been the home of some of the most prominent researchers in various scientific disciplines, notably physics, mathematics, chemistry and engineering. Before World War II, Germany had produced more Nobel laureates in scientific fields than any other nation, and was the preeminent country in the natural sciences. Germany is currently the nation with the 3rd most Nobel Prize winners, 115.\nThe German language, along with English and French, was one of the leading languages of science from the late 19th century until the end of World War II. After the war, because so many scientific researchers' and teachers' careers had been ended either by Nazi Germany which started a brain drain, the denazification process, the American Operation Paperclip and Soviet Operation Osoaviakhim which exacerbated the brain drain in post-war Germany, or simply losing the war, \"Germany, German science, and German as the language of science had all lost their leading position in the scientific community.\"\nToday, scientific research in the country is supported by industry, the network of German universities and scientific state-institutions such as the Max Planck Society and the Deutsche Forschungsgemeinschaft. The raw output of scientific research from Germany consistently ranks among the world's highest. Germany was declared the most innovative country in the world in the 2020 Bloomberg Innovation Index and  was ranked 11th in the Global Innovation Index in 2025.\n\nInstitutions\nThe Union of German Academies of Sciences and Humanities (abbreviated Academies Union) is an association of the eight largest academies of sciences in Germany.\nThe Deutsches Museum, 'German Museum' of Masterpieces of Science and Technology in Munich is one of the largest science and technology museums in the world in terms of exhibition space, with about 28,000 exhibited objects from 50 fields of science and technology.\nThe Bundesministerium für Bildung und Forschung, 'Federal Ministry of Education and Research' (BMBF) is a supreme authority of the Federal Republic of Germany for science and technology. The headquarter of the Federal Ministry is located in Bonn, the second office in Berlin. It was founded in 1972 as Federal Ministry of Research and Technology (BMFT) to promote basic research, applied research and technological development.\nFederal Ministry for Economic Affairs and Climate Action (German: Bundesministerium für Wirtschaft und Klimaschutz (BMWK, previous BMWi)\n",
    "source": "wikipedia",
    "title": "Science and technology in Germany",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_17912788",
    "text": "The history of science during the Age of Enlightenment traces developments in science and technology during the Age of Reason, when Enlightenment ideas and ideals were being disseminated across Europe and North America. Generally, the period spans from the final days of the 16th- and 17th-century Scientific Revolution until roughly the 19th century, after the French Revolution (1789) and the Napoleonic era (1799–1815). The scientific revolution saw the creation of the first scientific societies, the rise of Copernicanism, and the displacement of Aristotelian natural philosophy and Galen's ancient medical doctrine. By the 18th century, scientific authority began to displace religious authority, and the disciplines of alchemy and astrology lost scientific credibility.\nWhile the Enlightenment cannot be pigeonholed into a specific doctrine or set of dogmas, science came to play a leading role in Enlightenment discourse and thought. Many Enlightenment writers and thinkers had backgrounds in the sciences and associated scientific advancement with the overthrow of religion and traditional authority in favour of the development of free speech and thought. Broadly speaking, Enlightenment science greatly valued empiricism and rational thought, and was embedded with the Enlightenment ideal of advancement and progress. As with most Enlightenment views, the benefits of science were not seen universally; Jean-Jacques Rousseau criticized the sciences for distancing man from nature and not operating to make people happier.\nScience during the Enlightenment was dominated by scientific societies and academies, which had largely replaced universities as centres of scientific research and development. Societies and academies were also the backbone of the maturation of the scientific profession. Another important development was the popularization of science among an increasingly literate population. Philosophes introduced the public to many scientific theories, most notably through the Encyclopédie and the popularization of Newtonianism by Voltaire as well as by Émilie du Châtelet, the French translator of Newton's Philosophiæ Naturalis Principia Mathematica. Some historians have marked the 18th century as a drab period in the history of science; however, the century saw significant advancements in the practice of medicine, mathematics, and physics; the development of biological taxonomy; a new understanding of magnetism and electricity; and the maturation of chemistry as a discipline, which established the foundations of modern chemistry.\n\nUniversities\nThe number of universities in Paris remained relatively constant throughout the 18th century. Europe had about 105 universities and colleges by 1700. North America had 44, including the newly founded Harvard and Yale. The number of university students remained roughly the same throughout the Enlightenment in most Western nations, excluding Britain, where the number of institutions and students increased. University st",
    "source": "wikipedia",
    "title": "Science in the Age of Enlightenment",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_3143150",
    "text": "The history of scientific method considers changes in the methodology of scientific inquiry, as distinct from the history of science itself. The development of rules for scientific reasoning has not been straightforward; scientific method has been the subject of intense and recurring debate throughout the history of science, and eminent natural philosophers and scientists have argued for the primacy of one or another approach to establishing scientific knowledge.\nRationalist explanations of nature, including atomism, appeared both in ancient Greece in the thought of Leucippus and Democritus, and in ancient India, in the Nyaya, Vaisheshika and Buddhist schools, while Charvaka materialism rejected inference as a source of knowledge in favour of an empiricism that was always subject to doubt. Aristotle pioneered scientific method in ancient Greece alongside his empirical biology and his work on logic, rejecting a purely deductive framework in favour of generalisations made from observations of nature.\nSome of the most important debates in the history of scientific method center on: rationalism, especially as advocated by René Descartes; inductivism, which rose to particular prominence with Isaac Newton and his followers; and hypothetico-deductivism, which came to the fore in the early 19th century.  In the late 19th and early 20th centuries, a debate over realism vs. antirealism was central to discussions of scientific method as powerful scientific theories extended beyond the realm of the observable, while in the mid-20th century some prominent philosophers argued against any universal rules of science at all.\n\nEarly methodology\n\nEmergence of inductive experimental method\nDuring the Middle Ages issues of what is now termed science began to be addressed. There was greater emphasis on combining theory with practice in the Islamic world than there had been in Classical times, and it was common for those studying the sciences to be artisans as well, something that had been \"considered an aberration in the ancient world.\" Islamic experts in the sciences were often expert instrument makers who enhanced their powers of observation and calculation with them. Starting in the early ninth century, early Muslim scientists such as al-Kindi (801–873) and the authors writing under the name of Jābir ibn Hayyān (writings dated to c. 850–950) began to put a greater emphasis on the use of experiment as a source of knowledge. Several scientific methods thus emerged from the medieval Muslim world by the early 11th century, all of which emphasized experimentation as well as quantification to varying degrees.\n",
    "source": "wikipedia",
    "title": "History of scientific method",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_6231816",
    "text": "This is a list of priority disputes in history of science and science-related fields (such as mathematics).\n\nAstronomy\n1558 invention of the geoheliocentric system: Tycho Brahe, Nicolaus Raimarus Ursus\n1609–1610 Galilean moons: Galileo, Simon Marius\n1612 discovery of sunspots: Galileo Galilei, Christoph Scheiner\n1846 prediction of Neptune: Urbain Le Verrier, John Couch Adams\n2004–2005 controversy over the discovery of Haumea: José Luis Ortiz Moreno, Michael E. Brown.\nBiology and medicine\n1652 discovery of the lymphatic system: Olof Rudbeck, Thomas Bartholin\nc. 1660 teaching a deaf-mute person to speak: John Wallis, William Holder\nc. 1667 first human blood transfusion: Richard Lower, Henry Oldenburg, Jean-Baptiste Denys\nc. 1859 development of the theory of evolution: Charles Darwin, Alfred Russel Wallace, Patrick Matthew\n1877–1892 Bone Wars:  Edward Drinker Cope, Othniel Charles Marsh.\n1882–1889: Koch–Pasteur rivalry: Louis Pasteur, Robert Koch.\n1899–1902 discovery of the life cycle of malarial parasite: Giovanni Battista Grassi, Ronald Ross\n1953–1962 discovery of the DNA structure: Francis Crick, James D. Watson, Rosalind Franklin, Erwin Chargaff, Oswald Avery\n1971–1973 discovery of opiate receptors: Candace Pert, Solomon H. Snyder1971–1975 invention of magnetic resonance imaging (MRI): Paul Lauterbur, Peter Mansfield, Raymond Vahan Damadian, and others (see 2003 Nobel Prize in Physiology or Medicine)\n1983 discovery of HIV: Robert Gallo, Luc Montagnier (see 2008 Nobel Prize in Physiology or Medicine)\nChemistry\n1604-1777 discovery of oxygen: Joseph Priestley, Carl Wilhelm Scheele, Antoine Laurent Lavoisier\n1864 synthesis dicarboxylic acids from carboxylic acids (diacids from monoacid reactions): Hugo Müller, Hermann Kolbe, Hans Hübner, Friedrich Konrad Beilstein, Maxwell Simpson.\n1870–1895 development of the periodic table: Dmitri Mendeleev, Lothar Meyer\n1960–1994 Transfermium Wars: Lawrence Berkeley National Laboratory, Joint Institute for Nuclear Research.\nMathematics\n1550–1557 discovery of solutions to cubic equations: Niccolò Tartaglia, Gerolamo Cardano\n1669–1704 discovery of l'Hôpital's rule: Guillaume de l'Hôpital, Johann Bernoulli.\n1699–1716 Leibniz–Newton calculus controversy: Isaac Newton, Gottfried Leibniz\n1949 proof of the prime number theorem: Atle Selberg and/or Paul Erdős\n2002–2003 proof of the Poincaré conjecture: Grigori Perelman or Shing-Tung Yau\n",
    "source": "wikipedia",
    "title": "List of scientific priority disputes",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_64503842",
    "text": "The Second International Congress of the History of Science was held in London from June 29 to July 4, 1931. The Congress was organised by the International Committee of History of Science, in conjunction with the Comité International des Sciences Historiques. The History of Science Society and the Newcomen Society also supported the event. Charles Singer presided over the congress. Although organised by the International Committee of History of Science, it was during this congress that this organisation was transformed into an individual membership organisation called the International Academy of the History of Science.\nThe inaugural session was held in the Great Hall of the Royal Geographical Society. This was opened by Hastings Lees-Smith, President of the Board of Education. The rest of the congress was conducted in four sessions held in the lecture hall of the Science Museum.\n\nSessions\n\nReferences\n\nExternal links\nEn español: Pablo Huerga Melcón \"El Congreso de Londres de 1931\"\n[Kupriyanov, V.A. (2023). The Second Congress on the History of Science and Technology in London, 1931, in the history of science of the first half of the 20th century. Part I. The origin of the idea of international congresses on the history of science. The Digital Scholar: Philosopher's Lab, 6 (4): 127–144. DOI: 10.32326/2618-9267-2023-6-4-127-144]\n[Kupriyanov, V.A. (2023). The Second Congress on the History of Science and Technology in London, 1931 in the history of science of the first half of the XXth century. Part II. The holding of the congress and its significance for historiography.The Digital Scholar: Philosopher's Lab, 6 (4): 145–167. DOI: 10.32326/2618-9267-2023-6-4-145-167]\n",
    "source": "wikipedia",
    "title": "Second International Congress of the History of Science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_10981939",
    "text": "Small science (in contrast to big science) is science performed in a smaller scale, such as by individuals, small teams or within community projects.\nBodies which fund research, such as the National Science Foundation, DARPA, and the EU with its Framework programs, have a tendency to fund larger-scale research projects. Reasons include the idea that ambitious research needs significant resources devoted for its execution and the reduction of administrative and overhead costs on the funding body side. However, small science which has data that is often local and is not easily shared is funded in many areas such as chemistry and biology by these funding bodies.\n\nImportance\nSmall Science helps define the goals and directions of large scale scientific projects. In turn, results of large scale projects are often best synthesized and interpreted by the long-term efforts of the Small Science community. In addition, because Small Science is typically done at universities, it provides students and young researchers with an integral involvement in defining and solving scientific problems. Hence, small science can be seen as an important factor for bringing together science and society.\nAccording to the Chronicle for Higher Education,  James M. Caruthers, a professor of chemical engineering at Purdue University, data from Big Science is highly organized on the front end where researchers define it before it even starts rolling off the machines, making it easier to handle, understand, and archive. Small Science is \"horribly heterogeneous,\" and far more vast. In time, Small Science will generate two to three times more data than Big Science.\nThe American Geophysical Union stresses the importance of small science in a position statement.\nExamples of results with high impact\nMany historical examples show that results of Small Science can have enormous impacts:\n\nGalois theory, one of the foundational theories of abstract algebra was developed by Évariste Galois within just weeks.\nAlbert Einstein developed his theory of special relativity as a hobby while working full-time in a patent office.\nRobert Goddard invented the liquid propelled and multi stage rockets largely on his own.  These breakthroughs lead to the German V2 and the Apollo Saturn V rockets.\n",
    "source": "wikipedia",
    "title": "Small science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_56111973",
    "text": "Stalin and the Scientists: A History of Triumph and Tragedy 1905–1953 is a 2016 popular science non-fiction book on the history of science in the Soviet Union under Joseph Stalin by English novelist and science writer, Simon Ings. It is Ings' second non-fiction book, the first being The Eye: A Natural History (2007). He had previously published eight novels.\nStalin and the Scientists was longlisted for the 2016 Baillie Gifford Prize for Non-Fiction.\n\nBackground\nIngs' inspiration for Stalin and the Scientists came from Soviet psychologist, Alexander Luria's book Mind of a Mnemonist, about the life of Russian journalist and mnemonist, Solomon Shereshevsky. Ings said in 2016 interviews that Luria is often referred to as the founder of modern neuroanatomy and \"the godfather of the literary genre we call popular science\". \"Luria's account more or less set the template for modern popular science and ... pretty much set me on the path I'm on now.\" Ings had considered writing a biography about Luria, but felt that while Luria's achievements were \"extraordinary\", considering the climate of political repression he worked in, Ings was concerned that Western readers would consider his career too ordinary, and would miss the context in which it unfolded. Ings' passion for popular science and the need to explain the context within which Luria and other Soviet scientists worked, changed what would have been a one-year \"modest biography\" into a \"five-year behemoth\" that \"burned through three editors\" and, Ings added, \"nearly killed me\".\nIngs said, as a novelist, he was \"absurdly under-qualified\" to tackle a book like Stalin and the Scientists, but added that only a novelist could be so \"ridiculously ambitious\" and \"naive enough to stick his or her neck out so far\". Ings felt that given the kind of science prevalent in Russia at the time, perhaps this \"really has to be the job of a novelist rather than a historian\". Responding to statements that this is \"the first history\" of Soviet science, Ings said, \"Certainly no-one's been foolish enough to attempt to tell the whole story of science under Stalin in a single volume, but be assured I didn't dig this entire thing single-handed from virgin ground.\"\n",
    "source": "wikipedia",
    "title": "Stalin and the Scientists",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_224636",
    "text": "Supersymmetry is a theoretical framework in physics that suggests the existence of a symmetry between particles with integer spin (bosons) and particles with half-integer spin (fermions). It proposes that for every known particle, there exists a partner particle with different spin properties. There have been multiple experiments on supersymmetry that have failed to provide evidence that it exists in nature. If evidence is found, supersymmetry could help explain certain phenomena, such as the nature of dark matter and the hierarchy problem in particle physics. \nA supersymmetric theory is a theory in which the equations for force and the equations for matter are identical. In theoretical and mathematical physics, any theory with this property has the principle of supersymmetry (SUSY). Dozens of supersymmetric theories exist. In theory, supersymmetry is a type of spacetime symmetry between two basic classes of particles: bosons, which have an integer-valued spin and follow Bose–Einstein statistics, and fermions, which have a half-integer-valued spin and follow Fermi–Dirac statistics. The names of bosonic partners of fermions are prefixed with s-, because they are scalar particles. For example, if the electron existed in a supersymmetric theory, then there would be a particle called a selectron (superpartner electron), a bosonic partner of the electron.\nIn supersymmetry, each particle from the class of fermions would have an associated particle in the class of bosons, and vice versa, known as a superpartner. The spin of a particle's superpartner is different by a half-integer. In the simplest supersymmetry theories, with perfectly \"unbroken\" supersymmetry, each pair of superpartners would share the same mass and internal quantum numbers besides spin. More complex supersymmetry theories have a spontaneously broken symmetry, allowing superpartners to differ in mass.\nSupersymmetry has various applications to different areas of physics, such as quantum mechanics, statistical mechanics, quantum field theory, condensed matter physics, nuclear physics, optics, stochastic dynamics, astrophysics, quantum gravity, and cosmology. Supersymmetry has also been applied to high-energy physics, where a supersymmetric extension of the Standard Model is a possible candidate for physics beyond the Standard Model. However, no supersymmetric extensions of the Standard Model have been experimentally verified, and some physicists are saying the theory is dead.\n\nHistory\nA supersymmetry relating mesons and baryons was first proposed, in the context of hadronic physics, by Hironari Miyazawa in 1966. This supersymmetry did not involve spacetime, that is, it concerned internal symmetry, and was broken badly. Miyazawa's work was largely ignored at the time.\nJ. L. Gervais and B. Sakita (in 1971), Yu. A. Golfand and E. P. Likhtman (also in 1971), and D. V. Volkov and V. P. Akulov (1972), independently rediscovered supersymmetry in the context of quantum field theory, a radically n",
    "source": "wikipedia",
    "title": "Supersymmetry",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_71515213",
    "text": "The history of synthetic-aperture radar begins in 1951, with the invention of the technology by mathematician Carl A. Wiley, and its development in the following decade. Initially developed for military use, the technology has since been applied in the field of planetary science.\n\nInvention\nCarl A. Wiley, a mathematician at Goodyear Aircraft Company in Litchfield Park, Arizona, invented synthetic-aperture radar in June 1951 while working on a correlation guidance system for the Atlas ICBM program. In early 1952, Wiley, together with Fred Heisley and Bill Welty, constructed a concept validation system known as DOUSER (\"Doppler Unbeamed Search Radar\"). During the 1950s and 1960s, Goodyear Aircraft (later Goodyear Aerospace) introduced numerous advancements in SAR technology, many with help from Don Beckerleg.\nIndependently of Wiley's work, experimental trials in early 1952 by Sherwin and others at the University of Illinois' Control Systems Laboratory showed results that they pointed out \"could provide the basis for radar systems with greatly improved angular resolution\" and might even lead to systems capable of focusing at all ranges simultaneously.\nIn both of those programs, processing of the radar returns was done by electrical-circuit filtering methods. In essence, signal strength in isolated discrete bands of Doppler frequency defined image intensities that were displayed at matching angular positions within proper range locations. When only the central (zero-Doppler band) portion of the return signals was used, the effect was as if only that central part of the beam existed. That led to the term Doppler Beam Sharpening. Displaying returns from several adjacent non-zero Doppler frequency bands accomplished further \"beam-subdividing\" (sometimes called \"unfocused radar\", though it could have been considered \"semi-focused\"). Wiley's patent, applied for in 1954, still proposed similar processing. The bulkiness of the circuitry then available limited the extent to which those schemes might further improve resolution.\nThe principle was included in a memorandum authored by Walter Hausz of General Electric that was part of the then-secret report of a 1952 Dept. of Defense summer study conference called TEOTA (\"The Eyes of the Army\"), which sought to identify new techniques useful for military reconnaissance and technical gathering of intelligence. A follow-on summer program in 1953 at the University of Michigan, called Project Wolverine, identified several of the TEOTA subjects, including Doppler-assisted sub-beamwidth resolution, as research efforts to be sponsored by the Department of Defense (DoD) at various academic and industrial research laboratories. In that same year, the Illinois group produced a \"strip-map\" image exhibiting a considerable amount of sub-beamwidth resolution.\n",
    "source": "wikipedia",
    "title": "History of synthetic-aperture radar",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_2202274",
    "text": "Table-turning (also known as table-tapping, table-tipping or table-tilting) is a type of séance in which participants sit around a table, place their hands on it, and wait for rotations. The table was purportedly made to serve as a means of communicating with the spirits; the alphabet would be slowly spoken aloud and the table would tilt at the appropriate letter, thus spelling out words and sentences. The process is similar to that of a Ouija board. Scientists and skeptics consider table-turning to be the result of the ideomotor effect, or of conscious trickery.\n\nHistory\nWhen the movement of spiritualism first reached Europe from America in the winter of 1852–1853, the most popular method of consulting the spirits was for several persons to sit round a table, with their hands resting on it, and wait for the table to move. If the experiment was successful, the table would rotate with considerable rapidity and would occasionally rise in the air, or perform other movements.\nWhilst most spiritualists ascribed the table movements to the agency of spirits, two investigators, count de Gasparin and professor Thury (father of René Thury) of Geneva, conducted a careful series of experiments. They claimed to have demonstrated that the movements of the table were due to a physical force emanating from the bodies of the sitters, for which they proposed the name ectenic force. Their conclusion rested on the supposed elimination of all known physical causes for the movements; but it is doubtful from the description of the experiments whether the precautions taken were sufficient to exclude unconscious muscular action (the ideomotor effect) or even deliberate fraud.\nIn England, table-turning became a fashionable diversion and was practised all over the country in the year 1853. John Elliotson and his followers attributed the phenomena to mesmerism. The general public were content to find the explanation of the movements in spirits, animal magnetism, Odic force, galvanism, electricity, or even the rotation of the earth. Some Evangelical clergymen alleged that the spirits who caused the movements were of a diabolic nature. In France, Allan Kardec studied the phenomenon and concluded in The Book on Mediums that some communications were caused by an outside intelligence, as the message contained information that was not known to the group.\n",
    "source": "wikipedia",
    "title": "Table-turning",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_49535",
    "text": "A thought experiment is an imaginary scenario that is meant to elucidate or test an argument or theory. It is often an experiment that would be hard, impossible, or unethical to actually perform. It can also be an abstract hypothetical that is meant to test our intuitions about morality or other fundamental philosophical questions.\n\nHistory\nThe ancient Greek δείκνυμι, deiknymi, 'thought experiment', \"was the most ancient pattern of mathematical proof\", and existed before Euclidean mathematics, where the emphasis was on the conceptual, rather than on the experimental part of a thought experiment.\nJohann Witt-Hansen established that Hans Christian Ørsted was the first to use the equivalent German term Gedankenexperiment c. 1812. Ørsted was also the first to use the equivalent term Gedankenversuch in 1820.\nBy 1883, Ernst Mach used Gedankenexperiment in a different sense, to denote exclusively the imaginary conduct of a real experiment that would be subsequently performed as a real physical experiment by his students. Physical and mental experimentation could then be contrasted: Mach asked his students to provide him with explanations whenever the results from their subsequent, real, physical experiment differed from those of their prior, imaginary experiment.\nThe English term thought experiment was coined as a calque of Gedankenexperiment, and it first appeared in the 1897 English translation of one of Mach's papers. Prior to its emergence, the activity of posing hypothetical questions that employed subjunctive reasoning had existed for a very long time for both scientists and philosophers. The irrealis moods are ways to categorize it or to speak about it. This helps explain the extremely wide and diverse range of the application of the term thought experiment once it had been introduced into English.\n\nGalileo's demonstration that falling objects must fall at the same rate regardless of their masses was a significant step forward in the history of modern science. This is widely thought to have been a straightforward physical demonstration, involving climbing up the Leaning Tower of Pisa and dropping two heavy weights off it, whereas in fact, it was a logical demonstration, using the thought experiment technique. The experiment is described by Galileo in his 1638 work Two New Sciences thus:\n\nSalviati: If then we take two bodies whose natural speeds are different, it is clear that on uniting the two, the more rapid one will be partly retarded by the slower, and the slower will be somewhat hastened by the swifter. Do you not agree with me in this opinion?Simplicio: You are unquestionably right.Salviati: But if this is true, and if a large stone moves with a speed of, say, eight while a smaller moves with a speed of four, then when they are united, the system will move with a speed less than eight; but the two stones when tied together make a stone larger than that which before moved with a speed of eight. Hence the heavier body moves with less speed th",
    "source": "wikipedia",
    "title": "Thought experiment",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_3373310",
    "text": "Traditional knowledge (TK), indigenous knowledge (IK), folk knowledge, and local knowledge generally refers to knowledge systems embedded in the cultural traditions of regional, indigenous, or local communities.\nTraditional knowledge includes types of knowledge about traditional technologies of areas such as subsistence (e.g. tools and techniques for hunting or agriculture), midwifery, ethnobotany and ecological knowledge, traditional medicine, celestial navigation, craft skills, ethnoastronomy, climate, and others. These systems of knowledge are generally based on accumulations of empirical observation of and interaction with the environment, transmitted orally across generations. \nThe World Intellectual Property Organization (WIPO) and the United Nations (UN) include traditional cultural expressions (TCE) in their respective definitions of indigenous knowledge. Traditional knowledge systems and cultural expressions exist in the forms of culture, stories, legends, folklore, rituals, songs, and laws, languages, songlines, dance, games, mythology, designs, visual art and architecture.\n\nCharacteristics and related concepts\nA report of the International Council for Science (ICSU) Study Group on Science and Traditional Knowledge characterises traditional knowledge as:\n\na cumulative body of knowledge, know-how, practices and representations maintained and developed by peoples with extended histories of interaction with the natural environment. These sophisticated sets of understandings, interpretations and meanings are part and parcel of a cultural complex that encompasses language, naming and classification systems, resource use practices, ritual, spirituality and worldview.\n\nTraditional knowledge typically distinguishes one community from another. In some communities, traditional knowledge takes on personal and spiritual meanings. Traditional knowledge can also reflect a community's interests. Some communities depend on their traditional knowledge for survival. Traditional knowledge regarding the environment, such as taboos, proverbs and cosmological knowledge systems, may provide a conservation ethos for biodiversity preservation. This is particularly true of traditional environmental knowledge, which refers to a \"particular form of place-based knowledge of the diversity and interactions among plant and animal species, landforms, watercourses, and other qualities of the biophysical environment in a given place\". As an example of a society with a wealth of traditional ecological knowledge (TEK), the South American Kayapo people, have developed an extensive classification system of ecological zones of the Amazonian tropical savannah (i.e., campo / cerrado) to better manage the land.\nSome social scientists conceptualise knowledge within a naturalistic framework and emphasize the gradation of recent knowledge into knowledge acquired over many generations. These accounts use terms like adaptively acquired knowledge, socially constructed knowledge, and o",
    "source": "wikipedia",
    "title": "Traditional knowledge",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_63988166",
    "text": "The Troughton scale is a measurement scale that de facto served as the first national standard of length in the United States, from 1832 until 1856.\n\nPhysical description\nThe measurement scale spans 82 inches and is subdivided to tenths of inches. It is marked on a silver inlay in a brass bar; the bar itself is about 86 inches long.\nHistory\nThe scale was prepared for the Office of Coast Survey by Troughton of London and was brought to the United States in 1815 by F. R. Hassler, who a year later became first Superintendent of the Survey of the Coast and, in 1832, first Superintendent of Weights and Measures.\nAt the time, the United States Government was principally financed by duties on imports and exports (the federal income tax did not become a permanent feature of the US system until 1913). The appropriate import and export taxes on commercial items were determined at customhouses maintained by the federal government at various ports of entry. A reliable and uniform system of weights and measure was necessary for this system to work, as well as for settling commercial disputes.\nIn 1830, the US Senate requested the Secretary of the Treasury to ‘cause a comparison to be made of the standards of weight and measure now used at the principal custom houses in the United States, and report to the Senate at the next session of Congress.’  To carry out this mandate, the Treasury Secretary appointed Hassler, who found that (as was suspected) large discrepancies existed among the weights and measures in use at the principal customhouses at different US ports.\nThe Treasury Department immediately started constructing new necessary weights and measures for the customs service. For this purpose, the Treasury Department had to choose standards, and the standard yard adopted was the 36 inches comprised between the 27th and the 63rd inches of the Troughton scale. This 36-inch space was supposed to be identical with the English standard at 62 °F, though it had never been directly compared with that standard. The original English standard, in turn, was made in 1758, but was then damaged beyond the point of usability in the great fire of 1834.\nIn 1856, the US received two copies of the new British standard yard after Britain completed the manufacture of new imperial standards to replace those lost in 1834. As standards of length, the new yards, especially bronze No. 11, were far superior to the Troughton scale. They were therefore accepted by the Office of Weights and Measures (a predecessor of NIST) as length standards of the United States.\n",
    "source": "wikipedia",
    "title": "Troughton scale",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1232743",
    "text": "Universal science (German: Universalwissenschaft; Latin: scientia generalis, scientia universalis) is a branch of metaphysics, dedicated to the study of the underlying principles of all science. Instead of viewing knowledge as being separated into branches, Universalists view all knowledge as being part of a single category. Universal science is related to, but distinct from, universal language.\n\nPrecursors\nLogic and rationalism lie at the foundation of the ideas of universal science. In a broad sense, logic is the study of reasoning. Although there were individuals that implicitly utilized logical methods prior to Aristotle, it is generally agreed he was the originator of modern systems of logic. The Organon, Aristotle's books on logic, details this system. In Categories, Aristotle separates everything into 10 \"categories\": substance, quantity, quality, relation, place, time, position, state, action, and passion. In De Interpretatione, Aristotle studied propositions, detailing what he determined were the most basic propositions and the relationships between them. The Organon had several other books, which further detailed the process of constructing arguments, deducing logical consequences, and even contained the foundations of the modern scientific method. \nThe most immediate predecessor to universal science is the system of formal logic, which is the study of the abstract notions of propositions and arguments, usually utilizing symbols to represent these structures. Formal logic differs from previous systems of logic by looking exclusively at the structure of an argument, instead of at the specific aspects of each statement. Thus, while the statements \"Jeff is shorter than Jeremy and Jeremy is shorter Aidan, so Jeff is shorter than Aidan\" and \"Every triangle has less sides than every rectangle and every rectangle has less sides than every pentagon, so every triangle has less sides than every pentagon\" deal with different specific information, they are both are equivalent in formal logic to the expression\n\n  \n    \n      \n        ∀\n        x\n        ∈\n        X\n        ,\n        y\n        ∈\n        Y\n        ,\n        z\n        ∈\n        Z\n        ,\n        \n        x\n        <\n        y\n        ∧\n        y\n        <\n        z\n        \n        ⟹\n        \n        x\n        <\n        z\n      \n    \n    {\\displaystyle \\forall x\\in X,y\\in Y,z\\in Z,\\quad x<y\\wedge y<z\\implies x<z}\n  \n.                                  \nBy abstracting away from the specifics of each statement and argument, formal logic allows the overarching structure of logic to be studied. This viewpoint inspired later logicians to seek out a set of minimal size containing all of the requisite knowledge from which everything else could be derived and is the fundamental idea behind universal science.\n",
    "source": "wikipedia",
    "title": "Universal science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_35308606",
    "text": "The Welbeck Academy or Welbeck Circle is a name that has been given to the loose intellectual grouping around William Cavendish, 1st Duke of Newcastle-upon-Tyne in the first half of the 17th century. It takes its name from Welbeck Abbey, a country house in Nottinghamshire that was a Cavendish family seat. Another term used is Newcastle Circle. The geographical connection is, however, more notional than real; and these terms have been regarded also as somewhat misleading. Cavendish was Viscount Mansfield in 1620, and moved up the noble ranks to Duke, step by step; \"Newcastle\" applies by 1628.\nNewcastle was a royalist exile in continental Europe in the latter part of the First English Civil War and the Interregnum. He then returned to England and lived to 1676. His life shows many instances of cultural and intellectual patronage.\n\nScience and mathematics\nA scientific interest was optics. The group involved in these studies included Charles Cavendish (William's brother), Thomas Hobbes, Robert Payne and Walter Warner. This core \"academy\" group was disrupted when Newcastle took on responsibility for the Prince of Wales, in 1638. At a later point John Pell was in Newcastle's service.\nCharles Cavendish's circle included Henry Bond, Richard Reeve or Reeves the instrument-maker, John Twysden and John Wallis. He was a patron of William Oughtred.\nLiterature and the arts\nNewcastle in the 1630s became a major patron to Ben Jonson. His second wife was Margaret Cavendish, née Lucas, the writer. Newcastle was called \"our English Maecenas\" by Gerard Langbaine the Younger; he was a patron after the Restoration to both John Dryden and Thomas Shadwell. Other writers he supported included William Davenant, William Sampson, James Shirley and John Suckling. He bought sculptures by Francesco Fanelli for Welbeck.\nIn exile\nAs a consequence of the royalist defeat at the Battle of Marston Moor in 1644, Newcastle and some of his entourage went into exile. He returned to England only with the Restoration of 1660. Initially he went to Hamburg. By 1645 Newcastle was in Paris: his circle had contacts in Marin Mersenne and Claude Mydorge, whom Charles Cavendish had met in France at least 15 years earlier. In France Newcastle met and married that year Margaret Lucas who was with the exiled court of Queen Henrietta Maria. She studied with Charles Cavendish, and became a writer on natural philosophy, initially a proponent of atomism. Besides Hobbes, who joined them in Paris, the Cavendishes knew at this period René Descartes, Kenelm Digby, and Christiaan Huygens. Much of the latter part of their exile was spent at Antwerp; there, though in debt, they lived in the Rubenshuis. Other associations were with Walter Charleton who came to know Margaret Cavendish (not necessarily abroad, since she returned to England for a time), and William Brereton, 3rd Baron Brereton.\n",
    "source": "wikipedia",
    "title": "Welbeck Academy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_81304225",
    "text": "Western esotericism and psychology surveys the documented exchanges between Western esotericism—including Westernized hybrids of Asian traditions—and selected areas of psychology, psychotherapy, and popular psychology. From the late eighteenth century onward, conduits such as animal magnetism and early hypnosis (reinterpreted from mesmeric “somnambulism”), Spiritualism/psychical research, and fin de siècle occultism and comparative projects created channels by which esoteric repertoires (e.g., alchemy, astrology, and subtle body schemes) were translated into psychological idioms or embedded in therapeutic and self-development techniques. In the twentieth century, these exchanges were variously articulated in analytical psychology (including Jung’s alchemical hermeneutics), humanistic workshop cultures and the human potential movement, transpersonal psychology, and symbolic counselling that repurposed oracular media (e.g., tarot, astrology, or the I Ching).\nRather than a single genealogy, historians emphasise plural processes of transmission, translation, and hybridisation across specific networks and publics—among them Theosophy (with codified chakras and subtle bodies), Anthroposophy (linking esoteric doctrines to pedagogical and para-clinical projects), the Eranos circle (mediating Jungian hermeneutics and history-of-religions), and late-modern markets often labelled “New Age”. Sociological accounts frame the broader diffusion via the late-modern “cultic milieu” and “occulture”, which describe how esoteric symbols and narratives circulate beyond formal religion through publishing, workshops, retreats and wellness/coaching niches, where psychologised self-work became a prominent vector of reception.\n\nScope and definitions\n\nHistoriography and frameworks\nModern scholarship generally treats Faivre’s componential definition as a heuristic grid rather than as an ontology for what esotericism “is”. In this perspective, the grid helps historians describe how symbolic mediations and techniques of transformation were translated into psychological idioms in specific periods and publics, while avoiding essentialist claims.\nPlaced within nineteenth- and twentieth-century contexts, Wouter J. Hanegraaff proposes an etic use of “occultism” for modern currents that explored interfaces between science, comparative religion and esoteric practice, and describes a two-way traffic between religious and psychological languages—“psychologization of religion” and “sacralization of psychology”—to account for the reception of alchemy, astrology and subtle-body maps in theory, therapy and self-work.\nFor diffusion beyond academic or ecclesiastical institutions, historians of contemporary religion draw on sociological models. The notion of the cultic milieu proposed by Colin Campbell designates an environment in which heterodox repertoires (e.g., astrology, trance, subtle bodies) persist, recirculate and recombine; Christopher Partridge’s “occulture” points to a cultural re",
    "source": "wikipedia",
    "title": "Western esotericism and psychology",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_81007565",
    "text": "The relationship between Western esotericism and science (and particularly the origins of experimental science) is a historiographical overview intersecting academic study of Western esotericism and history of science about how learned esoteric currents (e.g., natural magic, alchemy, hermeticism) interacted with natural philosophy, artisanal knowledge, and later scientific institutions from Antiquity to the twentieth century. It summarizes major debates (e.g., the “Yates thesis”), the role of printing and learned/artisanal networks, and the transformations that led from alchemy and chymistry to early modern chemistry, it also traces nineteenth–twentieth-century continuities in mesmerism, spiritualism, and psychical research.\n\nScope and definitions\nThe scope covers learned currents conventionally grouped under Western esotericism and their interactions with natural-philosophical, artisanal, and later scientific practices. In current scholarship, “Western esotericism” functions as an analytic label devised by historians of ideas rather than a stable emic category across periods. Within this remit fall astrology (including astral/astrological magic), alchemy/chymistry, hermetic and theurgic philosophies, “natural magic”, Christian Kabbalah and related Christianized appropriations, and selected nineteenth–twentieth-century continuities (e.g., mesmerism, spiritualisms, psychical research) insofar as they engaged scientific methods, publics, or institutions.\nFollowing standard usage, esotericism is treated as a family-resemblance category centered on literate, textually mediated, often elite discourses and practices, rather than a catch-all for folk religion or popular magic. Vernacular healing, charms, and “cunning” practices are distinguished from the theorized “occult sciences” of the medieval and early modern Latin worlds; points of contact—such as the diffusion of printed “books of secrets” to artisanal publics—are noted as channels of exchange.\nThe term “science” is used heuristically with attention to historical vocabulary. Up to the seventeenth century the principal comparandum is natural philosophy and adjacent artisanal or medical know-how; only gradually did experimental and mathematical cultures crystallize into formations recognizable as “science,” often discussed under the Scientific Revolution. Modern disciplinary boundaries were themselves constructed through demarcation and boundary-work within the sociology of scientific knowledge, differentiating legitimate inquiry from “occult” pursuits. Historically sensitive labels are used where helpful: chymistry for the mixed alchemical–chemical enterprise c. 1400–1700, and “natural magic” for learned techniques operating through hidden properties and sympathies (see sympathetic magic for the anthropological sense).\nFor definitional clarity, Western esotericism denotes a historically connected set of learned currents characterized—in varying constellations—by ideas of correspondences, a living ",
    "source": "wikipedia",
    "title": "Western esotericism and science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_57537760",
    "text": "The William Phelps Ornithological Collection, also known as the Phelps Ornithological Museum, is a museum of natural sciences dedicated to the study, exhibition and preservation of the birds of Venezuela and the rest of Latin America. The collection is located east of Caracas and in the geographic center of Greater Caracas, in the heart of the Sabana Grande district. The William Phelps ornithological collection is the most important in Latin America and it is also the most important private collection in the world in its research area.\nIn this private museum one will find important Phelps family study books, as well as 8000 scientific volumes in the library, more than 83,000 anatomical specimens, more than 80,000 skins, etc.  For the year 1990, it was said that the William Phelps Ornithological Collection contained more than 76,300 skins and a small number of anatomical specimens, in the Gran Sabana Building of Sabana Grande. The Phelps library in 1990 already had 6,000 books, 800 journals and 5,500 reprints, mostly from natural sciences.\n\nHistory\nThe ornithological collection was born in 1938, although it did not have its own headquarters on the Boulevard of Sabana Grande until 1949. At the beginning of 2018, it celebrated its 80th anniversary in Caracas, Venezuela. With the passing of time, the collection has been growing and still has great international scientific relevance. In 2005, an investigation was carried out on \"plumage differences in four subspecies of golden warbler Basileuterus culicivorus in Venezuela\".\nSee more\nSabana Grande (Caracas)\nBoulevard of Sabana Grande\nWilliam Phelps\nEl Recreo Shopping Mall\nReferences\n\nExternal links\nhttps://fundacionwhphelps.org/\n",
    "source": "wikipedia",
    "title": "William Phelps Ornithological Collection",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1187515",
    "text": "Europe is traditionally defined as one of seven continents. Physiographically, it is the northwestern peninsula of the larger landmass known as Eurasia (or the larger Afro-Eurasia); Asia occupies the centre and east of this continuous landmass. Europe's eastern frontier is usually delineated by the Ural Mountains in Russia, which is the largest country by land area in the continent. The southeast boundary with Asia is not universally defined, but the modern definition is generally the Ural River or, less commonly, the Emba River. The boundary continues to the Caspian Sea, the crest of the Caucasus Mountains (or, less commonly, the river Kura in the Caucasus), and on to the Black Sea. The Bosporus, the Sea of Marmara, and the Dardanelles conclude the Asian boundary. The Mediterranean Sea to the south separates Europe from Africa. The western boundary is the Atlantic Ocean. Iceland is usually included in Europe because it is over twice as close to mainland Europe as mainland North America. There is ongoing debate on where the geographical centre of Europe falls.\n\nOverview\nSome geographical texts refer to a Eurasian continent given that Europe is not surrounded by sea and its southeastern border has always been variously defined for centuries.\nIn terms of shape, Europe is a collection of connected peninsulas and nearby islands. The two largest peninsulas are Europe itself and Scandinavia to the north, divided from each other by the Baltic Sea. Three smaller peninsulas—Iberia, Italy, and the Balkans—emerge from the southern margin of the mainland. The Balkan peninsula is separated from Asia by the Black and Aegean Seas. Italy is separated from the Balkans by the Adriatic Sea, and from Iberia by the Mediterranean Sea, which also separates Europe from Africa. Eastward, mainland Europe widens much like the mouth of a funnel, until the boundary with Asia is reached at the Ural Mountains and Ural River, the Caspian Sea, and the Caucasus Mountains.\nLand relief in Europe shows great variation within relatively small areas. The southern regions are mountainous while moving north the terrain descends from the high Alps, Pyrenees, and Carpathians, through hilly uplands, into broad, low northern plains, which are vast in the east. An arc of uplands also exists along the northwestern seaboard, beginning in southwestern Ireland, continuing across through western and northern Great Britain, and up along the mountainous, fjord-cut spine of Norway.\nThis description is simplified. Sub-regions such as Iberia and Italy contain their own complex features, as does mainland Europe itself, where the relief contains many plateaus, river valleys, and basins that complicate the general trend. Iceland and the British Isles are special cases. The former is of North Atlantic volcanic formation, while the latter consist of upland areas once joined to the mainland until cut off by rising sea levels.\nPartial list of European peninsulas\n\nBalkan Peninsula\nPeloponnese\nChalkidiki\nIstri",
    "source": "wikipedia",
    "title": "Geography of Europe",
    "topic": "Geography_of_Europe"
  },
  {
    "id": "wiki_19630660",
    "text": "Afro-Eurasia (also Afroeurasia and Eurafrasia) is a supercontinent comprising the continents of Africa, Asia, and Europe. The terms are compound words of the names of its constituent parts. Afro-Eurasia has also been called the \"Old World\", in contrast to the \"New World\" referring to the Americas.\nAfro-Eurasia encompasses 85,135,000 km2 (32,871,000 sq mi), 57% of the world's land area, and has a population of approximately 6.7 billion people, roughly 86% of the world population. Together with mainland Australia, they comprise the vast majority of the land in the world's Eastern Hemisphere. The Afro-Eurasian mainland is the largest and most populous contiguous landmass on Earth.\n\nRelated terms\nThe following terms are used for similar concepts:\n\nEcumene: a term from classical antiquity for the world as was known to ancient Greek scholars, which was limited to Europe and parts of Africa and Asia.\nOld World: a term from the Age of Discovery which, for European explorers, contrasted their previously known world from the New World of the Americas; still widely used in biology.\nWorld Island: a term coined by H.J. Mackinder in his \"The Geographical Pivot of History\" (1904) and used in geopolitical contexts. Mackinder defines the World Island as the large contiguous landmass, technically excluding islands such as the British Isles, the Japanese Archipelago, Madagascar, and the Malay Archipelago. \"Afro-Eurasia\" generally includes those islands usually considered parts of Africa, Asia, and Europe.\nGeology\nAlthough Afro-Eurasia is typically considered to comprise two or three separate continents, it is not a proper supercontinent. Instead, it is the largest present part of the supercontinent cycle.\nExtreme points\nThis is a list of the points that are farther north, south, east or west than any other location as well as the highest and lowest elevations on Afro-Eurasia.\nSee also\nExtreme points of Earth\nExtreme points of Africa\nExtreme points of Eurasia\nExtreme points of Asia\nExtreme points of Europe\nGeography of Africa\nGeography of Asia\nGeography of Europe\nIndo-Mediterranean, argued to be central within Afro-Eurasia until 1000 AD\nIntermediate Region\nThe Geographical Pivot of History\n",
    "source": "wikipedia",
    "title": "Afro-Eurasia",
    "topic": "Geography_of_Europe"
  },
  {
    "id": "wiki_36971",
    "text": "The Arctic (; from Ancient Greek  ἄρκτος (árktos) 'bear') is the polar region of Earth that surrounds the North Pole, lying north of the Arctic Circle. The Arctic region, from the IERS Reference Meridian travelling east, consists of parts of northern Norway (Nordland, Troms, Finnmark, Svalbard and Jan Mayen), northernmost Sweden (Västerbotten, Norrbotten and Lappland), northern Finland (North Ostrobothnia, Kainuu and Lappi), Russia (Murmansk, Siberia, Nenets Okrug, Novaya Zemlya), the United States (Alaska), Canada (Yukon, Northwest Territories, Nunavut), Danish Realm (Greenland), and northern Iceland (Grímsey and Kolbeinsey), along with the Arctic Ocean and adjacent seas.\nLand within the Arctic region has seasonally varying snow and ice cover, with predominantly treeless permafrost under the tundra. Arctic seas contain seasonal sea ice in many places.\nThe Arctic region is a unique area among Earth's ecosystems. The cultures in the region and the Arctic indigenous peoples have adapted to its cold and extreme conditions. Life in the Arctic includes zooplankton and phytoplankton, fish and marine mammals, birds, land animals, plants, and human societies. Arctic land is bordered by the subarctic.\n\nDefinition and etymology\nThe word Arctic comes from the Greek word ἀρκτικός arktikos \"near the Bear, northern\" and from the word ἄρκτος arktos meaning \"bear\" for either to the constellation known as Ursa Major, the \"Great Bear\", which is prominent in the northern portion of the celestial sphere, or the constellation Ursa Minor, the \"Little Bear\", which contains the celestial north pole (currently very near Polaris, the current north Pole Star, or North Star).\nThere are several definitions of what area is contained within the Arctic. The area can be defined as north of the Arctic Circle (about 66° 34'N), the approximate southern limit of the midnight sun and the polar night. Another definition of the Arctic, which is popular with ecologists, is the region in the Northern Hemisphere where the average temperature for the warmest month (July) is below 10 °C (50 °F); the northernmost tree line roughly follows the isotherm at the boundary of this region.\n",
    "source": "wikipedia",
    "title": "Arctic",
    "topic": "Geography_of_Europe"
  },
  {
    "id": "wiki_38314140",
    "text": "This is a list of countries and territories in Europe by population density. Data are from the United Nations unless otherwise specified.\nAbkhazia, Georgia and South Ossetia are each bordered on the north by the Greater Caucasus, and may have some territory north of these mountains and thus in Europe by the most common definition. These three, as well as Armenia and Azerbaijan would have more territory or all of their territory in Europe using a more expansive definition.\nSome countries in the Caucasus, as well as Greenland and the geopolitical subdivisions of the island of Cyprus (Akrotiri and Dhekelia, Cyprus and Northern Cyprus) are not considered geographically European, but are listed here because of their historical and cultural connections to the continent.\nThere is some discussion about whether Kosovo should be recognised as a separate country. De facto it can be considered as one, but de jure recognition is not clear-cut.\n\nEuropean countries and territories by population density\nFigures are for the European portion of the given countries listed, unless given in italics. Total density and other details may be found in the associated note.\nSee also\nEuropean Union statistics\nList of countries by population density\nList of European countries by area\nList of European countries by population\nNotes\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Area and population of European countries",
    "topic": "Geography_of_Europe"
  },
  {
    "id": "wiki_663896",
    "text": "Atlantic Europe encompasses the western portion of Europe which borders the Atlantic Ocean. The term may refer to the idea of Atlantic Europe as a cultural unit and/or as a biogeographical region.\nIt comprises Ireland, Great Britain, Iceland, Belgium, the Netherlands, the central and northern regions of Portugal, northwestern and northern Spain (including Galicia, Asturias, Cantabria, Southern Basque Country, and some portions of Castile and León), the southwestern and western portion of France (Northern Basque Country), western Scandinavia as well as western and northern Germany.\nWeather and overall physical conditions are relatively similar along this area (with the exception of parts of Scandinavia and the Baltic), resulting in similar landscapes with common endemic plant and animal species. From a strictly physical point of view most of the Atlantic European shoreline can be considered a single biogeographical region. Physical geographers label this biogeographical area as the European Atlantic Domain, part of the Euro-Siberian botanic region.\n\nCulture\n\nAtlantic Europe in politics\nThere is a multi-national association of regions, which acts as a co-ordinator of Atlantic European regions and its interests. This is the Atlantic Arc Commission. Operative since 1989, it includes 26 regions from four member States - Great Britain, France, Spain and Portugal.  The Atlantic Arc Commission is one of the seven Geographical Commissions in the Conference of Peripheral Maritime Regions of Europe.\nGenetics\nThe genetic link between the various Atlantic population is still under discussion. On the one hand, some studies show that modern and Iron Age British and Irish samples cluster genetically very closely with other North European populations, and not to southern Atlantic Europeans in Spain and France. However, as the authors acknowledge, the sample used is unlikely to include many members of smaller genetically isolated populations that exist within countries. On the other hand, an article published in the American Journal of Genetics indicates – after including samples from different regions within European countries – a shared ancestry throughout the Atlantic zone, from northwest Iberia (Galicia) to western Scandinavia, that dates back to end of the last Ice Age.\n",
    "source": "wikipedia",
    "title": "Atlantic Europe",
    "topic": "Geography_of_Europe"
  },
  {
    "id": "wiki_31463677",
    "text": "Determining the boundaries between the continents is generally a matter of geographical convention and consensus. Several slightly different conventions are in use. The number of continents is most commonly considered seven (in English-speaking countries) but may range as low as four when Afro-Eurasia and the Americas are both considered as single continents. An island can be considered to be associated with a given continent by either lying on the continent's adjacent continental shelf (e.g. Singapore, the British Isles) or being a part of a microcontinent on the same principal tectonic plate (e.g. Madagascar and Seychelles). An island can also be entirely oceanic while still being associated with a continent by geology (e.g. Bermuda, the Australian Indian Ocean Territories) or by common geopolitical convention (e.g. Ascension Island, the South Sandwich Islands). Another example is the grouping into Oceania of the Pacific Islands with Australia and Zealandia.\nThere are three overland boundaries subject to definition:\n\nbetween Africa and Asia (dividing Afro-Eurasia into Africa and Eurasia): at the Isthmus of Suez;\nbetween Asia and Europe (dividing Eurasia): along the Turkish straits, the Caucasus, and the Urals and the Ural River (historically also north of the Caucasus, along the Kuma–Manych Depression or along the Don River);\nbetween North America and South America (dividing the Americas): at some point on the Isthmus of Panama, with the most common demarcation in atlases and other sources following the Darién Mountains watershed along the Colombia–Panama border where the isthmus meets the South American continent (see Darién Gap).\nWhile today the isthmus between Asia and Africa is navigable via the Suez Canal, and that between North and South America via the Panama Canal, these artificial channels are not generally accepted as continent-defining boundaries in themselves. The Suez Canal happens to traverse the Isthmus of Suez between the Mediterranean Sea and the Red Sea, dividing Africa and Asia. The continental boundaries are considered to be within the very narrow land connections joining the continents.\nThe remaining boundaries concern the association of islands and archipelagos with specific continents, notably:\n\nthe delineation between Africa, Asia, and Europe in the Mediterranean Sea;\nthe delineation between Asia and Europe in the Arctic Ocean;\nthe delineation between Europe and North America in the North Atlantic Ocean;\nthe delineation between North and South America in the Caribbean Sea;\nthe delineation of Antarctica from Africa, Australia, and South America in the Indian, South Pacific, and South Atlantic oceans, respectively (referred to collectively by some geographers as the Southern Ocean or the Antarctic Ocean);\nthe delineation of Asia from Australia in the Ceram Sea, Arafura Sea, Timor Sea, Halmahera Sea, and the Wallacean region of the Indonesian Archipelago\nthe delineation of Asia from North America in the North Pacific Ocean.",
    "source": "wikipedia",
    "title": "Boundaries between the continents",
    "topic": "Geography_of_Europe"
  },
  {
    "id": "wiki_1512317",
    "text": "The Broad Fourteens is an area of the southern North Sea that is fairly consistently 14 fathoms (84 ft; 26 m) deep. Thus, on a nautical chart with depths given in fathoms, a broad area with many \"14\" notations can be seen.\n\nExtent\nThe Broad Fourteens region is located off the coast of the Netherlands and south of the Dogger Bank, roughly between longitude 3°E and 4°30'E and latitude 52°30'N and 53°30'N. The area is known to the Dutch and German navies as the Breeveertien (\"Fourteen\"). Geologically it is comparable to the Long Forties, another submerged plateau that has related origins.\nNaval battles\nNaval engagements in the region have included the torpedoing of three British armoured cruisers in the action of 22 September 1914 during World War I.\nNavigation\nThe shallowness of the area means that the largest oil tankers when fully loaded cannot traverse the Broad Fourteens to reach the English Channel from the North Sea because their draft is too deep.\nIn media\nThe area features as a major setting in the WWII film The Broad Fourteens, which is a dramatization of Royal Navy motor torpedo boat operations in the English Channel and surrounding areas.\nSee also\nDogger Bank for map and links to similar places\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Broad Fourteens",
    "topic": "Geography_of_Europe"
  },
  {
    "id": "wiki_32182414",
    "text": "The European Atlas of the Seas is an interactive web-based atlas that provides information on the coasts and seas in Europe. The latest version of the Atlas was released on 16 September 2020 and is available in the 24 official languages of the European Union.\n\nSee also\nEuropean Maritime Day\nExternal links\nAtlas of the Seas, European Commission\n",
    "source": "wikipedia",
    "title": "European Atlas of the Seas",
    "topic": "Geography_of_Europe"
  },
  {
    "id": "wiki_7393326",
    "text": "The European Soil Database is the only harmonized soil database in Europe from which many other data information and services are derived. For instance, the European Soil Database v2 Raster Library contains raster (grid) data files with cell sizes of 1 km x 1 km for a large number of soil related parameters. Each grid is aligned with the INSPIRE reference grid. These rasters are in the public domain and allow expert users to use the data for instance to run soil-, water- and air related models. The European Soil Database may be downloaded from the European Soil Data Center (ESDAC).\nThe soil raster data files are accompanied by as many static soil maps (PDF Format, A3), which allow the user to have a quick overview of the distribution of soil characteristics in a spatial way. The many rasters include parameters such as soil texture (clay, silt and sand), parent material, WRB soil type, Obstacle to roots, soil depth, Impermeability, Soil Water Regime, Water Management System, mineralogy, cation exchange capacity, packing density, available water capacity.\nThe European Soil Database is based on experts' knowledge. In addition to the European Soil Database, the European Commission runs the LUCAS topsoil survey collecting soil samples across the soils of European Union. Based on LUCAS topsoil database, scientists have modelled the spatial distribution of soil physical properties (Sand, Silt, Clay, Coarse fragments, etc).\nIn an attempt to expand soil data at European scale, the European Commission Joint Research Centre runs the LUCAS topsoil survey collecting soil samples across the soils of European Union. Based on LUCAS topsoil database, scientists have modelled the spatial distribution of soil physical properties (sand, silt, clay, coarse fragments, etc).\nAdditionally, in ESDAC, a number of mapping services has been developed in order to allow the public to navigate and query soil data. The ESDAC Map Viewer is a web-based application for the navigation of ESDB related data through a map interface. This map service allows interaction through soil parameter selection and map operations such as zooming and panning. The data can be queried and identified. According to INSPIRE principles, this application has been created and extended using international standards (OGC - WMS) so that it is possible to combine layers of maps located in different map servers all around the World.\nDownload Sources:\n\nVector Data\nRaster Library 1km x 1km\n\nSee also\nEuropean Soil Data Centre\n",
    "source": "wikipedia",
    "title": "European Soil Database",
    "topic": "Geography_of_Europe"
  },
  {
    "id": "wiki_2722676",
    "text": "The main European watershed is the drainage divide (\"watershed\") which separates the basins of the rivers that empty into the Atlantic Ocean, the North Sea and the Baltic Sea from those that feed the Mediterranean Sea, the Adriatic Sea and the Black Sea. It stretches from the tip of the Iberian Peninsula at Gibraltar in the southwest to the endorheic basin of the Caspian Sea in Russia in the northeast.\n\nCourse\n\nCharacteristics\nThe watershed is not a clearly defined divide.\nFor example, tectonics in the area which is now the Upper Rhine Plain created the river Rhine. The Rhine's sharper altitude gradient on its much shorter way to the North Sea causes much stronger headward erosion than that of the much older River Danube (see the  Danube upper river geology). Therefore the Rhine and its tributaries intrude deeper into phreatic zones of the Swabian Karst and even capture the upper Danube and its surface tributaries. It is expected that the Danube's upper course will one day disappear entirely in favour of the Rhine (\"stream capture\").\nSee also\nContinental Divide of the Americas\nContinental divide\nCategory:Drainage basins\nReferences\n\nExternal links\nInteractive topographical map of the European Continental Divide on the site continental-divide.eu\n",
    "source": "wikipedia",
    "title": "European watershed",
    "topic": "Geography_of_Europe"
  },
  {
    "id": "wiki_869534",
    "text": "The extreme points of Eurasia are farther north, south, east or west than any other location on the continent. Some of these locations are open to debate, owing to the diverse definitions of Europe and Asia.\nMainland Eurasia is entirely located within the northern hemisphere and mostly within the eastern hemisphere, yet it touches the western hemisphere on both extremes. Thus, both the easternmost and westernmost points of Eurasia are in the western hemisphere. Mainland Eurasia crosses 200° of longitude and 76° of latitude north to south.\n\nExtremes of Eurasia, including islands\nNorthernmost Point — Cape Fligeli, Rudolf Island, Franz Josef Land, Russia (81°50'N, 59°14'E)\nSouthernmost Point — Dana Island, Rote Ndao, Indonesia (11°00'S, 122°52'E)\nWhen Cocos (Keeling) Islands included as part of Southeast Asia, then South Island (12°11'22\"S, 96°54'22\"E)\nWesternmost point —Monchique Islet, Flores Island, Azores Islands, Portugal (39°29′42.8″N, 31°16′30″W)\nEasternmost point — Big Diomede, Russia (65°46'N, 169°03'W). The International Date Line runs between the Russian Big Diomede and the neighbouring U.S.-governed Little Diomede.\nExtremes of the Eurasian mainland\nNorthernmost Point — Cape Chelyuskin, Russia (77°44'N, 104°15'E)\nSouthernmost Point — Tanjung Piai, Malaysia (1°15'N, 103°30'E)\nWesternmost Point — Cabo da Roca, Portugal (38°46'N, 9°29'W)\nEasternmost Point — Cape Dezhnev, Russia (66°4'N, 169°39'W)\nOther\nHighest altitude: — Mount Everest, Nepal and China — 8,848 m (29,029 ft)\nLowest point on dry land: — The shore of the Dead Sea, Palestine and Jordan, 418 m (1,371 ft) below sea level. See List of places on land with elevations below sea level.\nFarthest from the ocean: — A place near Hoxtolgay in China (46°17′N 86°40′E) 2,645 km (1,644 mi) from the nearest coastline. See Pole of inaccessibility.\nSee also\nGeography of Europe\nGeography of Asia\nExtreme points of Europe\nExtreme points of Asia\nExtreme points of Afro-Eurasia\nExtreme points of Earth\n\n\n== Notes ==\n",
    "source": "wikipedia",
    "title": "Extreme points of Eurasia",
    "topic": "Geography_of_Europe"
  },
  {
    "id": "wiki_863532",
    "text": "This is a list of the extreme points of Europe: the geographical points that are higher or farther north, south, east or west than any other location in Europe. Some of these positions are open to debate, as the definition of Europe is diverse.\n\nExtremes of the European continent, including islands\nNorthernmost point. Cape Fligely, Rudolf Island, Franz Josef Land, Russia (81° 48′ 24″ N). Franz Josef Land is near the ill-defined border between Europe and Asia; if it is not considered a part of Europe, then the northernmost point is on the island of Rossøya, Svalbard, Norway (81°N).\n\nSouthernmost point. Cape Trypiti, Gavdos Island, Greece (34° 48′ 02″ N) is the least ambiguous southernmost point of Europe. However, there are other contenders, depending on definition. The island of Cyprus, although geographically in Asia, has cultural links with Europe and is also part of the European Union; Cyprus's southernmost point is the British base at Akrotiri (34°35′N). The Portuguese islands of Madeira are borderline between Europe and Africa; their southernmost point is at Bugio Island, at (32°24′14″N). La Restinga on the island of El Hierro (27°45′N) in the Spanish Canary Islands is yet further south and could be considered politically, though not physiographically as part of Europe.\nWesternmost point. Monchique Islet, Azores Islands, Portugal (31° 16′ 30″ W) (If considered part of Europe, though it sits on the North American Plate). If not, then the Capelinhos Volcano, Faial Island, Azores Islands, Portugal (28° 50′ 00″ W), the westernmost point of the Eurasian Plate above sea level.\nEasternmost point. Cape Flissingsky (69° 02′ E), Severny Island, Novaya Zemlya, Russia.\nExtreme points of Mainland Europe\nNorthernmost point. Cape Nordkinn (Kinnarodden), Norway (71°08′02″N 27°39′00″E)\nSouthernmost point. Punta de Tarifa, Spain   (36°00'00.2\"N). This is the southernmost point of a small island connected to the coast by a causeway in 1808, so no longer an island. If manmade land is not accepted, the continental southernmost point is located slightly more to the North at 36°00'25\"N.\nWesternmost point. Cabo da Roca, Portugal (9°29'56.44 W).\nEasternmost point.  The easternmost point is dependent upon the various definitions of Europe's eastern border. Utilizing the most common definition of Europe's eastern edge (the watershed divide of the Ural Mountains), the easternmost point of the Ural watershed (and thus mainland Europe) lies on an unnamed 545-metre (1,788 ft) peak at 68°18′37″N 66°37′05″E as shown on various detailed maps such as the Soviet General Staff maps and as shown on Google Earth/Maps.  This peak is 17 km (11 mi) northeast of an 875-metre (2,871 ft) peak named Gora Anoraga and 60 km (37 mi) southwest of Ostrov Lediyev (island) on Arctic waters south of the Kara Sea.\n",
    "source": "wikipedia",
    "title": "Extreme points of Europe",
    "topic": "Geography_of_Europe"
  },
  {
    "id": "wiki_48372669",
    "text": "This is a list of settlements located on the coastlines of the two Prespa Lakes: Great Prespa Lake (also known simply as Prespa Lake) and Small Prespa Lake. The Prespa Lakes are both freshwater lakes, located between the countries of the Republic of Macedonia, Greece and Albania. Settlements are automatically listed for Great Prespa Lake from Šurlenci, Republic of Macedonia, clockwise. For Small Prespa Lake, settlements are listed from Plati, Greece, clockwise. The table can be reorganised based on country, municipality name, population, and the language(s) spoken in the settlement. Major settlements (population of 1000 or greater) are highlighted in bold.\n\nGreat Prespa Lake\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "List of settlements on the Prespa Lakes shorelines",
    "topic": "Geography_of_Europe"
  },
  {
    "id": "wiki_5559381",
    "text": "North Western Metropolitan Area (NWMA) is a name given to the densely populated area of Europe situated at the demographic core of the old-line members of the European Union. It has about 137 million inhabitants and comprises parts of Belgium, France, Germany, Luxembourg, The Netherlands, and the United Kingdom. The center of the NWMA is formed by the triangle with apexes at London, Brussels and Paris; this central area is sometimes called the Central Capitals Region (CCR).\n\nReferences\nNorth Western Metropolitan Area - European Commission\n",
    "source": "wikipedia",
    "title": "North Western Metropolitan Area",
    "topic": "Geography_of_Europe"
  },
  {
    "id": "wiki_1002984",
    "text": "The subarctic zone is a region in the Northern Hemisphere immediately south of the true Arctic, north of hemiboreal regions and covering much of Alaska, Canada, Iceland, the north of Fennoscandia, Northwestern Russia, Siberia, and the Cairngorms. Generally, subarctic regions fall between 50°N and 70°N latitude, depending on local climates. Precipitation is usually low, and vegetation is characteristic of the taiga.\nDaylight at these latitudes is quite extreme between summer and winter due to its high latitude. Near the summer solstice for instance, subarctic regions can experience an all-night period of either civil, nautical, or astronomical twilight (or in the northern reaches full daylight), since the sun never dips more than 18 degrees below the horizon. Noctilucent clouds are best observed within this range of latitude.\n\nClimate and soils\nSubarctic temperatures are above 10 °C (50 °F) for at least one and at most three months of the year. Precipitation tends to be low due to the low moisture content of the cold air but isn't to the point to semiarid regions. Precipitation is typically greater in warmer months, with a summer maximum ranging from moderate in North America to extreme in the Russian Far East. Except in the wettest areas glaciers are not large because of the lack of winter precipitation; in the wettest areas, however, glaciers tend to be very abundant and Pleistocene glaciation covered even the lowest elevations. Soils of the subarctic are in which leaching of nutrients takes place even in the most heavily glaciated regions. The dominant soil orders are podsols and, further north, gelisols.\nSubarctic regions are often characterized by taiga forest vegetation as deciduous trees can't withstand the long winters, though where winters are relatively mild, as in northern Norway, broadleaf forest may occur—though in some cases soils remain too saturated almost throughout the year to sustain any tree growth and the dominant vegetation is a peaty herbland dominated by grasses and sedges. Typically, there are only a few species of large terrestrial mammals in the subarctic regions, the most important being elk, moose (Alces alces), bears, reindeer (Rangifer tarandus), and wolves (Canis lupus). Agriculture is mainly limited to animal husbandry as many crops can't be grown here, though in some areas barley can be grown. Canada and Siberia are very rich in minerals, notably nickel, molybdenum, cobalt, lead, zinc and uranium, whilst the Grand Banks and Sea of Okhotsk are two of the richest fisheries in the world and provide support for many small towns.\nExcept for those areas that are well-drained or adjacent to warm ocean currents, there is almost always continuous permafrost due to the very cold winters and short summers. This means that building in most subarctic regions is very difficult and expensive: cities are very few (Murmansk being the largest) and generally small, whilst roads are also few. Subarctic rail transport only exists in E",
    "source": "wikipedia",
    "title": "Subarctic",
    "topic": "Geography_of_Europe"
  },
  {
    "id": "wiki_6545069",
    "text": "North America is the third largest continent, and is also a portion of the second largest supercontinent if North and South America are combined into the Americas and Africa, Europe, and Asia are considered to be part of one supercontinent called Afro-Eurasia. With an estimated population of 580 million and an area of 24,709,000 km2 (9,540,000 mi2), the northernmost of the two continents of the Western Hemisphere is bounded by the Pacific Ocean on the west; the Atlantic Ocean on the east; the Caribbean Sea on the south; and the Arctic Ocean on the north.\nThe northern half of North America is sparsely populated and covered mostly by Canada, except for the northeastern portion, which is occupied by Greenland, and the northwestern portion, which is occupied by Alaska, the largest state of the United States. The central and southern portions of the continent are occupied by the contiguous United States, Mexico, and numerous smaller states in Central America and in the Caribbean.\nThe continent is delimited on the southeast by most geographers at the Darién watershed along the Colombia-Panama border, placing all of Panama within North America.  Alternatively, a less common view would end North America at the man-made Panama Canal.  Islands generally associated with North America include Greenland, the world's largest island, and archipelagos and islands in the Caribbean. The terminology of the Americas is complex, but  \"Anglo-America\" can describe Canada and the U.S., while \"Latin America\" comprises Mexico and the countries of Central America and the Caribbean, as well as the entire continent of South America.\nNatural features of North America include the northern portion of the American Cordillera, represented by the geologically new Rocky Mountains in the west; and the considerably older Appalachian Mountains to the east. The north hosts an abundance of glacial lakes formed during the last glacial period, including the Great Lakes. North America's major continental divide is the Great Divide, which runs north and south down through Rocky Mountains. The major watersheds draining to the include the Mississippi/Missouri and Rio Grande draining into the Gulf of Mexico (part of the Atlantic Ocean), and the St. Lawrence draining into the Atlantic. The Colorado, Colombia, and Yukon Rivers drain west to the Pacific Ocean. \nClimate is determined to a large extent by the latitude, ranging from Arctic cold in the north to tropical heat in the south. There are steppes (known as \"prairies\") in the central and western portions, and deserts in the Southwestern United States of Arizona, Colorado, California, Nevada, New Mexico, Utah, and Texas; along with the Mexican states of Baja California, Baja California Sur, Sonora, Chihuahua, Coahuila, Nuevo Leon and Tamaulipas.\n\nPaleogeography\nSeventy percent of North America is underlain by the Laurentia craton, which is exposed as the Canadian Shield in much of central and eastern Canada around the Hudson Bay, and as far s",
    "source": "wikipedia",
    "title": "Geography of North America",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_26488041",
    "text": "The following sortable table comprises the 230 mountain peaks of greater North America with at least 100 kilometers (62.14 miles) of topographic isolation and at least 500 meters (1640 feet) of topographic prominence.\nThe summit of a mountain or hill may be measured in three principal ways:\n\nThe topographic elevation of a summit measures the height of the summit above a geodetic sea level.\nThe topographic prominence of a summit is a measure of how high the summit rises above its surroundings.\nThe topographic isolation (or radius of dominance) of a summit measures how far the summit lies from its nearest point of equal elevation.\nDenali is one of only three summits on Earth with more than 6000 kilometers (3728 miles) of topographic isolation.  Four major summits of greater North America exceed 2000 kilometers (1243 miles), eight exceed 1000 kilometers (621.4 miles), 35 exceed 500 kilometers (310.7 miles), 107 exceed 200 kilometers (124.3 miles), the following 230 major summits exceed 100 kilometers (62.14 miles), and 413 exceed 50 kilometers (31.07 miles) of topographic isolation.\n\nMajor 100-kilometer summits\nOf these 230 major 100-kilometer summits of North America, 103 are located in the United States (excluding four in Hawaiʻi), 50 in Canada, 33 in México, 21 in Greenland, four in Honduras, three in Cuba, two in Guatemala, two in Haiti, two in Panamá, and one each in the Dominican Republic, Costa Rica, Guadeloupe, Puerto Rico, Jamaica, Saint Kitts and Nevis, Saint Vincent and the Grenadines, Trinidad and Tobago, Nicaragua, Belize, Grenada, and the British Virgin Islands.  Two of these peaks lie on the Canada-United States border and one lies on the Nicaragua-Honduras border.\nGallery\n\nSee also\nNorth America\nGeography of North America\nGeology of North America\nLists of mountain peaks of North America\nList of mountain peaks of North America\nList of the highest major summits of North America\nList of the highest islands of North America\nList of Ultras of North America\nList of the major 100-kilometer summits of North America\nList of extreme summits of North America\nList of mountain peaks of Greenland\nList of mountain peaks of Canada\nList of mountain peaks of the Rocky Mountains\nList of mountain peaks of the United States\nList of mountain peaks of México\nList of mountain peaks of Central America\nList of mountain peaks of the Caribbean\nCategory:Mountains of North America\ncommons:Category:Mountains of North America\nPhysical geography\nTopography\nTopographic elevation\nTopographic prominence\nTopographic isolation\n",
    "source": "wikipedia",
    "title": "List of the major 100-kilometer summits of North America",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_49987405",
    "text": "The following sortable table comprises the 403 mountain peaks of greater North America with at least 3000 meters (9843 feet) of elevation and at least 500 meters (1640 feet) of topographic prominence.\nThe summit of a mountain or hill may be measured in three principal ways:\n\nThe topographic elevation of a summit measures the height of the summit above a geodetic sea level.\nThe topographic prominence of a summit is a measure of how high the summit rises above its surroundings.\nThe topographic isolation (or radius of dominance) of a summit measures how far the summit lies from its nearest point of equal elevation.\nIn greater North America, only Denali exceeds 6000 meters (19,685 feet) elevation.  Three major summits exceed 5500 meters (18,045 feet), 11 exceed 5000 meters (16,404 feet), 21 exceed 4500 meters (14,764 feet), 124 exceed 4000 meters (13,123 feet), 277 exceed 3500 meters (11,483 feet), and the following 403 major summits exceed 3000 meters (9843 feet) elevation.\n\nMajor 3000-meter summits\nOf the 403 major 3000-meter summits of greater North America, 299 are located in the United States (excluding three in Hawaiʻi), 67 in Canada, 30 in México, and eight in Guatemala, four in Greenland, two in Costa Rica, and one each in Panamá and the Dominican Republic.  Eight of these peaks lie on the Canada-United States border and one lies on the México-Guatemala border.  Additional references and maps for the 200 highest of these major summits can be found on the List of the highest major summits of North America.\nGallery\n\nSee also\nNorth America\nGeography of North America\nGeology of North America\nLists of mountain peaks of North America\nList of mountain peaks of North America\nList of the highest major summits of North America\nList of the highest islands of North America\nList of the most prominent summits of North America\nList of the ultra-prominent summits of North America\nList of the most isolated major summits of North America\nList of the major 100-kilometer summits of North America\nList of extreme summits of North America\nList of mountain peaks of Greenland\nList of mountain peaks of Canada\nList of mountain peaks of the Rocky Mountains\nList of mountain peaks of the United States\nList of mountain peaks of México\nList of mountain peaks of Central America\nList of mountain peaks of the Caribbean\nCategory:Mountains of North America\ncommons:Category:Mountains of North America\nPhysical geography\nTopography\nTopographic elevation\nTopographic prominence\nTopographic isolation\n",
    "source": "wikipedia",
    "title": "List of the highest major summits of North America",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_64112368",
    "text": "American Nations: A History of the Eleven Rival Regional Cultures of North America is an American non-fiction book written by Colin Woodard and published in 2011. Woodard proposes a framework for examining American history and current events based on a view of the country as a federation of eleven nations, each defined by a shared culture established by each nation's founding population.\nNoting that the original Thirteen Colonies were established at different times and by different groups with different goals and values, Woodard shows how these colonies both cooperated and competed from their founding. The principles held dear by each colony often conflicted with those of other colonies, and those conflicting agendas shaped the founding and growth of the United States. As the country expanded, the populace that moved into the new territory brought with it the culture of the society from which they came, resulting in nations – a group that shares a common culture and origin – divorced from legal state and international boundaries. American Nations argues that the contrasts between regional cultures, as opposed to state or national borders, provide a more useful and accurate explanation of events and movements.\nWoodard has written two sequels: American Character: A History of the Epic Struggle Between Individual Liberty and the Common Good (Viking, 2016) and Union: The Struggle to Forge the Story of United States Nationhood (Viking, 2020). He has referred to them as an \"informal American Nations trilogy.\"\n\nThe eleven nations\nOf the nations, Woodard explains, \"It isn’t that residents of one or another nation all think the same, but rather that they are all embedded within a cultural framework of deep-seated preferences and attitudes – each of which a person may like or hate, but has to deal with nonetheless.\"\n\nYankeedom began with the Puritans (Calvinist English settlers) in New England and spread across upper New York, the northern parts of Pennsylvania, Ohio, Indiana, Illinois, and Iowa, into the eastern Dakotas, Michigan, Wisconsin, Minnesota, and the Canadian Maritime. The area values education, communal decision-making and aims at creating a religious utopian communal society to be spread over other regions.\nDeep South was settled by former Anglo-American West Indies plantation owners in Charleston, and spread to encompass South Carolina, Georgia, Alabama, Mississippi, Florida, Louisiana, western Tennessee, and the southeastern parts of North Carolina, Arkansas, and Texas. It values old Greco-Roman enlightened, civilized, idle slave society, free-markets and individual freedoms. It has fought centuries with Yankeedom over the dominance of North America, such as in the Civil War and the \"culture wars\" started by the civil rights movement since the 1960s.\nNew Netherland, established by Dutch colonists in the 17th century, is now Greater New York City, as well as the lower Hudson Valley, northern New Jersey, western Long Island, and southwestern C",
    "source": "wikipedia",
    "title": "American Nations",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_25585015",
    "text": "The North American Arctic is composed of the northern polar regions of Alaska (USA), Northern Canada and Greenland. Major bodies of water include the Arctic Ocean, Hudson Bay, the Gulf of Alaska and North Atlantic Ocean. The North American Arctic lies above the Arctic Circle. It is part of the Arctic, which is the northernmost region on Earth. The western limit is the Seward Peninsula and the Bering Strait. The southern limit is the Arctic Circle latitude of 66° 33’N, which is the approximate limit of the midnight sun and the polar night.\nThe Arctic region is defined by environmental limits where the average temperature for the warmest month (July) is below 10 °C (50 °F). The northernmost tree line roughly follows the isotherm at the boundary of this region. The climate of the region is known to be intensely cold during the year due to its extreme polar location. The area has tundra, Arctic vegetation, glaciers, and, for most of the year, is covered in thick blankets of snow and ice.\nIt is home to various species of plants, and land, air and marine animals. Due to the severe weather conditions, the region's flora and fauna has had to adapt to survive. In addition to the extreme climate, permafrost and short growing seasons means that trees are unable to grow. The indigenous peoples who migrated from other lands and settled in the North American Arctic also had to adapt to living conditions. Their population has declined since then, however.\nClimate change in the Arctic has caused the region to feel the effects of global warming, with sea levels and temperatures rising, and a changing wildlife population. Marine ecosystems are struggling under increasing pressure from changes in sea ice characteristics. The effects of climate change have also impacted the human population whose way of living and working is facing pressure from the effects as they are finding it challenging to adapt.\n\nGeography\nThe North American Arctic geographical region consists of large land masses. A major portion of the region also consists of large bodies of water. This region is a relatively flat topographic part of the Earth. Since the region is located in an extreme northerly part of the Earth, the sun may only appear above the horizon for a couple of hours during winter while appearing for longer during summer.\n",
    "source": "wikipedia",
    "title": "North American Arctic",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_18951556",
    "text": "The Arctic Ocean is the smallest and shallowest of the world's five oceanic divisions. It spans an area of approximately 14,060,000 km2 (5,430,000 sq mi) and is the coldest of the world's oceans. The International Hydrographic Organization (IHO) recognizes it as an ocean, although some oceanographers call it the Arctic Mediterranean Sea or North Polar Sea. It has also been described as an estuary of the Atlantic Ocean. It is also seen as the northernmost part of the all-encompassing world ocean.\nThe Arctic Ocean includes the North Pole region in the middle of the Northern Hemisphere and extends south to about 60°N. The Arctic Ocean is surrounded by Eurasia and North America, and the borders follow topographic features: the Bering Strait on the Pacific side and the Greenland Scotland Ridge on the Atlantic side. It is mostly covered by sea ice throughout the year and almost completely in winter. The Arctic Ocean's surface temperature and salinity vary seasonally as the ice cover melts and freezes; its salinity is the lowest on average of the five major oceans, due to low evaporation, heavy fresh water inflow from rivers and streams, and limited connection and outflow to surrounding oceanic waters with higher salinities. The summer shrinking of the ice has been quoted at 50%.The US National Snow and Ice Data Center (NSIDC) uses satellite data to provide a daily record of Arctic sea ice cover and the rate of melting compared to an average period and specific past years, showing a continuous decline in sea ice extent.In September 2012, the Arctic ice extent reached a new record minimum. Compared to the average extent (1979–2000), the sea ice had diminished by 49%.\n\nHistory\n\nGeography\n\nGeology\nThe crystalline basement rocks of mountains around the Arctic Ocean were recrystallized or formed during the Ellesmerian orogeny, the regional phase of the larger Caledonian orogeny in the Paleozoic Era. Regional subsidence in the Jurassic and Triassic periods led to significant sediment deposition, creating many of the reservoirs for current day oil and gas deposits. During the Cretaceous period, the Canadian Basin opened, and tectonic activity due to the assembly of Alaska caused hydrocarbons to migrate toward what is now Prudhoe Bay. At the same time, sediments shed off the rising Canadian Rockies built out the large Mackenzie Delta.\nThe rifting apart of the supercontinent Pangea, beginning in the Triassic period, opened the early Atlantic Ocean. Rifting then extended northward, opening the Arctic Ocean as mafic oceanic crust material erupted out of a branch of Mid-Atlantic Ridge. The Amerasia Basin may have opened first, with the Chukchi Borderland moved along to the northeast by transform faults. Additional spreading helped to create the \"triple-junction\" of the Alpha-Mendeleev Ridge in the Late Cretaceous epoch.\nThroughout the Cenozoic Era, the subduction of the Pacific plate, the collision of India with Eurasia, and the continued opening of the North Atlan",
    "source": "wikipedia",
    "title": "Arctic Ocean",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_145792",
    "text": "The Continental Divide of the Americas (also known as the Great Divide, the Western Divide or simply the Continental Divide; Spanish: Divisoria continental de las Américas, Gran Divisoria) is the principal, and largely mountainous, hydrological divide of the Americas. The Continental Divide extends from the Bering Strait to the Strait of Magellan, and separates the watersheds that drain into the Pacific Ocean from those river systems that drain into the Atlantic and Arctic Ocean, including those that drain into the Gulf of Mexico, the Caribbean Sea, and Hudson Bay.\nAlthough there are many other hydrological divides in the Americas, the Continental Divide is by far the most prominent of these because it tends to follow a line of high peaks along the main ranges of the Rocky Mountains and Andes, at a generally much higher elevation than the other hydrological divisions.\n\nGeography\nBeginning at the westernmost point of the Americas, Cape Prince of Wales, just south of the Arctic Circle, the Continental Divide's geographic path runs through Arctic Alaska, where it reaches its more northerly point close to the U.S.–Canada border near the Beaufort Sea. The Divide zig-zags southwardly over Yukon, and forms part of the boundary between Yukon and the Northwest Territories in the Mackenzie Mountains. It then proceeds through the Northern British Columbia Interior via the Cassiar Mountains, Omineca Mountains and northern Nechako Plateau to Summit Lake, north of the city of Prince George and just south of the community of McLeod Lake. From there the Divide traverses the McGregor Plateau to the spine of the Rockies, following the crest of the Canadian Rockies southeast to the 120th meridian west, from there forming the boundary between southern British Columbia and southern Alberta.\nThe Divide crosses into the United States in northwestern Montana, at the boundary between Waterton Lakes National Park and Glacier National Park. In Canada, it forms the western boundary of Waterton Lakes National Park, and in the US bisects Glacier National Park.  Further south, the Divide forms the backbone of the Rocky Mountain Front (Front Range) in the Bob Marshall Wilderness, heads south towards Helena and Butte, then west past the namesake community of Divide, Montana, through the Anaconda-Pintler Wilderness to the Bitterroot Range, where it forms the eastern third of the state boundary between Idaho and Montana. The Divide crosses into Wyoming within Yellowstone National Park and continues southeast around the Great Divide Basin, through the Sierra Madre Range into Colorado where it reaches its highest point in North America at the summit of Grays Peak at 4,352 metres (14,278 ft). It crosses US Hwy 160 in southwestern Colorado at Wolf Creek Pass, where a line symbolizes the division. The Divide then proceeds south into western New Mexico, passing along the western boundary of the endorheic Plains of San Agustin.  Although the Divide represents the height of land between w",
    "source": "wikipedia",
    "title": "Continental Divide of the Americas",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_427762",
    "text": "This list of North American deserts identifies areas of the continent that receive less than 10 in (250 mm) annual precipitation. The \"North American Desert\" is also the term for a large U.S. Level 1 ecoregion (EPA) of the North American Cordillera, in the Deserts and xeric shrublands biome (WWF). The continent's deserts are largely between the Rocky Mountains and Sierra Madre Oriental on the east, and the rain shadow–creating Cascades, Sierra Nevada, Transverse, and Peninsular Ranges on the west. The North American xeric region of over 95,751 sq mi (247,990 km2) includes three major deserts, numerous smaller deserts, and large non-desert arid regions  in the Western United States and in northeastern, central, and northwestern Mexico.\n\nOverview\nThe following are three major hot and dry deserts in North America, all located in the Southwestern United States and Northern Mexico.\n\nThe Chihuahuan Desert is the largest hot desert in North America, located in the Southwestern United States and Northern Mexico. Its total area is 140,000 sq mi (360,000 km2).\nThe Sonoran Desert is a desert located in the Southwestern United States and northwest Mexico. It is the second largest hot desert in North America. Its total area is 120,000 sq mi (310,000 km2).\nThe Mojave Desert is the hottest desert in North America, located primarily in southeastern California and Southern Nevada. Its total area is 22,000 sq mi (57,000 km2).\nThe largest cold desert is the Great Basin Desert, which encompasses much of the northern Basin and Range Province, north of the Mojave Desert.\nOther cold deserts lie within the Columbia Plateau/Columbia Basin, the Snake River Plain, and the Colorado Plateau regions.\nDesert ecoregions\nListed from north to south, distinct North American desert regions include:\n\nGreat Kobuk Sand Dunes three small deserts in northwestern Alaska, part of the Kobuk Valley National Park\nYukon - Carcross Desert, smallest desert in the world\nWashington – British Columbia – Idaho – Wyoming – Oregon – Nevada\nMuch of the Columbia Basin is desert, such as the\nChanneled Scablands, a desert in the Columbia Basin of eastern Washington\nMost of the Snake River Plain (ecoregion) is sagebrush steppe, but barren lava fields form small deserts, such as\nCraters of the Moon National Monument in Idaho\nThe Wyoming Basin (ecoregion) is dominated by arid grasslands and shrub steppe, but also contains the\nRed Desert (Wyoming)\nOwyhee Desert, in southwestern Idaho, northern Nevada, and southeastern Oregon.\nY P Desert, a portion of the Owyhee Desert in Idaho\nOregon High Desert, aka \"Great Sandy Desert\", eastern Oregon\nAlvord Desert, a dry lake bed.\nNorthwest Lahontan subregion in Nevada-part of the Northern Basin and Range (ecoregion)\nBlack Rock Desert, a dry lake bed.\nGreat Basin Desert\nNevada, dominated by sagebrush steppe\nForty Mile Desert, in northwest Nevada\nSmoke Creek Desert, Nevada (980 sq mi)\nCarson Desert\nUtah\nGreat Salt Lake Desert, Utah\nSevier Desert surrounds the intermittent,",
    "source": "wikipedia",
    "title": "List of North American deserts",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_873541",
    "text": "This is a list of the extreme points of North America: the points that are highest and lowest, and farther north, south, east or west than any other location on the continent. Some of these points are debatable, given the varying definitions of North America.\n\nNorth America and surrounding islands\nNorthernmost point — Kaffeklubben Island, Greenland 83°40′N 29°50′W. Although politically part of the Kingdom of Denmark, Greenland is geologically part of the North American Plate.  If non-permanent islands are included, then the northernmost point is Qeqertaq Avannarleq. If Greenland is excluded from North America, the northernmost point is Cape Columbia, Ellesmere Island, Nunavut, Canada 83°8′N 74°13′W.\nSouthernmost point — Cocos Island, Costa Rica 5°31′8″N 87°4′18″W\nWesternmost point — The westernmost point depends on the definition of \"westernmost\". If following the International Date Line, it would be Cape Wrangell on Attu Island, Alaska, United States 52°55′28″N 172°28′22″E, or the tiny (ca. 200m diameter) Peaked Island, just off the coast to the west.  However, if the border between east and west is defined by the 180th meridian, the westernmost point is the West Point of Amatignak Island 51°17′N 179°9′W, as Attu Island is in the Eastern Hemisphere.\nEasternmost point — Nordostrundingen, Greenland 81°26′25″N 11°29′22″W. If Greenland is excluded, then the easternmost point is Cape Spear, Newfoundland, Canada 47°31′25″N 52°37′10″W.  Again, if one uses the technical definition of longitude, it is Pochnoi Point on Semisopochnoi Island, Alaska, 51°57′N 179°52′E since the state stretches into the Eastern Hemisphere. However, other sources argue it would be Cape Wrangell on Attu Island, Alaska, United States at 52°55′28″N 172°28′22″E, as well as the tiny (ca. 200m diameter) Peaked Island, just off the coast to the west.\nContinental North America\nNorthernmost point —  Murchison Promontory, Canada 71°58′N 094°57′W\nSouthernmost point — Punta Mariato, Panama 7°12′32″N 80°53′10″W\nWesternmost point — Cape Prince of Wales, Alaska 65°35′47″N 168°5′5″W\nEasternmost point — Cape Saint Charles, Labrador 52°13′3″N 55°37′15″W\n",
    "source": "wikipedia",
    "title": "Extreme points of North America",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_50389704",
    "text": "This article comprises four sortable tables of mountain summits of greater North America that are the higher than any other point north or south of their latitude or east or west their longitude in North America.\nThe summit of a mountain or hill may be measured in three principal ways:\n\nThe topographic elevation of a summit measures the height of the summit above a geodetic sea level.\nThe topographic prominence of a summit is a measure of how high the summit rises above its surroundings.\nThe topographic isolation (or radius of dominance) of a summit measures how far the summit lies from its nearest point of equal elevation.\n\nNorthernmost high summits\nThe following summits range from Greenland and Ellesmere Island to Alaska.\nSouthernmost high summits\nThe following summits range from Panamá to Alaska.\nEasternmost high summits\nThe following summits range from Greenland to Costa Rica to Alaska.\nWesternmost high summits\nAll of the following summits are located in the US State of Alaska.\nGallery\n\nSee also\nNorth America\nGeography of North America\nGeology of North America\nLists of mountain peaks of North America\nList of mountain peaks of North America\nList of the highest major summits of North America\nList of the major 5000-meter summits of North America\nList of the major 4000-meter summits of North America\nList of the major 3000-meter summits of North America\nList of the highest islands of North America\nList of the most prominent summits of North America\nList of the ultra-prominent summits of North America\nList of the most isolated major summits of North America\nList of the major 100-kilometer summits of North America\nList of extreme summits of North America\nList of mountain peaks of Greenland\nList of mountain peaks of Canada\nList of mountain peaks of the Rocky Mountains\nList of mountain peaks of the United States\nList of mountain peaks of México\nList of mountain peaks of Central America\nList of mountain peaks of the Caribbean\nCategory:Mountains of North America\ncommons:Category:Mountains of North America\nPhysical geography\nTopography\nTopographic elevation\nTopographic prominence\nTopographic isolation\n",
    "source": "wikipedia",
    "title": "List of extreme summits of North America",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_49301930",
    "text": "Western North America is the western edge of the North American continent that borders the Pacific Ocean. It consists of Alaska at the farthest north, down through the western Canadian province of British Columbia, the western U.S. states of Washington, Oregon, and California, and then Mexico farthest south.  The region consists of one long continuous mountain range formed over the last 350 million years through the movement of tectonic plates, as the large Pacific plate submerged under the North American plate through the process called subduction.\n\nSee also\nPacific Northwest\nGeologic timeline of Western North America\nHistory of the west coast of North America\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Western North America",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_77851547",
    "text": "This article lists the highest natural elevation of each sovereign state on the continent of North America defined physiographically.\n\nSee also\nList of elevation extremes by country\nList of highest points of African countries\nList of highest points of Asian countries\nList of highest points of European countries\nList of highest points of Oceanian countries\nList of highest points of South American countries\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "List of highest points of North American countries",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_13062662",
    "text": "This article comprises three sortable tables of major mountain peaks of greater North America.\nThe summit of a mountain or hill may be measured in three principal ways:\n\nThe topographic elevation of a summit measures the height of the summit above a geodetic sea level.  The first table below ranks the 100 highest major summits of greater North America by elevation.\nThe topographic prominence of a summit is a measure of how high the summit rises above its surroundings.  The second table below ranks the 50 most prominent summits of greater North America.\nThe topographic isolation (or radius of dominance) of a summit measures how far the summit lies from its nearest point of equal elevation.  The third table below ranks the 50 most isolated major summits of greater North America.\n\nHighest major summits\nOf the 100 highest major summits of greater North America, only Denali exceeds 6000 meters (19,685 feet) elevation, 11 peaks exceed 5000 meters (16,404 feet), and all 100 peaks exceed 4076 meters (13,373 feet) elevation.\nOf these 100 peaks, 81 are located in the United States, 17 in Canada, seven in México, and one in Guatemala.  Six of these peaks lie on the Canada-United States border.\nMost prominent summits\nOf the 50 most prominent summits of greater North America, only Denali exceeds 6000 meters (19,685 feet) of topographic prominence, Mount Logan exceeds 5000 meters (16,404 feet), four peaks exceed 4000 meters (13,123 feet), 17 peaks exceed 3000 meters (9843 feet), and all 50 peaks exceed 2343 meters (7687 feet) of topographic prominence.  All of these peaks are ultra-prominent summits.\nOf these 50 peaks, 27 are located in the United States, 19 in Canada, three in México, and one each in Guatemala, Costa Rica, Greenland, the Dominican Republic, and Haiti.  Four of these peaks lie on the Canada-United States border.\nMost isolated major summits\nOf the 50 most isolated major summits of greater North America, only Denali exceeds 4000 kilometers (2485 miles) of topographic isolation, Gunnbjørn Fjeld exceeds 3000 kilometers (1864 miles), four peaks exceed 2000 kilometers (1243 miles), nine peaks exceed 1000 kilometers (621.4 miles), 35 peaks exceed 500 kilometers (310.7 miles), and all 50 peaks exceed 392 kilometers (243.6 miles) of topographic isolation.\nOf these 50 peaks, 16 are located in Canada, 15 in the United States, 7 in Greenland, 6 in México, and one each in the Dominican Republic, Costa Rica, Guatemala, Guadeloupe, Puerto Rico, and Cuba.\n",
    "source": "wikipedia",
    "title": "List of mountain peaks of North America",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_23711165",
    "text": "The Nordic countries (also known as the Nordics or Norden; lit. 'the North') are a geographical and cultural region in Northern Europe, as well as the Arctic and North Atlantic oceans. It includes the sovereign states of Denmark, Finland, Iceland, Norway and Sweden; the autonomous territories of the Faroe Islands and Greenland; and the autonomous region of Åland.\nThe Nordic countries have much in common in their way of life, history, religion and social and economic model. They have a long history of political unions and other close relations but do not form a singular state or federation today. The Scandinavist movement sought to unite Denmark, Norway and Sweden into one country in the 19th century. With the dissolution of the union between Norway and Sweden (Norwegian independence), the independence of Finland in the early 20th century and the 1944 Icelandic constitutional referendum, this movement expanded into the modern organised Nordic cooperation. Since 1962, this cooperation has been based on the Helsinki Treaty that sets the framework for the Nordic Council and the Nordic Council of Ministers.\nThe Nordic countries cluster near the top in numerous metrics of national performance, including education, economic competitiveness, civil liberties, quality of life and human development. Each country has its own economic and social model, sometimes with large differences from its neighbours. Still, they share aspects of the Nordic model of economy and social structure to varying degrees. This includes a mixed market economy combined with strong labour unions and a universalist welfare sector financed by high taxes, enhancing individual autonomy and promoting social mobility. There is a high degree of income redistribution, commitment to private ownership and little social unrest.\nNorth Germanic peoples, who comprise over three-quarters of the region's population, are the largest ethnic group, followed by the Baltic Finnic Peoples, who comprise the majority in Finland; other ethnic groups are the Greenlandic Inuit, the Sami people and recent immigrants and their descendants. Historically, the main religion in the region was Norse paganism. This gave way first to Roman Catholicism after the Christianisation of Scandinavia. Then, following the Protestant Reformation, the main religion became Lutheran Christianity, the state religion of several Nordic countries.\nAlthough the area is linguistically heterogeneous, with three unrelated language groups, the common linguistic heritage is one factor that makes up the Nordic identity. Most Nordic languages belong to one of the North Germanic, Finno-Ugric, and Eskimo-Aleut language families. Danish, Norwegian and Swedish are considered mutually intelligible, and they are the working languages of the region's two political bodies. Swedish is a mandatory subject in Finnish schools and Danish in Faroese schools. Danish is also taught in schools in Iceland.\nThe combined area of the Nordic countries is 3,425,804",
    "source": "wikipedia",
    "title": "Nordic countries",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_240388",
    "text": "The Pacific Coast Ranges (officially gazetted as the Pacific Mountain System in the United States; French: chaînes côtières du Pacifique; Spanish: cadena costera del Pacífico) are the series of mountain ranges that stretch along the West Coast of North America from Alaska south to northern and central Mexico. Although they are commonly thought to be the westernmost mountain range of the continental United States and Canada, the geologically distinct Insular Mountains of Vancouver Island lie farther west.\nThe Pacific Coast Ranges are part of the North American Cordillera (sometimes known as the Western Cordillera, or in Canada, as the Pacific Cordillera and/or the Canadian Cordillera), which includes the Rocky Mountains, the Columbia Mountains, the Interior Mountains, the Interior Plateau, the Sierra Nevada, the Great Basin mountain ranges, and other ranges and various plateaus and basins.\nThe Pacific Coast Ranges designation, however, only applies to the Western System of the Western Cordillera, which comprises the Saint Elias Mountains, Coast Mountains, Insular Mountains, Olympic Mountains, Cascade Range, Oregon Coast Range, California Coast Ranges, Transverse Ranges, Peninsular Ranges, and the Sierra Madre Occidental.\n\nOther uses\nThe term Coast Range is used by the United States Geological Survey to refer only to the ranges south of the Strait of Juan de Fuca in Washington to the California-Mexico border, and to those west of Puget Sound, the Willamette Valley, and the Sacramento and San Joaquin valleys (the California Central Valley).\nThat definition excludes the Sierra Nevada and Cascade Ranges, the Mojave (High), and Sonoran (Low) Deserts, i.e. the Pacific Border province. The same term is used informally in Canada to refer to the Coast Mountains and adjoining inland ranges such as the Hazelton Mountains, and sometimes also the Saint Elias Mountains.\nGeography\nThe character of the ranges varies considerably, from the record-setting tidewater glaciers in the ranges of Alaska, to the rugged Central and Southern California ranges, the Transverse Ranges and Peninsular Ranges, in the chaparral and woodlands eco-region with Oak Woodland, Chaparral shrub forest or Coastal sage scrub-covering them. The coastline is often seen dropping steeply into the sea with photogenic views.  Along the British Columbia and Alaska coast, the mountains intermix with the sea in a complex maze of fjords, with thousands of islands. Off the Southern California coast the Channel Islands archipelago of the Santa Monica Mountains extends for 160 miles (260 km).\nThere are coastal plains at the mouths of rivers that have punched through the mountains spreading sediments, most notably at the Copper River in Alaska, the Fraser River in British Columbia, and the Columbia River between Washington and Oregon. In California: the Sacramento and San Joaquin Rivers' San Francisco Bay, the Santa Clara River's Oxnard Plain, the Los Angeles, San Gabriel, and Santa Ana Rivers' Los Angel",
    "source": "wikipedia",
    "title": "Pacific Coast Ranges",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_23070",
    "text": "The Pacific Ocean is the largest and deepest of Earth's five oceanic divisions. It extends from the Arctic Ocean in the north to the Southern Ocean, or, depending on the definition, to Antarctica in the south, and is bounded by the continents of Asia and Australia in the west and the Americas in the east.\nAt 165,250,000 square kilometers (63,800,000 square miles) in area (as defined with a southern Antarctic border), the Pacific Ocean is the largest division of the World Ocean and the hydrosphere and covers approximately 46% of Earth's water surface and about 32% of the planet's total surface area, larger than its entire land area (148,000,000 km2 (57,000,000 sq mi)). The centers of both the water hemisphere and the Western Hemisphere, as well as the oceanic pole of inaccessibility, are in the Pacific Ocean. Ocean circulation (caused by the Coriolis effect) subdivides it into two largely independent volumes of water that meet at the equator, the North Pacific Ocean and the South Pacific Ocean (or more loosely the South Seas). The Pacific Ocean can also be informally divided by the International Date Line into the East Pacific and the West Pacific, which allows it to be further divided into four quadrants, namely the Northeast Pacific off the coasts of North America, the Southeast Pacific off South America, the Northwest Pacific off Far Eastern/Pacific Asia, and the Southwest Pacific around Oceania.\nThe Pacific Ocean's mean depth is 4,000 meters (13,000 feet). The Challenger Deep in the Mariana Trench, located in the northwestern Pacific, is the deepest known point in the world, reaching a depth of 10,928 meters (35,853 feet). The Pacific also contains the deepest point in the Southern Hemisphere, the Horizon Deep in the Tonga Trench, at 10,823 meters (35,509 feet). The third deepest point on Earth, the Sirena Deep, was also located in the Mariana Trench. It is the warmest ocean, as its temperatures can reach as high as 31°C (88°F) due to it surrounding major and minor Pacific islands, which have a tropical, hot climate.\nThe Pacific has many major marginal seas, including (listed clockwise from the west) the Philippine Sea, South China Sea, East China Sea, Sea of Japan, Sea of Okhotsk, Bering Sea, Gulf of Alaska, Gulf of California, Tasman Sea, and the Coral Sea.\n\nEtymology\nIn 1513, during the Age of Discovery, Spanish explorer Vasco Núñez de Balboa crossed the Isthmus of Panama and sighted the great \"Southern Sea\", which he named Mar del Sur (in Spanish). Afterwards, the ocean's current name was coined by Portuguese explorer Ferdinand Magellan during the Spanish circumnavigation of the world in 1520, as he encountered favorable winds upon reaching the ocean. He called it Mar Pacífico, which in Portuguese and Spanish means 'peaceful sea'.\n",
    "source": "wikipedia",
    "title": "Pacific Ocean",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_36543579",
    "text": "Portages in North America usually began as animal tracks and were improved by tramping or blazing. In a few places iron-plated wooden rails were laid to take a handcart. Heavily used routes sometimes evolved into roads when sledges, rollers or oxen were used, as at Methye Portage.  Sometimes railways were built (Champlain and St. Lawrence Railroad). The basic purpose of most canals is to avoid portages.\nPlaces where portaging occurred often became temporary and then permanent settlements (such as Hull, Quebec; Sault Ste. Marie, Ontario; New Orleans, Louisiana; and Chicago, Illinois). The importance of free passage through portages found them included in laws and treaties. The Northwest Ordinance says \"The navigable waters leading into the Mississippi and St. Lawrence, and the carrying places between the same, shall be common highways and forever free, as well to the inhabitants of the said territory as to the citizens of the United States...\" The Treaty of Greenville between the U.S. and the Indian tribes of the area includes: \"And the said Indian tribes will allow to the people of the United States a free passage by land and by water, as one and the other shall be found convenient, through their country,...\" Then four portages are mentioned specifically. Portages are also used in the treaty to set boundaries (\"The general boundary line between the lands of the United States and the lands of the said Indian tribes, shall begin at the mouth of Cayahoga river, and run thence up the same to the portage...\"). One historically important fur trade portage is now Grand Portage National Monument. Recreational canoeing routes often include portages between lakes, for example, the Seven Carries route in Adirondack Park. Algonquin Park, Boundary Waters Canoe Area Wilderness and Sylvania Wilderness have famous portage routes. \nNumerous portages were upgraded to carriageways and railways due to their economic importance. The Niagara Portage had a gravity railway in the 1760s. The passage between the Chicago and Des Plaines Rivers (and so between the Great Lakes and the Mississippi River systems) was through a short swamp portage which seasonally flooded and it is thought that a channel gradually developed unintentionally from the dragging of the boat bottoms. The 1835 Champlain and St. Lawrence Railroad connected the cities of New York and Montreal without needing to go through the Atlantic. The passage between Lake Superior and Lake Huron was by a portage dragway of greased rails with capstans until a railway was built in 1850 and a canal in 1855. The 5-mile-long Nosbonsing and Nipissing Railway was built just to carry logs between lakes on their way to the sawmill. Allegheny Portage Railroad and Morris Canal both used canal inclined planes to pass loaded boats through portages.\n\nSettlements named for being on a portage\nSometimes the settlements were named for being on a portage. Some places in the United States and Canada so named are:\n\nCranberry Portage, M",
    "source": "wikipedia",
    "title": "Portages in North America",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_14640744",
    "text": "Turtle Island is a name for Earth or North America, used by some American Indian peoples, as well as by some Indigenous rights activists. The name is based on an oral history once common to the indigenous peoples of the Northeastern Woodlands of North America. \nA number of contemporary works continue to use and/or tell the Turtle Island creation story.\n\nLenape\nThe Lenape story of the \"Great Turtle\" was first recorded by European colonists from an interview on October 16, 1679, in the 1679–80 journal of the Labadist missionary Jasper Danckaerts. It was explained to the missionary by the Hackensack elder Tantaqué, who was then visiting the Fort James area in Lower Manhattan. The story is shared by other Northeastern Woodlands tribes, notably the Iroquois peoples.\nThe Lenape believe that before creation there was nothing, an empty dark space. However, in this emptiness, there existed a spirit of their creator, Kishelamàkânk. Eventually in that emptiness, he fell asleep. While he slept, he dreamt of the world as we know it today, the Earth with mountains, forests, and animals. He also dreamt up man, and he saw the ceremonies man would perform. Then he woke up from his dream to the same nothingness he was living in before. Kishelamàkânk then started to create the Earth as he had dreamt it.\nFirst, he created helper spirits, the Grandfathers of the North, East, and West, and the Grandmother of the South. Together, they created the Earth just as Kishelamàkânk had dreamt it. One of their final acts was creating a special tree. From the roots of this tree came the first man, and when the tree bent down and kissed the ground, woman sprang from it.\nAll the animals and humans did their jobs on the Earth, until a problem eventually arose. There was a tooth of a giant bear that could give the owner magical powers, and the humans started to fight over it. Eventually, the wars got so bad that people moved away, and made new tribes and new languages. Kishelamàkânk saw this fighting and decided to send down a spirit, Nanabush, to bring everyone back together. He went on top of a mountain and started the first Sacred Fire, which gave off a smoke that caused all the people of the world to come investigate what it was. When they all came, Nanabush created a pipe with a sumac branch and a soapstone bowl, and the creator gave him Tobacco to smoke with. Nanabush then told the people that whenever they fought with each other, to sit down and smoke tobacco in the pipe, and they would make decisions that were good for everyone.\nThe same bear tooth later caused a fight between two evil spirits, a giant toad and an evil snake. The toad was in charge of all the waters, and amidst the fighting he ate the tooth and the snake. The snake then proceeded to bite his side, releasing a great flood upon the Earth. Nanabush saw this destruction and began climbing a mountain to avoid the flood, all the while grabbing animals that he saw and sticking them in his sash. At the top of the mo",
    "source": "wikipedia",
    "title": "Turtle Island",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_26172677",
    "text": "The following sortable tables comprise the most topographically prominent mountain peaks of greater North America.  Each of these 353 summits has at least 1500 meters (4921 feet) of topographic prominence.\nThis article defines greater North America as the portion of the continental landmass of the Americas extending westward and northward from the Isthmus of Panama plus the islands surrounding that landmass.  This article defines the islands of North America to include the coastal islands of North America, the islands of the Caribbean Sea, the Lucayan Archipelago, the Bermuda Islands, the Islands of Greenland (Kalaallit Nunaat), the islands of Northern Canada, the islands of Alaska, and the islands of the northeastern Pacific Ocean.  The Hawaiian Islands are not included because they are considered part of Oceania. With the exceptions of North Carolina's Mount Mitchell and New Hampshire's Mount Washington, all of the ultras in the United States are found west of the 100th parallel. \nTopographic elevation is the vertical distance above the reference geoid, a mathematical model of the Earth's sea level as an equipotential gravitational surface.  The topographic prominence of a summit is the elevation difference between that summit and the highest or key col to a higher summit.  The topographic isolation of a summit is the minimum great-circle distance to a point of equal elevation.\nAll elevations in the 48 states of the contiguous United States include an elevation adjustment from the National Geodetic Vertical Datum of 1929 (NGVD 29) to the North American Vertical Datum of 1988 (NAVD 88).  For further information, please see this United States National Geodetic Survey note. If a summit elevation or prominence has a range of values, the arithmetic mean is cited.\n\nDistribution\nThe majority of ultra-prominent peaks are in western North America (especially Alaska, Yukon, and British Columbia), as well as a sizeable minority of peaks along the coast of the southern half of Greenland and the northeastern part of Nunavut.\n",
    "source": "wikipedia",
    "title": "List of ultras of North America",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_46961190",
    "text": "The following sortable tables comprise the most topographically prominent mountain peaks of greater North America.  Each of these 353 summits has at least 1500 meters (4921 feet) of topographic prominence.\nThis article defines greater North America as the portion of the continental landmass of the Americas extending westward and northward from the Isthmus of Panama plus the islands surrounding that landmass.  This article defines the islands of North America to include the coastal islands of North America, the islands of the Caribbean Sea, the Lucayan Archipelago, the Bermuda Islands, the Islands of Greenland (Kalaallit Nunaat), the islands of Northern Canada, the islands of Alaska, and the islands of the northeastern Pacific Ocean.  The Hawaiian Islands are not included because they are considered part of Oceania. With the exceptions of North Carolina's Mount Mitchell and New Hampshire's Mount Washington, all of the ultras in the United States are found west of the 100th parallel. \nTopographic elevation is the vertical distance above the reference geoid, a mathematical model of the Earth's sea level as an equipotential gravitational surface.  The topographic prominence of a summit is the elevation difference between that summit and the highest or key col to a higher summit.  The topographic isolation of a summit is the minimum great-circle distance to a point of equal elevation.\nAll elevations in the 48 states of the contiguous United States include an elevation adjustment from the National Geodetic Vertical Datum of 1929 (NGVD 29) to the North American Vertical Datum of 1988 (NAVD 88).  For further information, please see this United States National Geodetic Survey note. If a summit elevation or prominence has a range of values, the arithmetic mean is cited.\n\nDistribution\nThe majority of ultra-prominent peaks are in western North America (especially Alaska, Yukon, and British Columbia), as well as a sizeable minority of peaks along the coast of the southern half of Greenland and the northeastern part of Nunavut.\n",
    "source": "wikipedia",
    "title": "List of ultras of North America",
    "topic": "Geography_of_North_America"
  },
  {
    "id": "wiki_9127632",
    "text": "Biology is the scientific study of life and living organisms. It is a broad natural science that encompasses a wide range of fields and unifying principles that explain the structure, function, growth, origin, evolution, and distribution of life. Central to biology are five fundamental themes: the cell as the basic unit of life, genes and heredity as the basis of inheritance, evolution as the driver of biological diversity, energy transformation for sustaining life processes, and the maintenance of internal stability (homeostasis).\nBiology examines life across multiple levels of organization, from molecules and cells to organisms, populations, and ecosystems. Subdisciplines include molecular biology, physiology, ecology, evolutionary biology, developmental biology, and systematics, among others. Each of these fields applies a range of methods to investigate biological phenomena, including observation, experimentation, and mathematical modeling. Modern biology is grounded in the theory of evolution by natural selection, first articulated by Charles Darwin, and in the molecular understanding of genes encoded in DNA. The discovery of the structure of DNA and advances in molecular genetics have transformed many areas of biology, leading to applications in medicine, agriculture, biotechnology, and environmental science.\nLife on Earth is believed to have originated over 3.7 billion years ago. Today, it includes a vast diversity of organisms—from single-celled archaea and bacteria to complex multicellular plants, fungi, and animals. Biologists classify organisms based on shared characteristics and evolutionary relationships, using taxonomic and phylogenetic frameworks. These organisms interact with each other and with their environments in ecosystems, where they play roles in energy flow and nutrient cycling. As a constantly evolving field, biology incorporates new discoveries and technologies that enhance the understanding of life and its processes, while contributing to solutions for challenges such as disease, climate change, and biodiversity loss.\n\nEtymology\nFrom Greek βίος (bíos) 'life', (from Proto-Indo-European root *gwei-, to live) and λογία (logia) 'study of'. The compound appears in the title of Volume 3 of Michael Christoph Hanow's Philosophiae naturalis sive physicae dogmaticae: Geologia, biologia, phytologia generalis et dendrologia, published in 1766. The term biology in its modern sense appears to have been introduced independently by Thomas Beddoes (in 1799), Karl Friedrich Burdach (in 1800), Gottfried Reinhold Treviranus (Biologie oder Philosophie der lebenden Natur, 1802) and Jean-Baptiste Lamarck (Hydrogéologie, 1802).\n",
    "source": "wikipedia",
    "title": "Biology",
    "topic": "Biology"
  },
  {
    "id": "wiki_2537522",
    "text": "A biologist is a scientist who conducts research in biology. Biologists are interested in studying life on Earth, whether it is an individual cell, a multicellular organism, or a community of interacting populations. They usually specialize in a particular branch (e.g., molecular biology, zoology, and evolutionary biology) of biology and have a specific research focus (e.g., studying malaria or cancer).\nBiologists who are involved in basic research have the aim of advancing knowledge about the natural world. They conduct their research using the scientific method, which is an empirical method for testing hypotheses. Their discoveries may have applications for some specific purpose such as in biotechnology, which has the goal of developing medically useful products for humans.\nIn modern times, most biologists have one or more academic degrees such as a bachelor's degree, as well as an advanced degree such as a master's degree or a doctorate. Like other scientists, biologists can be found working in different sectors of the economy such as in academia, nonprofits, private industry, or government.\n\nHistory\nFrancesco Redi, the founder of experimental biology, is recognized to be one of the greatest biologists of all time. Robert Hooke, an English natural philosopher, coined the term cell, suggesting plant structure's resemblance to honeycomb cells.\nCharles Darwin and Alfred Wallace independently formulated the theory of evolution by natural selection, which was described in detail in Darwin's book On the Origin of Species, which was published in 1859. In it, Darwin proposed that the features of all living things, including humans, were shaped by natural processes of descent with accumulated modification leading to divergence over long periods of time. The theory of evolution in its current form affects almost all areas of biology. Separately, Gregor Mendel formulated the principles of inheritance in 1866, which became the basis of modern genetics.\nIn 1953, James D. Watson and Francis Crick described the basic structure of DNA, the genetic material for expressing life in all its forms, building on the work of Maurice Wilkins and Rosalind Franklin, suggested that the structure of DNA was a double helix.\nIan Wilmut led a research group that in 1996 first cloned a mammal from an adult somatic cell, a Finnish Dorset lamb named Dolly.\n",
    "source": "wikipedia",
    "title": "Biologist",
    "topic": "Biology"
  },
  {
    "id": "wiki_44707607",
    "text": "This is a list of encyclopedias as well as encyclopedic and biographical dictionaries published on the subject of biology in any language.\n\nEntries are in the English language unless specifically stated as otherwise.\n\nGeneral biology\nBecher, Anne, Joseph Richey. American environmental leaders: From colonial times to the present. Grey House, 2008. ISBN 9781592371198.\nButcher, Russell D., Stephen E. Adair, Lynn A. Greenwalt. America's national wildlife refuges: A complete guide. Roberts Rinehart Publishers in cooperation with Ducks Unlimited, 2003. ISBN 1570983798.\nCullen, Katherine E. (2009). Encyclopedia of Life Science. Infobase Publishing. ISBN 978-0-8160-7008-4.\nEcological Internet, Inc. EcoEarth.info: Environment portal and search engine. Ecological Internet, Inc. [1].\nEncyclopedia of Life Sciences. John Wiley & Sons. 25 May 2007. ISBN 978-0-470-06651-5.\nEncyclopedia of Life Sciences. Groves Dictionaries Incorporated. 1 November 2001. ISBN 978-1-56159-238-8.\nFriday, Adrian & Davis S. Ingram. The Cambridge Encyclopedia of Life Sciences. Cambridge, 1985.\nGaither, Carl C., Alma E. Cavazos-Gaither, Andrew Slocombe. Naturally speaking: A dictionary of quotations on biology, botany, nature and zoology. Institute of Physics, 2001. ISBN 0750306815.\nGibson, Daniel, National Audubon Society. Audubon guide to the national wildlife refuges. Southwest: Arizona, Nevada, New Mexico, Texas. St. Martin's Griffin, 2000. ISBN 0312207778.\nGoudie, Andrew, David J. Cuff. Encyclopedia of global change: Environmental change and human society. Oxford University Press, 2002. ISBN 0195108256.\nGove, Doris. Audubon guide to the national wildlife refuges. Southeast : Alabama, Florida, Georgia, Kentucky, Mississippi, North Carolina, Puerto Rico, South Carolina, Tennessee, U.S. Virgin Islands. St. Martin's Griffin, 2000. ISBN 0312241283.\nGrassy, John. Audubon guide to the national wildlife refuges: Northern Midwest: Illinois, Indiana, Iowa, Michigan, Minnesota, Nebraska, North Dakota, Ohio, South Dakota, Wisconsin. St. Martin's Griffin, c2000. ISBN 0312243154.\nGrassy, John. Audubon guide to the national wildlife refuges: Rocky Mountains: Colorado, Idaho, Montana, Utah, Wyoming. St. Martin's Griffin, 2000. ISBN 0312245742.\nGray, Peter. Encyclopedia of the Biological Sciences. Krieger, 1981.\nGrinstein, Louise S., Carol A. Biermann, Rose K. Rose. Women in the biological sciences: A biobibliographic sourcebook. Greenwood Press, 1997. ISBN 0313291802.\nHancock, John M., Marketa J. Zvelebil. Dictionary of bioinformatics and computational biology. Wiley-Liss, 2004. ISBN 0471436224.\nHosansky, David. The environment A to Z. CQ Press, 2001. ISBN 1568025831.\nLaubach, René. Audubon guide to the national wildlife refuges. New England : Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont. St. Martin's Griffin, 2000. ISBN 0312204507.\nMac Arthur, Loren, Debbie S. Miller. Audubon guide to the national wildlife refuges. Alaska and the Northwest: Alaska, Oregon, Washington.",
    "source": "wikipedia",
    "title": "Bibliography of encyclopedias: biology",
    "topic": "Biology"
  },
  {
    "id": "wiki_55931886",
    "text": "A bioactive terrarium (or vivarium) is a terrarium for housing one or more terrestrial animal species that includes live plants and populations of small invertebrates and microorganisms to consume and break down the waste products of the primary species. In a functional bioactive terrarium, the waste products will be broken down by these detritivores, reducing or eliminating the need for cage cleaning. Bioactive vivariums are used by zoos and hobbyists to house reptiles and amphibians in an aesthetically pleasing and enriched environment.\n\nEnclosure\nAny terrarium can be made bioactive by addition of the appropriate substrate, plants, and detritivores. Bioactive enclosures are often maintained as display terraria constructed of PVC, wood, glass and/or acrylic. Bioactive enclosures in laboratory \"rack\" style caging are uncommon.\nCleanup crew\nWaste products of the primary species are consumed by a variety of detritivores, referred to as the \"cleanup crew\" by hobbyists.  These can include woodlice, springtails, earthworms, millipedes, and various beetles, with different species being preferred in different habitats - the cleanup crew for a tropical rainforest bioactive terrarium may rely primarily on springtails, isopods, and earthworms, while a desert habitat might use beetles.  If the primary species is insectivorous, they may consume the cleanup crew, and thus the cleanup crew must have sufficient retreats to avoid being completely depopulated.\nAdditionally, bioactive terraria typically have a flourishing population of bacteria and other microorganisms which break down the wastes of the cleanup crew and primary species.  Fungi may occur as part of the terrarium cycle and will be consumed by the cleanup crew.\nSubstrate\nBioactive enclosures require some form of substrate to grow plants and to provide habitat for the cleanup crew.  The choice of substrate is typically determined by the habitat of the primary species (e.g. jungle vs desert), and created by mixing a variety of components such as organic topsoil (free of pesticides and non-biological fertilizers), peat, coco fiber, sand, long-fiber sphagnum moss, cypress mulch, and orchid bark in varying proportions.  \nIn wet habitats, there is typically a drainage layer beneath the substrate to allow water to pool without saturating the substrate. The drainage layer may be constructed via coarse gravel, stones, expanded clay aggregate, or may be wholly synthetic; the drainage layer is typically separated from the overlying substrate with a fine plastic mesh.  Additionally, some bioactive terraria include leaf-litter, which can serve as food and microhabitat for the cleanup crew.\n",
    "source": "wikipedia",
    "title": "Bioactive terrarium",
    "topic": "Biology"
  },
  {
    "id": "wiki_78359137",
    "text": "Bioliteracy is the ability to understand and engage with biological topics. The concept is used particularly in the contexts of biotechnology and biodiversity.\n\nDescription\nIn the biotechnology context, bioliteracy is considered important for promoting the biotechnology industry and the development of biological engineering products. It has also been defined as \"the concept of imbuing people, personnel, or teams with an understanding of and comfort with biology and biotechnology.\" The use in the context of biodiversity is somewhat distinct, focusing on improving awareness of different organisms with the goal of conservation.\nCitizen science initiatives, such as iNaturalist, are considered effective ways to increase bioliteracy, engaging students with the direct observation of nature.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Bioliteracy",
    "topic": "Biology"
  },
  {
    "id": "wiki_6920635",
    "text": "Biological constraints are factors which make populations resistant to evolutionary change. One proposed definition of constraint is \"A property of a trait that, although possibly adaptive in the environment in which it originally evolved, acts to place limits on the production of new phenotypic variants.\"  Constraint has played an important role in the development of such ideas as homology and body plans.\n\nTypes of constraint\nAny aspect of an organism that has not changed over a certain period of time could be considered to provide evidence for \"constraint\" of some sort. To make the concept more useful, it is therefore necessary to divide it into smaller units. First, one can consider the pattern of constraint as evidenced by phylogenetic analysis and the use of phylogenetic comparative methods; this is often termed phylogenetic inertia, or phylogenetic constraint. It refers to the tendency of related taxa sharing traits based on phylogeny.  Charles Darwin spoke of this concept in his 1859 book \"On the Origin of Species\", as being \"Unity of Type\" and went on to explain the phenomenon as existing because organisms do not start over from scratch, but have characteristics that are built upon already existing ones that were inherited from their ancestors; and these characteristics likely limit the amount of evolution seen in that new taxa due to these constraints. \nIf one sees particular features of organisms that have not changed over rather long periods of time (many generations), then this could suggest some constraint on their ability to change (evolve). However, it is not clear that mere documentation of lack of change in a particular character is good evidence for constraint in the sense of the character being unable to change. For example, long-term stabilizing selection related to stable environments might cause stasis.  It has often been considered more fruitful, to consider constraint in its causal sense: what are the causes of lack of change?\nRelationships of constraint classes\nAlthough they are separate, the types of constraints discussed are nevertheless relatable to each other. In particular, stabilizing selection, mechanical, and physical constraints might lead through time to developmental integration and canalisation. However, without any clear idea of any of these mechanisms, deducing them from mere patterns of stasis as deduced from phylogenetic patterns or the fossil record remains problematic.  In addition, the terminology used to describe constraints has led to confusion.\n",
    "source": "wikipedia",
    "title": "Biological constraints",
    "topic": "Biology"
  },
  {
    "id": "wiki_4318932",
    "text": "The biology of romantic love has been explored by such biological sciences as evolutionary psychology, evolutionary biology, anthropology and neuroscience. Neurochemicals and hormones such as dopamine and oxytocin are studied along with a variety of interrelated brain systems which produce the psychological experience and behaviors of romantic love.\nThe study of romantic love is still in its infancy. As of 2021, there were a total of 42 biological studies on romantic love.\n\nDefinition of romantic love\nThe meaning of the term \"romantic love\" has changed considerably throughout history, making it difficult to simply define. Initially it was coined to refer to certain attitudes and behaviors described in a body of literature now referred to as courtly love. However, academic psychology and especially biology also consider romantic love in a different sense, which refers to a brain system (or systems) related to pair bonding or mating with associated psychological properties.\nBode and Kushnick undertook a comprehensive review of romantic love from a biological perspective in 2021. They considered the psychology of romantic love, its mechanisms, development across the lifespan, functions, and evolutionary history. Based on the content of that review, they proposed a biological definition of romantic love:\n\nRomantic love is a motivational state typically associated with a desire for long-term mating with a particular individual. It occurs across the lifespan and is associated with distinctive cognitive, emotional, behavioral, social, genetic, neural, and endocrine activity in both sexes. Throughout much of the life course, it serves mate choice, courtship, sex, and pair-bonding functions. It is a suite of adaptations and by-products that arose sometime during the recent evolutionary history of humans.\nRomantic love in this sense is also not necessarily \"dyadic\", \"social\" or \"interpersonal\", despite being related to pair bonding. Romantic love can be experienced outside the context of a relationship, for example in the case of unrequited love where the feelings are not reciprocated. A person can develop romantic love feelings before any relationship has occurred, for only a potential partner. The potential partner can even be somebody they do not know well or are not acquainted with at all, as in cases of love at first sight and parasocial attachments.\nThe early stage of romantic love (which has obsessive and addictive features) might also be referred to as being \"in love\", passionate love, infatuation, limerence or obsessive love. Research has never settled on a unified terminology or set of methods. Distinctions are drawn between this early stage of romantic love and the \"attachment system\" theorized by the attachment theorists like John Bowlby. In the past, attachment theorists have argued that attachment theory and attachment styles can replace other theories of love, but academics on love have argued this is incorrect and that romantic love and att",
    "source": "wikipedia",
    "title": "Biology of romantic love",
    "topic": "Biology"
  },
  {
    "id": "wiki_43317198",
    "text": "Biospeleology, also known as cave biology, is a branch of biology dedicated to the study of organisms that live in caves and are collectively referred to as troglofauna.\n\nBiospeleology as a science\n\nNotable biospeleologists\nEmil Racoviță, co-founder of biospeleology, the first Romanian to go to the arctic, collected the type specimens of the flightless midge Belgica antarctica\nCarl Eigenmann, is credited with identifying and describing for the first time 195 genera containing nearly 600 species of fishes of North America and South America\nLouis Fage, was a founding member of the Commission de spéléologie\nRené Jeannel, co-founder of biospeleology\nCurt Kosswig, is known as the Father of Turkish Zoology.\nBoris Sket, Approximately 35 animal species are named sketi, and three genera.\nEndre Dudich, founder and committee member of the Hungarian Speleological Society in 1926.\nKarel Absolon, though more famous for his archaeological and speleological discoveries, Absolon started his career out as a biospeleologist.\nBibliography\nBernard Collignon, Caving, scientific approaches., Edisud 1988\nFabien Steak, Approach biospéologie, File EFS Instruction No. 116, 1st Edition, 1997\nC. Delamare-Debouteville, Life in caves, PUF, Que sais-je?, Paris 1971\nBernard Gèze, Scientific caving, Seuil, Paris, 1965, p. 137-167\nR. and V. Decou Ginet, Introduction to biology and groundwater ecology, University Publishing Delarge 1977\nRené Jeannel, Animal cave in France, Lechevalier, Paris, 1926\nRené Jeannel, Living fossils caves, Gallimard, Paris, 1943\nEdward Alfred Martel, Groundwater evolution, Flammarion, Paris, 1908, p. 242-289\nGeorges Émile Racovitza, Essay on biospéologiques problems, I Biospeologica 1907\nMichel Siffre, Animals sinkholes and caves, Hachette, 1979\nMichel Siffre, France The caves and caverns, ed. Privat, 1999, p. 136-153\nG. and R. Thines Tercafs, Atlas of the underground life: the cave animals, Boubée (Paris) and De Visscher (Brussels), 1972\nAlbert Vandel Biospéologie: the biology of cave animals, Gauthier-Villars, Paris, 1964\nArmand Vire, The subterranean fauna of France, Paris, 1900\n",
    "source": "wikipedia",
    "title": "Biospeleology",
    "topic": "Biology"
  },
  {
    "id": "wiki_78136945",
    "text": "The cancer exodus hypothesis establishes that circulating tumor cell clusters (CTC clusters) maintain their multicellular structure throughout the metastatic process. It was previously thought that these clusters must dissociate into single cells during metastasis. According to the hypothesis, CTC clusters intravasate (enter the bloodstream), travel through circulation as a cohesive unit, and extravasate (exit the bloodstream) at distant sites without disaggregating, significantly enhancing their metastatic potential. This concept is considered a key advancement in understanding of cancer biology and CTCs role in cancer metastasis.\n\nMechanism\nTraditionally, it was believed that CTC clusters needed to dissociate into individual cells during their journey through the bloodstream to seed secondary tumors. However, recent studies show that CTC clusters can travel through the bloodstream intact, enabling them to perform every step of metastasis while maintaining their group/cluster structure. \nThe cancer exodus hypothesis asserts that CTC clusters have several distinct advantages that increase their metastatic potential:\n\nHigher metastatic efficiency: CTC clusters have been shown to possess superior seeding capabilities at distant sites compared to single CTCs.\nSurvival and proliferation: The collective nature of CTC clusters allows them to share resources and offer intercellular support, improving their overall survival rates in the bloodstream.\nResistance to treatment: CTC clusters exhibit unique gene expression profiles that contribute to their ability to evade certain cancer therapies, making them more resistant than individual tumor cells.\nClinical relevance\nThe cancer exodus hypothesis offers important insights into how metastasis occurs and highlights the significance of CTC clusters in cancer progression. Detecting and analyzing CTC clusters through liquid biopsies could offer valuable information about the aggressiveness and metastatic potential of cancers. This information is particularly useful for identifying patients who may benefit from more aggressive treatment strategies.\n",
    "source": "wikipedia",
    "title": "Cancer exodus hypothesis",
    "topic": "Biology"
  },
  {
    "id": "wiki_38495892",
    "text": "Chlororespiration is a respiratory process that is thought to occur in plant chloroplasts, involving the electron transport chain (ETC) in the thylakoid membrane. It is thought to involve NAD(P)H dehydrogenase (NDH) and plastid terminal oxidase (PTOX/IMMUTANS), forming an ETC utilizing molecular oxygen as the electron acceptor. This process also interacts with the ETC in the mitochondrion where respiration takes place, as well as with photosynthesis. If photosynthesis is inhibited by environmental stressors like water deficiency, increased heat, and/or increased/decreased light exposure, or even chilling stress, then chlororespiration is one of the crucial ways that plants use to compensate for chemical energy synthesis.\n\nChlororespiration – the latest model\nInitially, the presence of chlororespiration as a legitimate respiratory process in plants was heavily doubted. However, experimentation on Chlamydomonas reinhardtii discovered plastoquinone (PQ) to be a redox carrier. The role of this redox carrier is to transport electrons from the NAD(P)H enzyme to an oxidase enzyme on the thylakoid membrane. Using this cyclic electron chain around photosystem I (PSI), chlororespiration compensates for the lack of light. This cyclic pathway also allows electrons to re-enter the PQ pool through NDH, which is then used to supply ATP to plant cells.\n\nIn the year 2002, the discovery of the molecules: plastid terminal oxidase (PTOX) and NDH complexes, have revolutionised the concept of chlororespiration. Using evidence from experimentation on a Meillandina rose, this latest model observes the role of PTOX to be an enzyme that prevents the PQ pool from over-reducing, by stimulating its reoxidation. Whereas, the NDH complexes are responsible for providing a gateway for electrons to form an ETC. The presence of such molecules are apparent in the non-appressed thylakoid membranes of higher plants like Meillandina roses.\nThe relation between chlororespiration, photosynthesis and respiration\nExperimentation with respiratory oxidase inhibitors (for instance, cyanide) on unicellular algae has revealed interactive pathways to be present between chloroplasts and mitochondria. Metabolic pathways responsible for photosynthesis are present in chloroplasts, whereas respiratory metabolic pathways are present in mitochondria. In these pathways, metabolic carriers (like phosphate) exchange NAD(P)H molecules between photosynthetic and respiratory ETCs. Evidence using mass spectrometry on algae and photosynthetic mutants of Chlamydomonas discovered that oxygen molecules were also being exchanged between photosynthetic and chlororespiratory ETCs. The mutant Chlamydomonas alga species lacks photosystems I and II (PSI and PSII), so when the alga underwent flash-induced PSI activity, it resulted in no effect on mitochondrial pathways of respiration. Instead, this flash-induced PSI activity caused an exchange between photosynthetic and chlororespiratory ETCs, which was observed using ",
    "source": "wikipedia",
    "title": "Chlororespiration",
    "topic": "Biology"
  },
  {
    "id": "wiki_76151489",
    "text": "A dermestarium (pl. dermestaria) is a room, container, or terrarium where taxidermists let invertebrates - typically beetle larvae - remove (eat) flesh and other soft parts from animal carcasses or parts of animal carcasses, such as skulls. The purpose is to expose and clean individual bones or entire skeletons for study and preservation. The word dermestarium derives from Dermestes, the genus name of the beetle most commonly used for the task. Other invertebrates, notably isopods and clothes moths, have also been used.\nDermestaria are used by museums and other research institutions. Dermestaria are sometimes located in separate, bespoke buildings, as is the case at the University of Wisconsin-Madison's zoological museum. Other dermestaria are of the container or terrarium type and can be kept inside regular research and museum facilities. The use of dermestaria in the USA goes back to at least the 1940s.\nPrior to dermestarium deposition, the animal carcass is skinned, and excess flesh and other soft parts are removed to minimize the dermestarium time needed. The carcass is left to dry somewhat, since Dermestes larvae prefer dry over moist meat. The carcass is then put on a tray and placed inside the dermestarium, which has been prepared with a colony of Dermestes larvae. The dermestarium time needed for the larvae to strip the bones clean of flesh and soft parts ranges from a few days for small animals to several years for very large animals; once the bones have been stripped clean of all soft parts, the tray with the carcass is removed and placed in a freezer for a period sufficient to kill off any larvae and adult beetles that happened to tag along.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Dermestarium",
    "topic": "Biology"
  },
  {
    "id": "wiki_790808",
    "text": "Endogeny, in biology, refers to the property of originating or developing from within an organism, tissue, or cell. \nFor example, endogenous substances, and endogenous processes are those that originate within a living system (e.g. an organism or a cell). For instance, estradiol is an endogenous estrogen hormone produced within the body, whereas ethinylestradiol is an exogenous synthetic estrogen, commonly used in birth control pills.\nIn contrast, exogenous substances and exogenous processes are those that originate from outside of an organism.\n\nReferences\n\nExternal links\n The dictionary definition of endogeny at Wiktionary\n",
    "source": "wikipedia",
    "title": "Endogeny (biology)",
    "topic": "Biology"
  },
  {
    "id": "wiki_56880369",
    "text": "Erinacin is a antihaemorrhagic protein first isolated in 1996 from the European Hedgehog, which is thought to convey (partial) immunity to snake venom.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Erinacin",
    "topic": "Biology"
  },
  {
    "id": "wiki_255468",
    "text": "Excretion is elimination of metabolic waste, which is an essential process in all organisms. In vertebrates, this is primarily carried out by the lungs, kidneys, and skin. This is in contrast with secretion, where the substance may have specific tasks after leaving the cell. For example, placental mammals expel urine from the bladder through the urethra, which is part of the excretory system. Unicellular organisms discharge waste products directly through the surface of the cell. Another example would be how mammals release solid waste (feces) through the anus during defecation.\nDuring activities such as cellular respiration, several chemical reactions take place in the body. These are known as metabolism. These chemical reactions produce waste products such as carbon dioxide, water, salts, urea and uric acid. Accumulation of these wastes beyond a level inside the body is harmful to the body. The excretory organs remove these wastes. This process of removal of metabolic waste from the body is known as excretion.\n\nProcesses across various types of life\n\nSee also\n\nReferences\n\nExternal links\n\nUAlberta.ca, Animation of excretion\nBrian J Ford on leaf fall in Nature\n",
    "source": "wikipedia",
    "title": "Excretion",
    "topic": "Biology"
  },
  {
    "id": "wiki_79707214",
    "text": "The concept of functional information is an attempt to rigorously define the information content of biological systems. The concept was originated by a group led by Jack W. Szostak in 2003.\n\nDefinition\nThey define functional information as follows:\n\nthe concept of degree of function is introduced, where the degree of function \n  \n    \n      \n        \n          E\n          \n            x\n          \n        \n      \n    \n    {\\displaystyle E_{x}}\n  \n is a non-negative objective measure of the capability of system \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n to do the physical function \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n.\nthe fraction of possible configurations of the system that can achieve at least a particular level of function \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n in regard to the physical function \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n is defined to be \n  \n    \n      \n        F\n        (\n        \n          E\n          \n            x\n          \n        \n        ≥\n        θ\n        )\n      \n    \n    {\\displaystyle F(E_{x}\\geq \\theta )}\n  \n\nthe functional information relative to a given level of function \n  \n    \n      \n        \n          E\n          \n            x\n          \n        \n        =\n        θ\n      \n    \n    {\\displaystyle E_{x}=\\theta }\n  \n is defined as \n  \n    \n      \n        I\n        (\n        \n          E\n          \n            x\n          \n        \n        ≥\n        θ\n        )\n        =\n        −\n        l\n        o\n        \n          g\n          \n            2\n          \n        \n        F\n        (\n        \n          E\n          \n            x\n          \n        \n        ≥\n        θ\n        )\n      \n    \n    {\\displaystyle I(E_{x}\\geq \\theta )=-log_{2}F(E_{x}\\geq \\theta )}\n  \n\nThis leads to two conclusions:\n\nbecause all possible configurations can achieve zero or more functionality, that is to say \n  \n    \n      \n        F\n        (\n        \n          E\n          \n            x\n          \n        \n        ≥\n        0\n        )\n        =\n        1\n      \n    \n    {\\displaystyle F(E_{x}\\geq 0)=1}\n  \n, the minimum possible functional information for a system is \n  \n    \n      \n        −\n        l\n        o\n        \n          g\n          \n            2\n          \n        \n        1\n      \n    \n    {\\displaystyle -log_{2}1}\n  \n, which is zero.\nfor the highest possible level of a degree of function of a system \n  \n    \n      \n        \n          E\n          \n            x\n          \n        \n        =\n        \n          θ\n          \n            m\n            a\n            x\n          \n        \n      \n    \n    {\\displaystyle E_{x}=\\theta _{max}}\n  \n, there will be a well defined \n  \n    \n      \n        I\n        (\n        \n          E\n          \n            x\n          \n        \n        =\n        \n          θ\n          \n            m\n            a\n            x\n          \n        \n        )\n        =\n        −\n        l\n        o\n  ",
    "source": "wikipedia",
    "title": "Functional information",
    "topic": "Biology"
  },
  {
    "id": "wiki_18101603",
    "text": "High throughput biology (or high throughput cell biology) is the use of automation equipment with classical cell biology techniques to address biological questions that are  otherwise unattainable using conventional methods. It may incorporate techniques from optics, chemistry, biology or image analysis to permit rapid, highly parallel research into how cells function, interact with each other and how pathogens exploit them in disease.\nHigh throughput cell biology has many definitions, but is most commonly defined by the search for active compounds in natural materials like in medicinal plants. This is also known as high throughput screening (HTS) and is how most drug discoveries are made today, many cancer drugs, antibiotics, or viral antagonists have been discovered using HTS. The process of HTS also tests substances for potentially harmful chemicals that could be potential human health risks. HTS generally involves hundreds of samples of cells with the model disease and hundreds of different compounds being tested from a specific source. Most often a computer is used to determine when a compound of interest has a desired or interesting effect on the cell samples.\nUsing this method has contributed to the discovery of the drug Sorafenib (Nexavar). Sorafenib is used as medication to treat multiple types of cancers, including renal cell carcinoma (RCC, cancer in the kidneys), hepatocellular carcinoma (liver cancer), and thyroid cancer. It helps stop cancer cells from reproducing by blocking the abnormal proteins present. In 1994, high throughput screening for this particular drug was completed. It was initially discovered by Bayer Pharmaceuticals in 2001. By using a RAF kinase biochemical assay, 200,000 compounds were screened from medicinal chemistry directed synthesis or combinatorial libraries to identify active molecules against activeRAF kinase. Following three trials of testing, it was found to have anti-angiogenic effects on the cancers, which stops the process of creating new blood vessels in the body.\nAnother discovery made using HTS is Maraviroc. It is an HIV entry inhibitor, and slows the process and prevents HIV from being able to enter human cells. It is used to treat a variety of cancers as well, reducing or blocking the metastasis of cancer cells, which is when cancer cells spread to a completely different part of the body from where it started. High throughput screening for Maraviroc was completed in 1997, and finalized in 2005 by Pfizer global research and development team.\nHigh-throughput biology serves as one facet of what has also been called \"omics research\" - the interface between large scale biology (genome, proteome, transcriptome), technology and researchers. High throughput cell biology has a definite focus on the cell, and methods accessing the cell such as imaging, gene expression microarrays, or genome wide screening. The basic idea is to take methods normally performed on their own and do a very large number of them w",
    "source": "wikipedia",
    "title": "High throughput biology",
    "topic": "Biology"
  },
  {
    "id": "wiki_75185459",
    "text": "Interdigitation is the interlinking of biological components that resembles the fingers of two hands being locked together. It can be a naturally occurring or man-made state.\n\nExamples\nNaturally occurring interdigitation includes skull sutures that develop during periods of brain growth, and which remain thin and straight, and later develop complex fractal interdigitations that provide interlocking strength. A layer of the retina where photoreception occurs is called the interdigitation zone. Adhesion or diffusive bonding occurs when sections of polymer chains from one surface interdigitate with those of an adjacent surface. In the dermis, dermal papillae (DP) (singular papilla, diminutive of Latin papula, 'pimple') are small, nipple-like extensions  of the dermis into the epidermis, also known as interdigitations. The distal convoluted tubule (DCT), a portion of kidney nephron, can be recognized by several distinct features, including lateral membrane interdigitations with neighboring cells.\nSome hypotheses contend that crown shyness, the interdigitation of canopy branches, leads to \"reciprocal pruning\" of adjacent trees.\nInterdigitation is also found in biological research. Interdigitation fusion is a method of preparing calcium- and phosphate-loaded liposomes. Drugs inserted in the bilayer biomembrane may influence the lateral organization of the lipid membrane, with interdigitation of the membrane to fill volume voids. A similar interdigitation process involves investigating dissipative particle dynamics (DPD) simulations by adding alcohol molecules to the bilayers of double-tail lipids. Pressure-induced interdigitation is used to study hydrostatic pressure of bicellular dispersions containing anionic lipids.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Interdigitation",
    "topic": "Biology"
  },
  {
    "id": "wiki_78141628",
    "text": "Plasmagene is a term used to describe genetic elements that exist outside of the nucleus, typically within the cytoplasm of a cell. These elements are usually associated with organelles like mitochondria and chloroplasts, which contain their own genetic material and replicate independently of the nuclear genome. Plasmagene theory as proposed by Tracy Sonneborn has significantly contributed to the understanding of non-Mendelian inheritance patterns, where traits are passed through cytoplasmic inheritance rather than through nuclear DNA.\n\nFunction and characteristics\nPlasmagenes play crucial roles in various cellular processes, especially those involved in energy production. For instance, mitochondrial plasmagenes are integral to oxidative phosphorylation, the process responsible for generating most of the cell's ATP, the main energy currency of cells. Though they can replicate independently, plasmagenes are often semi-autonomous, as they rely on nuclear genes for many essential proteins that support their functions.\nInheritance and implications\nOne of the most noteworthy aspects of plasmagenes is their involvement in non-Mendelian inheritance patterns. Unlike nuclear genes, which are inherited from both parents, plasmagenes are typically inherited maternally. This occurs because cytoplasmic organelles, like mitochondria, are transferred primarily through the egg cell during fertilization. Consequently, mutations or abnormalities in plasmagenes are linked to various inherited disorders, particularly those affecting muscular and neurological systems.\nHistorical context and research\nResearch on plasmagenes dates back to the mid-20th century, focusing on their role in extranuclear inheritance and its implications for genetic diseases. These studies have been instrumental in elucidating how genetic information can be passed outside the nuclear DNA, altering the understanding of inheritance patterns and disease transmission. The plasmagene theory was later disproved, and in 1976 Sonneborn affirmed this. But later researches after Sonneborn's death, provided validation allowing continued studies.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Plasmagene",
    "topic": "Biology"
  },
  {
    "id": "wiki_78237247",
    "text": "Poison exons (PEs); also called premature termination codon (PTC) exons or nonsense-mediated decay (NMD) exons] are a class of cassette exons that contain PTCs. Inclusion of a PE in a transcript targets the transcript for degradation via NMD. PEs are generally highly conserved elements of the genome and are thought to have important regulatory roles in biology. Targeting PE inclusion or exclusion in certain transcripts is being evaluated as a therapeutic strategy.\n\nDiscovery\nIn 2002, a model termed regulated unproductive splicing and translation (RUST) was proposed based on the finding that many (~one-third) alternatively spliced transcripts contain PEs. In this model, coupling alternative splicing to NMD (AS-NMD) is thought to tune transcript levels to regulate protein expression. Alternative splicing may also lead to NMD via other pathways besides PE inclusion, e.g., intron retention.\nPEs were initially characterized in RNA-binding proteins from the SR protein family. Genes for other RNA-binding proteins (RBPs) such as those for heterogenous nuclear ribonucleoprotein (hnRNP) also contain PEs. Numerous chromatin regulators also contain PEs, though these are less conserved than PEs within RBPs such as the SR proteins. Multiple spliceosomal components contain PEs. Certain PEs may occur only in specific tissues.\nPE-containing transcripts generally represent a minority of the overall transcript population, in part due to their active degradation via NMD, though this relative abundance can be elevated upon inhibition of NMD or certain biological states. Certain PE-containing transcripts are resistant to NMD and may be translated into truncated proteins.\nRegulation\nCis-regulatory elements neighboring PEs have been found to affect PE inclusion.\nMany proteins whose corresponding genes contain PEs autoregulate PE inclusion in their respective transcripts and thereby control their own levels via a feedback loop. Cross-regulation of PE inclusion has also been observed.\nDifferential splicing of PEs is implicated in biological processes such as differentiation, neurodevelopment, dispersal of nuclear speckles during hypoxia, tumorigenesis, organism growth, and T cell expansion.\nProtein kinases that regulate phosphorylation of splicing factors can affect splicing processes, thus kinase inhibitors may affect inclusion of PEs. For example, CMGC kinase inhibitors and CDK9 inhibitors have been found to induce PE inclusion in RBM39.\nSmall molecules that modulate chromatin accessibility can affect PE inclusion.\nMutations in splicing factors can lead to inclusion of PEs in certain transcripts.\nPE inclusion can be regulated by external variables such as temperature and electrical activity. For example, PE inclusion in RBM3 transcript is lowered during hypothermia. This is mediated by temperature-dependent binding of the splicing factor HNRNPH1 to the RBM3 transcript. The neuronal RBPs NOVA1/2 are translocated from the nucleus to the cytoplasm during pilocarpine-induce",
    "source": "wikipedia",
    "title": "Poison exon",
    "topic": "Biology"
  },
  {
    "id": "wiki_78277844",
    "text": "The term polylecty or generalist is used in pollination ecology to refer to bees that collect pollen from a range of unrelated plants.  Honey bees exemplify this behavior,  collecting nectar from a wide array of flowers. Other predominantly polylectic genera include Bombus, Ceratina, Heriades and Halictus. The opposite term is oligolecty, and refers to bees that exhibit a narrow, specialized preference for pollen sources, typically to a single family or genus of flowering plants. \nRoughly two-thirds of bee species in Europe are polylectic, relying on a diverse array of pollen sources. This broad foraging approach allows these generalist bees to thrive in various environments and quickly adjust to shifting conditions. However, despite their adaptability, they are less efficient pollen gatherers than oligolectic bees, whose  morphology is often specialized for more effective pollen collection from a narrower range of plants.\nA species that exhibits polylecty is known as polylectic. This term should not be confused with Polylectic as a grammatical term, which has a similar etymology to the biological definition but instead refers to the adjective of a multi-word term, as opposed to a monolectic which is the adjective for a one-word term.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Polylecty",
    "topic": "Biology"
  },
  {
    "id": "wiki_78142714",
    "text": "Spatial biology is the study of biomolecules and cells in their native three-dimensional context. Spatial biology encompasses different levels of cellular resolution including (1) subcellular localization of DNA, RNA, and proteins, (2) single-cell resolution and in situ communications like cell-cell interactions and cell signaling, (3) cellular neighborhoods, regions, or microenvironments, and (4) tissue architecture and organization in organs. Dysregulation of tissue organization is a common feature in human disease progression including tumorigenesis and neurodegeneration. Many fields within biology are studied for their individual contribution to spatial biology.\n\nSpatial transcriptomics\nSpatial transcriptomics measures mRNA transcript abundance and distribution in situ across a tissue. Spatial method for RNA in situ detection is first described in a 1969 landmark paper by Joseph G. Gall and Mary-Lou Pardue. Previous to spatial transcriptomics techniques, whole transcriptome profiling lacked spatial context because tissues were ground up in bulk RNA-seq or dissociated into single cells suspensions in single cell RNA-seq. Although some literature refers to \"spatial genomics\" for RNA, growing consensus has settled on usage of \"spatial transcriptomics\" or \"spatially resolved transcriptomics.\"\nSpatial proteomics\nSpatial proteomics measures the localization and abundance of proteins at the subcellular level across a tissue. Immunohistochemistry-based spatial proteomic methods include oligo barcoded antibodies, cyclic immunofluorescence (cycIF), co-detection by indexing (CODEX), iterative bleaching extends multiplicity (IBEX), multiplexed ion beam imaging (MIBI) and imaging mass cytometry (IMC). Other methods includes deep visual proteomics that profile protein expression in single cells by laser capture microdissection and mass spectroscopy. The term \"spatial medicine\" is recently coined by Eric Topol to refer to a study that used deep visual proteomics to find a therapeutic treatment for patients with a rare skin condition.\n",
    "source": "wikipedia",
    "title": "Spatial biology",
    "topic": "Biology"
  },
  {
    "id": "wiki_44057611",
    "text": "Tokogeny or tocogeny is the biological relationship between parent and offspring, or more generally between ancestors and descendants.  In contradistinction to phylogeny it applies to individual organisms as opposed to species.\nIn the tokogentic system shared characteristics are called traits.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Tokogeny",
    "topic": "Biology"
  },
  {
    "id": "wiki_20433613",
    "text": "The universality–diversity paradigm (UDP) is the analysis of biological materials based on the universality and diversity of its fundamental structural elements and functional mechanisms. The analysis of biological systems based on this classification has been a cornerstone of modern biology.\n\nExample: proteins\nFor example, proteins constitute the elementary building blocks of a vast variety of biological materials such as cells, spider silk or bone, where they create extremely robust, multi-functional materials by self-organization of structures over many length- and time scales, from nano to macro. Some of the structural features are commonly found in many different tissues, that is, they are highly conserved. Examples of such universal building blocks include alpha-helices, beta-sheets or tropocollagen molecules. In contrast, other features are highly specific to tissue types, such as particular filament assemblies, beta-sheet nanocrystals in spider silk or tendon fascicles. This coexistence of universality and diversity is an overarching feature in biological materials and a crucial component of materiomics. It might provide guidelines for bioinspired and biomimetic material development, where this concept is translated into the use of inorganic or hybrid organic-inorganic building blocks.\nSee also\nBionics\nMateriomics\nNanotechnology\nPhylogenetics\nReferences\nAckbarow, Theodor; Buehler, Markus J. (2008-07-01). \"Hierarchical Coexistence of Universality and Diversity Controls Robustness and Multi-Functionality in Protein Materials\". Journal of Computational and Theoretical Nanoscience. 5 (7). American Scientific Publishers: 1193–1204. Bibcode:2008JCTN....5.1193A. doi:10.1166/jctn.2008.2554. ISSN 1546-1955.\n[1] Going nature one better (MIT News Release, October 22, 2010).\n[2] S. Cranford, M. Buehler, Materiomics: biological protein materials, from nano to macro, Nanotechnology, Science and Applications, Vol. 3, pp. 127–148, 2010.\nBuehler, Markus J. (2010). \"Tu(r)ning weakness to strength\". Nano Today. 5 (5). Elsevier BV: 379–383. doi:10.1016/j.nantod.2010.08.001. ISSN 1748-0132.\n[3] S. Cranford, M.J. Buehler, Biomateriomics, 2012 (Springer, New York).\n",
    "source": "wikipedia",
    "title": "Universality–diversity paradigm",
    "topic": "Biology"
  },
  {
    "id": "wiki_22939",
    "text": "Physics is the scientific study of matter, its fundamental constituents, its motion and behavior through space and time, and the related entities of energy and force. It is one of the most fundamental scientific disciplines. A scientist who specializes in the field of physics is called a physicist.\nPhysics is one of the oldest academic disciplines. Over much of the past two millennia, physics, chemistry, biology, and certain branches of mathematics were a part of natural philosophy, but during the Scientific Revolution in the 17th century, these natural sciences branched into separate research endeavors. Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms studied by other sciences and suggest new avenues of research in these and other academic disciplines such as mathematics and philosophy.\nAdvances in physics often enable new technologies. For example, advances in the understanding of electromagnetism, solid-state physics, and nuclear physics led directly to the development of technologies that have transformed modern society, such as television, computers, domestic appliances, and nuclear weapons; advances in thermodynamics led to the development of industrialization; and advances in mechanics inspired the development of calculus.\n\nHistory\nThe word physics comes from the Latin physica ('study of nature'), which itself is a borrowing of the Greek φυσική (phusikḗ 'natural science'), a term derived from φύσις (phúsis 'origin, nature, property').\nCore theories\nPhysics deals with a wide variety of systems, although certain theories are used by all physicists. Each of these theories was experimentally tested numerous times and found to be an adequate approximation of nature.\nThese central theories are important tools for research into more specialized topics, and any physicist, regardless of their specialization, is expected to be literate in them. These include classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, and special relativity.\n",
    "source": "wikipedia",
    "title": "Physics",
    "topic": "Physics"
  },
  {
    "id": "wiki_80358385",
    "text": "The Atominstitute (German: Atominstitut) is an Austrian University research facility with its own inhouse nuclear reactor located in Vienna. The institute most known member is 2022 Nobel laureate Anton Zeilinger.\nAdditional to the academic activities, the inspectors of the International Atomic Energy Agency (IAEA), headquartered nearby in Vienna, use the reactor for training and education, before being sent to deployments sites worldwide. These nuclear facilities include those in warzones like the Nuclear program of Iran, and nuclear power stations in Ukraine, e.g. (Zaporizhia).\n\nHistory and location\nIn the 1950s, the institute was founded as common nuclear physics research facility of the Austrian Universities. With its  nuclear research reactor, which was officially commissioned in 1962, it is today the only facility remaining in Austria that has a running nuclear fission reactor. \nAs in 2025 the official name of the institute in English is \"Institute of Atomic and Subatomic Physics\". However internationally the easier to remember name of Atomic Institute, or Atominstitute is more widespread in use. Administratively it is part of Vienna University of Technology (TU Wien) and, together with the Institutes of Theoretical, Applied and Solid State Physics, forms the Faculty of Physics of this university.\nUnusual for a nuclear reactor, the institute is located within the densely populated 2nd district of Vienna, Leopoldstadt, less than 3km from the city center. It boards the most popular large recreational park of Vienna, the Prater, and the most western branch of the Danube stream, the Donaukanal.\nCurrent routine activity at nuclear reactor and cooperation with IAEA\nA central inhouse facility is the TRIGA Mark II research reactor. It is used for University teaching, together with other research infrastructure, at the institute. The instate allows for practical education in the handling and work with radioactive materials and ionizing radiation. In addition to research and teaching in the fields of reactor physics, radiation protection, radiopharmaceuticals, radiochemistry and archaeometry, the area of reactor operation management is covered also. This includes organisation and practice in radiation protection, security and nuclear safety.\nThe know-how in nuclear management methods is fruitful for the routine cooperation with the International Atomic Energy Agency (IAEA), headquartered at the UNO City located at the main branch of the Danube, only few kilometers away. The reactor allows for the theoretical and practical training of international IQEA experts in live radiation fields. These experts serve then as inspectors for nuclear programmes and facilities worldwide within the United Nations framework of the  non-proliferations of nuclear weapons (NPT).\nAdditionally, the reactor is used for education at the college level on nuclear physics also. Over 1.000 undergraduate students visit the facility in guided tours annually.\n",
    "source": "wikipedia",
    "title": "Atominstitute",
    "topic": "Physics"
  },
  {
    "id": "wiki_74985603",
    "text": "In solid state physics, edge states are the topologically protected electronic states that exist at the boundary of the material and cannot be removed without breaking the system's symmetry.\n\nBackground\n\nThe electronic band structure of materials is primarily studied based on the extent of the band gap, the gap between highest occupied valence bands and lowest unoccupied conduction bands. The possible energy level of the material that provides the discrete energy values of all possible states in the energy profile diagram can be represented by solving the Hamiltonian of the system. This solution provides the corresponding energy eigenvalues and eigenvectors. Based on the energy eigenvalues, conduction band are the high energy states (energy E > 0) while valence bands are the low energy states (E < 0). In some materials, for example, in graphene and zigzag graphene quantum dot, there exists the energy states having energy eigenvalues exactly equal to zero (E = 0) besides the conduction and valence bands. These states are called edge states which modifies the electronic and optical properties of the materials significantly.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Edge states",
    "topic": "Physics"
  },
  {
    "id": "wiki_78147827",
    "text": "In space physics, an electrostatic solitary wave (ESW) is a type of electromagnetic soliton occurring during short time scales (when compared to the general time scales of variations in the average electric field) in plasma. When a rapid change occurs in the electric field in a direction parallel to the orientation of the magnetic field, and this perturbation is caused by a unipolar or dipolar electric potential, it is classified as an ESW.\nSince the creation of ESWs is largely associated with turbulent fluid interactions, some experiments use them to compare how chaotic a measured plasma's mixing is. As such, many studies which involve ESWs are centered around turbulence, chaos, instabilities, and magnetic reconnection.\n\nHistory\nThe discovery of solitary waves in general is attributed to John Scott Russell in 1834, with their first mathematical conceptualization being finalized in 1871 by Joseph Boussinesq (and later refined and popularized by Lord Rayleigh in 1876). However, these observations and solutions were for oscillations of a physical medium (usually water), and not describing the behavior of non-particle waves (including electromagnetic waves). For solitary waves outside of media, which ESWs are classified asa, the first major framework was likely developed by Louis de Broglie in 1927, though his work on the subject was temporarily abandoned and was not completed until the 1950s.\nElectrostatic structures were first observed near Earth's polar cusp by Donald Gurnett and Louis A. Frank using data from the Hawkeye 1 satellite in 1978. However, it is Michael Temerin, William Lotko, Forrest Mozer, and Keith Cernyb who are credited with the first observation of electrostatic solitary waves in Earth's magnetosphere in 1982. Since then, a wide variety of magnetospheric satellites have observed and documented ESWs, allowing for analysis of them and the surrounding plasma conditions.\nDetection\nElectrostatic solitary waves, by their nature, are a phenomenon occurring in the electric field of a plasma. As such, ESWs are technically detectable by any instrument that can measure changes to the electric field during a sufficiently short time window. However, since a given plasma's electric field can vary widely depending on the properties of the plasma and since ESWs occur in short time windows, detection of ESWs can require additional screening of the data in addition to the measurement of the electric field itself. One solution to this obstacle for detecting ESWs, implemented by NASA's Magnetospheric Multiscale Mission (MMS), is to use a digital signal processor to analyze the electric field data and isolate short-duration spikes as a candidate for an ESW. Though the following detection algorithm is specific to MMS, other ESW-detecting algorithms function on similar principles.\nTo detect an ESW, the data from a device measuring the electric field is sent to the digital signal processor. This data is analyzed across a short time window (in the case ",
    "source": "wikipedia",
    "title": "Electrostatic solitary wave",
    "topic": "Physics"
  },
  {
    "id": "wiki_80064998",
    "text": "In statistical physics, frenesy is a measure of the dynamical activity of a system's microscopic trajectories under non-equilibrium conditions. Frenesy complements the notion of entropy production (which measures time-antisymmetric aspects associated with irreversibility), and represents how frequently states are visited or how many transitions occur over time, as well as how busy the system's trajectories are. It relates to reactivities, escape rates, and residence times of a physical state, as it quantifies the rate of microscopic configuration changes that accompany entropy production in nonequilibrium steady states.\n\nOrigin and context\nThe concept of frenesy was introduced in 2006 in the study of non-equilibrium processes. In systems described by trajectory ensembles or path-space measures (e.g. arising from Markov processes or Langevin dynamics), frenesy corresponds to the time-symmetric component of the action functional, which includes trajectory-dependent quantities such as escape rates, undirected traffic, and total configuration changes. As with many physical observables, the change in frenesy is often the relevant measurable quantity, especially in the context of non-equilibrium response theory.\nThe role of dynamical activity in trajectory ensembles was explored in the study of large deviations. The need to characterize the time-symmetric fluctuation sector was emphasized in another 2006 paper by Christian Maes et al. Earlier work had referred to this quantity as \"traffic\". A year later, the term \"frenetic\" was introduced in the framework of response theory.\nMathematically, for stochastic trajectories obeying local detailed balance, entropy production quantifies the asymmetry between forward and time-reversed paths, while frenesy quantifies the symmetric contribution invariant under time reversal. It therefore measures changes in dynamical activity or quiescence relative to a reference process and level of description.\nRole in fluctuation–response\nFrenesy contributes to the generalization of fluctuation–dissipation relations beyond equilibrium. In non-equilibrium steady states, the linear response of an observable depends on correlations with both entropy production and frenesy. This correction helps describe response phenomena in systems driven far from equilibrium.\nExtending the Kubo and Green–Kubo formalisms, non-equilibrium linear response theory decomposes the response into an \"entropic\" term and a \"frenetic\" term. The frenetic component is absent in equilibrium but becomes significant under external driving forces. This behavior appears in non-equilibrium versions of the Sutherland–Einstein relation, where mobility depends not only on the diffusion matrix of the unperturbed system but also on force–current correlations. The frenetic term can lead to negative responses, such as in differential mobility or non-equilibrium specific heats. This effect—where the response decreases despite stronger driving—has theoretical support in se",
    "source": "wikipedia",
    "title": "Frenesy (physics)",
    "topic": "Physics"
  },
  {
    "id": "wiki_79966709",
    "text": "Haloscopes are experimental devices designed to detect axions, hypothetical particles that are candidates for dark matter. These instruments typically use a resonant microwave cavity placed in a strong magnetic field to convert axions into detectable photons via the Primakoff effect.\nHaloscopes probe for axions in a specific mass range and operate by tuning the cavity to resonate at frequencies corresponding to those masses. They have provided the lowest limits to the axion-photon coupling constant in their mass region. They are a part of the current experimental effort in search for axions.\n\nThe most well-known haloscope experiment to date is ADMX (Axion Dark Matter eXperiment). Other axion experiments, like IAXO (International AXion Observatory), may incorporate haloscope techniques in its broader axion detection strategy. One of these techniques is RADES (Relic Axion Dark matter Exploratory Setup) which was operated in CAST.\nHaloscope techniques, have also been proposed for the detection of high-frequency gravitational waves. In these concepts, a resonant cavity placed in a strong magnetic field can convert gravitational wave energy into electromagnetic signals through axion-like couplings or other beyond-standard-model interactions. Such approaches aim to explore gravitational wave frequencies in the MHz to GHz range, which are not accessible to conventional interferometers like LIGO or Virgo.\n\nReferences\n\nBibliography\nCrescini, Nicolò (14 March 2022). \"Building instructions for a ferromagnetic axion haloscope\". The European Physical Journal Plus. 137 (3). arXiv:2201.04081. doi:10.1140/epjp/s13360-022-02533-w. Retrieved 16 July 2025.\n",
    "source": "wikipedia",
    "title": "Haloscope (physics)",
    "topic": "Physics"
  },
  {
    "id": "wiki_76868734",
    "text": "The HUN-REN Wigner Research Centre for Physics  is the largest Hungarian research institute studying physics. Formerly a research institute of the Hungarian Academy of Sciences, it became a member of the Eötvös Loránd Research Network  and after the ELKH's reorganisation it became part of the HUN-REN Hungarian Research Network. The Wigner Research Centre was established in 2012 by the merger of the MTA KFKI Institute for Particle and Nuclear Physics and the MTA Institute for Solid State Physics and Optics, and takes the name of the Nobel Prize winning physicist Eugene Wigner. The research centre has two institutes, the Wigner Institute for Particle and Nuclear Physics  and the Wigner Institute for Solid State Physics and Optics.\n\nHistory\nThe predecessor of the research centre was the Central Research Institute of Physics of the Hungarian Academy of Sciences, founded in 1950. Originally established with two departments, the institute was soon expanded by several departments under the leadership of researchers such as Károly Simonyi and Lajos Jánossy. According to the preparatory committee, its aim was \"to raise Hungarian physics research from its present state, which is far behind that of other disciplines, and to enable productive scientific research in all fields of physics which are of primary importance for the development and application of science\". From the very beginning, KFKI has been the home to a wide variety of research, and Wigner FK is no different. The direct or indirect exploitation of results has always been a feature. Physics was not the only field at the institute, but also various technical and even life sciences. After the change of regime, the KFKI was dissolved in 1992. and its scientific institutes were given full autonomy within the MTA.\nSubsequently, the MTA Wigner Research Centre for Physics  was established on January 1, 2012, by the merger of the former MTA KFKI Institute for Particle and Nuclear Physics and the former MTA Institute for Solid State Physics and Optics. Since 2013, the Wigner Data Centre has been part of the Research Centre. From September 1, 2019, the Wigner RCP has been under the management of the Eötvös Loránd Research Network, and continues to operate as an MTA Institute of excellence, today one of the largest research institutes for physics at the HUN-REN. The main research areas are: quantum technology, experimental and theoretical particle physics, nuclear physics, general relativity and gravity, fusion plasma physics, space physics, nuclear materials science, experimental and theoretical solid-state physics, statistical physics, atomic physics, optics and materials science. Wigner RCP is also part of many international collaborations, experiments and projects, such as ALICE experiment, CMS, Na61  or EuPRAXIA\nResearchers working in Wigner RCP or in its predecessors: Géza Györgyi, Lajos Jánossy, István Kovács, Vlagyimir Naumovics Gribov, Károly Simonyi, Dezső Kiss, Zoltán Perjés, József Zimányi, Pé",
    "source": "wikipedia",
    "title": "HUN-REN Wigner Research Centre for Physics",
    "topic": "Physics"
  },
  {
    "id": "wiki_79826334",
    "text": "Joaquim da Costa Ribeiro (Rio de Janeiro, July 8th 1906 - July 29th, 1960) was a Brazilian physicist and university professor in Brazil. He discovered the thermodielectric effect, also known as the Workman-Reynolds in the US. Ribeiro was a member of the Brazilian Academy of Sciences and was the first Scientific Director of CNPq.\nHe is the father of anthropologist Yvonne Maggie and grandfather of movie author Ana Costa Ribeiro, who directed \"Termodielétrico\", a memoir film about him and his legacy.\n\nBiography\nCosta Ribeiro was born at his family's house, on Barão de Itapejipe street, 82, in what was then the federal district of Brazil. His parents were Antonio Marques da Costa Ribeiro and Maria Constança Alburquerque da Costa Ribeiro. His father and grandfather, after whom Joaquim was named, were judges. \nCosta Ribeiro studied in a Jesuit school called Santo Inácio, until 1923, enrolling in the National School of Engineering the next year at the University of Brazil.\nTen years later, he got tenure at the same university. In 1940, Costa Ribeiro started researching new methods to measure radioactivity, and later studied the production of electret using several dielectric materials\nCosta Ribeiro observed that, during electret formation, electric current was unnecessary: the dielectric's natural freezing was enough to electrify the end material, provided that one of the cooling phases was solid.\nThe phenomenom was named \"thermodielectric effect\" by Costa Ribeiro and fully described by him in a 1944 article in the Annals of the Brazilian Academy of Sciences (ABC) that gathered significant attention at home and abroad. This marked the first physical phenomenom completely observed and described by a Brazilian researcher.\nTwo years later, he got a permanent position in general and experimental Physics at the National Philosophy College, at the same university.\nDeath\nCosta Ribeiro died on July 29, 1960 at 54 years of age, at the Casa de Saúde Santa Lúcia. He was survived by his nine children.\n",
    "source": "wikipedia",
    "title": "Joaquim da Costa Ribeiro",
    "topic": "Physics"
  },
  {
    "id": "wiki_78751748",
    "text": "Lofting, sometimes referred to as \"trajectory shaping\", is a trajectory optimization technique used in some missile systems to extend range and improve target engagement effectiveness, usually in beyond-visual range scenarios.\n\nMethod\nLofting involves a missile ascending to a higher altitude after launch, creating a parabolic arc similar to ballistic missiles, before descending toward its target. This elevated flight path allows the missile to capitalize on reduced air resistance at higher altitudes, increasing both the missile's potential energy and the kinetic energy during terminal guidance, thus enabling greater range and probability of kill.\nPeak altitiude of a lofted trajectory can be at altitudes ranging from 20,000–110,000 ft (6–34 km), with most air-to-air missiles peaking at around 80,000–100,000 ft (24–30 km), although the peaks of ballistic missiles' parabolic arcs can range from 50 km (164,042 ft) to 1,500 km (4,921,260 ft).\nAdvantages\nLofting offers several distinct advantages compared to sea-skimming and direct-intercept trajectories, particularly in beyond-visual-range engagements.\nUnlike sea-skimming, which prioritizes low-altitude flight to avoid radar detection but suffers from increased drag and limited range, lofting allows the missile to ascend to higher altitudes where air resistance is lower. This reduced drag enables greater range and energy efficiency, allowing the missile to retain more kinetic energy for terminal guidance and target interception.\nCompared to direct-intercept trajectories, lofting also improves engagement flexibility by providing a steeper attack angle, which is particularly effective against maneuvering or high-altitude targets.\nDisadvantages\nIn comparison to sea-skimming trajectories, lofting lacks radar-avoidance characteristics, making it susceptible to detection by its target and potential interceptors.\nLofting is also more mathematically and technologically complex in comparison to direct-interception, and is only viable in long-range engagements.\nAdditionally, the thinner air which lofting utilizes to reduce drag and increase range carries the downside of impeding the ability for control surfaces to maneuver the missile. This can reduce a missile's ability to adjust for fast-moving or maneuvering targets, however can be circumvented with the use of thrust vectoring - at the downside of added cost and complexity.\n",
    "source": "wikipedia",
    "title": "Missile lofting",
    "topic": "Physics"
  },
  {
    "id": "wiki_844186",
    "text": "Modern physics is a branch of physics that developed in the early 20th century and onward or branches greatly influenced by early 20th century physics. Notable branches of modern physics include quantum mechanics, special relativity, and general relativity.\nClassical physics is typically concerned with everyday conditions: speeds are much lower than the speed of light, sizes are much greater than that of atoms, and energies are relatively small. Modern physics, however, is concerned with more extreme conditions, such as high velocities that are comparable to the speed of light (special relativity), small distances comparable to the atomic radius (quantum mechanics), and very high energies (relativity). In general, quantum and relativistic effects are believed to exist across all scales, although these effects may be very small at human scale. While quantum mechanics is compatible with special relativity (See: Relativistic quantum mechanics), one of the unsolved problems in physics is the unification of quantum mechanics and general relativity, which the Standard Model of particle physics currently cannot account for.\nModern physics is an effort to understand the underlying processes of the interactions of matter using the tools of science and engineering. In a literal sense, the term modern physics means up-to-date physics. In this sense, a significant portion of so-called classical physics is modern. However, since roughly 1890, new discoveries have caused significant paradigm shifts: especially the advent of quantum mechanics (QM) and relativity (ER). Physics that incorporates elements of either QM or ER (or both) is said to be modern physics. It is in this latter sense that the term is generally used.\nModern physics is often encountered when dealing with extreme conditions. Quantum mechanical effects tend to appear when dealing with \"lows\" (low temperatures, small distances), while relativistic effects tend to appear when dealing with \"highs\" (high velocities, large distances), the \"middles\" being classical behavior. For example, when analyzing the behavior of a gas at room temperature, most phenomena will involve the (classical) Maxwell–Boltzmann distribution. However, near absolute zero, the Maxwell–Boltzmann distribution fails to account for the observed behavior of the gas, and the (modern) Fermi–Dirac or Bose–Einstein distributions have to be used instead.\n\nVery often, it is possible to find – or \"retrieve\" – the classical behavior from the modern description by analyzing the modern description at low speeds and large distances (by taking a limit, or by making an approximation). When doing so, the result is called the classical limit.\n\nHallmark experiments\nThese are generally considered to be experiments regarded leading to the foundation of modern physics:\n",
    "source": "wikipedia",
    "title": "Modern physics",
    "topic": "Physics"
  },
  {
    "id": "wiki_168907",
    "text": "Naïve physics or folk physics is the untrained human perception of basic physical phenomena. In the field of artificial intelligence the study of naïve physics is a part of the effort to formalize the common knowledge of human beings.\nMany ideas of folk physics are simplifications, misunderstandings, or misperceptions of well-understood phenomena, incapable of giving useful predictions of detailed experiments, or simply are contradicted by more thorough observations. They may sometimes be true, be true in certain limited cases, be true as a good first approximation to a more complex effect, or predict the same effect but misunderstand the underlying mechanism.\nNaïve physics is characterized by a mostly intuitive understanding humans have about objects in the physical world. Certain notions of the physical world may be innate.\n\nExamples\nSome examples of naïve physics include commonly understood, intuitive, or everyday-observed rules of nature:\n\nWhat goes up must come down\nA dropped object falls straight down\nA solid object cannot pass through another solid object\nA vacuum sucks things towards it\nAn object is either at rest or moving, in an absolute sense\nTwo events are either simultaneous or they are not\nMany of these and similar ideas formed the basis for the first works in formulating and systematizing physics by Aristotle and the medieval scholastics in Western civilization. In the modern science of physics, they were gradually contradicted by the work of Galileo, Newton, and others. The idea of absolute simultaneity survived until 1905, when the special theory of relativity and its supporting experiments discredited it.\nPsychological research\nThe increasing sophistication of technology makes possible more research on knowledge acquisition. Researchers measure physiological responses such as heart rate and eye movement in order to quantify the reaction to a particular stimulus. Concrete physiological data is helpful when observing infant behavior, because infants cannot use words to explain things (such as their reactions) the way most adults or older children can.\nResearch in naïve physics relies on technology to measure eye gaze and reaction time in particular. Through observation, researchers know that infants get bored looking at the same stimulus after a certain amount of time. That boredom is called habituation. When an infant is sufficiently habituated to a stimulus, he or she will typically look away, alerting the experimenter to his or her boredom. At this point, the experimenter will introduce another stimulus. The infant will then dishabituate by attending to the new stimulus. In each case, the experimenter measures the time it takes for the infant to habituate to each stimulus.\nAs an example of the use of this method, research by Susan Hespos and colleagues studied five-month-old infants' responses to the physics of liquids and solids. Infants in this research were shown liquid being poured from one glass to another until they were ",
    "source": "wikipedia",
    "title": "Naïve physics",
    "topic": "Physics"
  },
  {
    "id": "wiki_77326503",
    "text": "Negative air ions (NAI) are negatively charged single gas molecules or ion clusters in the air. They play a role in maintaining the charge balance of the atmosphere. The main components of air are molecular nitrogen and oxygen. Due to the strong electronegativity of oxygen and oxygen-containing molecules, they can easily capture electrons to form negatively charged air ions, most of which are superoxide radicals ·O2−, so NAI are mainly composed of negative oxygen ions, also called air negative oxygen ions.\nThe ions can be produced by natural means such as lightning, or artificially by methods such as a corona discharge. They can play a role in electrostatic removal of air particulates both for industrial applications and with indoor air, and there are claims that they have a beneficial health effect although the evidence for this is weak.\n\nGeneration mechanism\nVarious negative air ions are formed by combining active neutral molecules and electrons in the gas through a series of ion-molecule reactions.\nIn the air, due to the presence of many water molecules, the negative air ions typically combine with water to form hydrated negative air ions, such as O−·(H2O)n, O2−·(H2O)n, O3−·(H2O)n, OH−·(H2O)n, CO3−·(H2O)n, HCO3−·(H2O)n, CO4−·(H2O)n, NO2−·(H2O)n, NO3−·(H2O)n, etc. The ion clusters formed by the combination of small ions and water molecules have a longer survival period due to their large volume and the fact that the charge is protected by water molecules and is not easy to transfer. This is because in the molecular collision, the larger the molecular volume, the less energy is lost when encountering collisions with other molecules, thereby extending the survival time of negative air ions.\nGeneration methods\nNegative air ions can be produced by two methods: natural or artificial. The methods of producing negative air ions in nature include the waterfall effect, lightning ionization and from plants. Natural methods can produce a large number of negative air ions. The artificial means of producing negative air ions include corona discharge and water vapour. Compared with the negative air ions produced in nature, although artificial methods can produce high levels of negative air ions, there are differences in the types and concentrations of negative air ions.\n",
    "source": "wikipedia",
    "title": "Negative air ions",
    "topic": "Physics"
  },
  {
    "id": "wiki_78245824",
    "text": "In condensed matter physics, the Nottingham effect is a surface cooling and heating mechanism that occurs during field and thermionic electron emission. The effect is named after physicist Wayne B. Nottingham who explained it in a commentary to 1940 experiments by Gertrude M. Fleming and Joseph E. Henderson.\nThe temperature at which electron emission goes from heating to cooling is called the Nottingham inversion temperature.\n\nDescription\nNotably, the effect can be either heating or cooling of the surface emitting the electrons, depending upon the energy at which they are supplied. Above the Nottingham inversion temperature, the emission energy exceeds the Fermi energy of the electron supply and the emitted electron carries more energy away from the surface than is returned by the supply of a replacement electron, and the net heat flux from the Nottingham effect switches from heating to cooling the cathode.\nAlong with Joule heating, the Nottingham effect contributes to the thermal equilibrium of electron emission systems, typically becoming the dominant contributor at very high emission current densities. It comes into play in the operation of field emission array cathodes and other devices that rely upon stimulating Fowler-Nordheim electron emission, usually at the apex of a sharp tip used to create a field enhancement effect.  In extreme cases, the Nottingham effect can heat the emitter tips to temperatures exceeding the melting point of the tip material, causing the tip to deform and emit material that may cause a vacuum arc; this is a significant failure mode for tip-based cathodes.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Nottingham effect",
    "topic": "Physics"
  },
  {
    "id": "wiki_1996857",
    "text": "In thermodynamics, nucleation is the first step in the formation of either a new thermodynamic phase or structure via self-assembly or self-organization within a substance or mixture. Nucleation is typically defined to be the process that determines how long an observer has to wait before the new phase or self-organized structure appears. For example, if a volume of water is cooled (at atmospheric pressure) significantly below 0 °C, it will tend to freeze into ice, but volumes of water cooled only a few degrees below 0 °C often stay completely free of ice for long periods (supercooling). At these conditions, nucleation of ice is either slow or does not occur at all. However, at lower temperatures nucleation is fast, and ice crystals appear after little or no delay.\nNucleation is a common mechanism which generates first-order phase transitions, and it is the start of the process of forming a new thermodynamic phase. In contrast, new phases at continuous phase transitions start to form immediately.\nNucleation is often very sensitive to impurities in the system. These impurities may be too small to be seen by the naked eye, but still can control the rate of nucleation. Because of this, it is often important to distinguish between heterogeneous nucleation and homogeneous nucleation. Heterogeneous nucleation occurs at nucleation sites on surfaces in the system. Homogeneous nucleation occurs away from a surface.\n\nCharacteristics\nNucleation is usually a stochastic (random) process, so even in two identical systems nucleation will occur at different times. A common mechanism is illustrated in the animation to the right. This shows nucleation of a new phase (shown in red) in an existing phase (white). In the existing phase microscopic fluctuations of the red phase appear and decay continuously, until an unusually large fluctuation of the new red phase is so large it is more favourable for it to grow than to shrink back to nothing. This nucleus of the red phase then grows and converts the system to this phase. The standard theory that describes this behaviour for the nucleation of a new thermodynamic phase is called classical nucleation theory. However, the CNT fails in describing experimental results of vapour to liquid nucleation even for model substances like argon by several orders of magnitude.\nFor nucleation of a new thermodynamic phase, such as the formation of ice in water below 0 °C, if the system is not evolving with time and nucleation occurs in one step, then the probability that nucleation has not occurred should undergo exponential decay. This is seen for example in the nucleation of ice in supercooled small water droplets. The decay rate of the exponential gives the nucleation rate. Classical nucleation theory is a widely used approximate theory for estimating these rates, and how they vary with variables such as temperature. It correctly predicts that the time you have to wait for nucleation decreases extremely rapidly when supersaturated.\n",
    "source": "wikipedia",
    "title": "Nucleation",
    "topic": "Physics"
  },
  {
    "id": "wiki_2137509",
    "text": "In physics, a perfect fluid or ideal fluid  is a fluid that can be completely characterized by its rest frame mass density \n  \n    \n      \n        \n          ρ\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\rho _{m}}\n  \n and isotropic pressure ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠. Real fluids are viscous (\"sticky\") and contain (and conduct) heat. Perfect fluids are idealized models in which these possibilities are ignored. Specifically, perfect fluids have no shear stresses, viscosity, or heat conduction. \nA quark–gluon plasma\nand graphene are examples of nearly perfect fluids that could be studied in a laboratory.\n\nNon-relativistic fluid mechanics\nIn classical mechanics, ideal fluids are described by Euler equations. Ideal fluids produce no drag according to d'Alembert's paradox. If a fluid produced drag, then work would be needed to move an object through the fluid and that work would produce heat or fluid motion. However, a perfect fluid can not dissipate energy and it can't transmit energy infinitely far from the object.\nA flock of birds in the medium of air is an example of a perfect fluid; an electron gas is also modeled as a perfect fluid.\nCosmology and astrophysics\nPerfect fluids are a fluid solution used in general relativity to model idealized distributions of matter, such as the interior of a star or an isotropic universe. In the latter case, the symmetry of the cosmological principle and the equation of state of the perfect fluid lead to Friedmann equation for the expansion of the universe.\nSee also\nEquation of state\nIdeal gas\nFluid solutions in general relativity\nPotential flow\nReferences\n\nFurther reading\nS.W. Hawking; G.F.R. Ellis (1973), The Large Scale Structure of Space-Time, Cambridge University Press ISBN 0-521-20016-4, ISBN 0-521-09906-4 (pbk.)\nJackiw, R; Nair, V P; Pi, S-Y; Polychronakos, A P (2004-10-22). \"Perfect fluid theory and its extensions\". Journal of Physics A: Mathematical and General. 37 (42): R327–R432. arXiv:hep-ph/0407101. doi:10.1088/0305-4470/37/42/R01. ISSN 0305-4470. Topical review.\n",
    "source": "wikipedia",
    "title": "Perfect fluid",
    "topic": "Physics"
  },
  {
    "id": "wiki_79820729",
    "text": "Physics of life is a branch of physics that studies the fundamental principles governing living systems. It applies methods from mechanics, thermodynamics, statistical physics, and information theory to biological phenomena ranging from molecular assemblies to ecosystems. The field seeks to understand how complex behaviors of life arise from interactions among physical components under conditions far from equilibrium. Biological physics has gained wider recognition as a distinct and essential area within physics research.\n\nOverview\nThe physics of life investigates how the familiar laws of physics apply to living matter, and how living systems sometimes require new physical principles for their understanding. Rather than viewing biology as an exception, researchers treat biological phenomena as fertile ground for discovering general laws of non-equilibrium matter and information processing.\nBiological physics has grown substantially and now constitutes one of the largest divisions of the American Physical Society (APS). It bridges traditional disciplines and introduces concepts such as stochasticity, phase transitions, and self-organization into the study of life.\nConceptual Foundations\nA 2022 decadal survey by the National Academies of Sciences, Engineering, and Medicine outlined five central questions guiding research in the physics of life:\nExperimental and Theoretical Approaches\nExperimental tools include optical tweezers, cryo-electron microscopy, and single-molecule tracking. Theoretical approaches combine statistical mechanics, continuum mechanics, machine learning, and non-equilibrium physics.\nConcepts such as phase transitions, self-organization, and stochastic fluctuations, traditionally studied in inanimate systems, have become central for understanding biological systems.\nRecognition and Growth\nBiological physics has evolved from an interdisciplinary curiosity into a central part of modern physics. Researchers emphasize that studying life offers opportunities to discover new organizing principles of matter and information. The National Academies report and commentary from the American Physical Society call for expanded funding, interdisciplinary training, and infrastructure to accelerate progress.\n",
    "source": "wikipedia",
    "title": "Physics of Life",
    "topic": "Physics"
  },
  {
    "id": "wiki_27481335",
    "text": "In physics, the plasmaron was proposed by Lundqvist in 1967 as a quasiparticle arising in a system that has strong plasmon-electron interactions. In the original work, the plasmaron was proposed to describe a secondary peak (or satellite) in the photoemission spectral function of the electron gas. More precisely it was defined as an additional zero of the quasi-particle equation \n  \n    \n      \n        (\n        ω\n        −\n        \n          ϵ\n          \n            H\n          \n        \n        −\n        R\n        e\n        [\n        Σ\n        (\n        ω\n        )\n        ]\n        =\n        0\n        )\n      \n    \n    {\\displaystyle (\\omega -\\epsilon _{H}-Re[\\Sigma (\\omega )]=0)}\n  \n. The same authors pointed out, in a subsequent work, that this extra solution might be an artifact of the used approximations:\n\nWe want to stress again that the discussion we have given of the one-electron spectrum is based on the assumption that vertex corrections are small. As discussed in the next section recent work by Langreth [29] shows that vertex corrections in the core electron problem can have a quite large effect on the form of satellite structures, while their effect on the quasi particle properties seems to be small. Preliminary investigations by one of us (L.H.) show similar strong vertex effects on the conduction band satellite. The details of the plasmaron structure should thus not be taken very seriously. A more mathematical discussion is provided.\nThe plasmaron was also studied in more recent works in the literature. It was shown, also with the support of the numerical simulations, that the plasmaron energy is an artifact of the approximation used to numerically compute the spectral function, e.g. solution of the dyson equation for the many body green function with a frequency dependent GW self-energy. This approach give rise to a wrong plasmaron peak instead of the plasmon satellite which can be measured experimentally.\nDespite this fact, experimental observation of a plasmaron was reported in 2010 for graphene.\nAlso supported by earlier theoretical work. However subsequent works discussed that the theoretical interpretation of the experimental measure was not correct, in agreement with the fact that the plasmaron is only an artifact of the GW self-energy used with the Dyson equation. The artificial nature of the plasmaron peak was also proven via the comparison of experimental and numerical simulations for the photo-emission spectrum of bulk silicon. Other works on plasmaron have been published in the literature.\nObservation of plasmaron peaks have also been reported in optical measurements of elemental bismuth and in other optical measurements.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Plasmaron",
    "topic": "Physics"
  },
  {
    "id": "wiki_75463818",
    "text": "A quasi-isodynamic (QI) stellarator is a type of stellarator (a magnetic confinement fusion reactor) that satisfies the property of omnigeneity, avoids the potentially hazardous toroidal bootstrap current, and has minimal neoclassical transport in the collisionless regime.\nWendelstein 7-X, the largest stellarator in the world, was designed to be roughly quasi-isodynamic (QI).\nIn contrast to quasi-symmetric fields, exactly QI fields on flux surfaces cannot be expressed analytically. However, it has been shown that nearly-exact QI can be extremely well approximated through mathematical optimization, and that the resulting fields enjoy the aforementioned properties.\nIn a QI field, level curves of the magnetic field strength \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n on a flux surface close poloidally (the short way around the torus), and not toroidally (the long way around), causing the stellarator to resemble a series of linked magnetic mirrors.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Quasi-isodynamic stellarator",
    "topic": "Physics"
  },
  {
    "id": "wiki_81547074",
    "text": "SDSS J120136.02+300305.5 is an optically inactive, quiescent galaxy that possibly contains a  milliparsec supermassive black hole binary.\n\nDiscovery\nX-ray fluxes in the galaxy were first detected in June 2010 with a flux 56 times higher than an upper limit from ROSAT, corresponding to LX ~ 3 × 1044 erg s−1. It had the rough optical spectrum of quiescent galaxy. All in all, the flux evolved in fair consistency with the normal t−5/3 model, something that would be normal for returning stellar debris, before fading by a factor of roughly 300 after 300 days. However, the source was incredibly volatile, becoming invisible between 27–48 days after discovery. It may have matched a Bremsstrahlung or double-power-law model and usually softens as time passes.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "SDSS J120136.02+300305.5",
    "topic": "Physics"
  },
  {
    "id": "wiki_76197486",
    "text": "Shockwave cosmology is a non-standard cosmology proposed by Joel Smoller and Blake Temple in 2003. In this model, the “big bang” is an explosion inside a black hole, producing the expanding volume of space and matter that includes the observable universe.\n\nIntegration with general relativity\nSmoller and Temple integrate shock waves into Einstein's general relativity. This produces a universe that \"looks essentially identical to the aftermath of the big bang\" according to cosmologists Barnes and Lewis. They explain that Smoller and Temple's version is distinguished from the big bang only by there being a shockwave at the leading edge of an explosion – one that, for Smoller and Temple's model, must be beyond the observable universe. However, Barnes and Lewis do not support shockwave cosmology because they see it as not testable; they point out that there is no explosion in the standard theory of the Big Bang.\nCurrent and future state of the universe\nFrom Smoller and Temple's calculations, we are still inside an expanding black hole. The configuration of 'flat' spacetime (see Minkowski space) inside a black hole, also occurs during the moments of the formation of a black hole from a collapsing star.\nEventually, according to shockwave cosmology, the mass of our expanding volume of space and matter will fall in density as it expands. At some point, the event horizon of the black hole will cease to be. An outside observer will then see it appear as a white hole. The matter would then continue to expand.\nAlternative to dark energy\nIn related work, Smoller, Temple, and Vogler propose that this shockwave may have resulted in our part of the universe having a lower density than that surrounding it, causing the accelerated expansion normally attributed to dark energy.  \nThey also propose that this related theory could be tested: a universe with dark energy should give a figure for the cubic correction to redshift versus luminosity C = −0.180 at a = a whereas for Smoller, Temple, and Vogler's alternative C should be positive rather than negative. They give a more precise calculation for their wave model alternative as: the cubic correction to redshift versus luminosity at a = a is C = 0.359.\n",
    "source": "wikipedia",
    "title": "Shockwave cosmology",
    "topic": "Physics"
  },
  {
    "id": "wiki_21276538",
    "text": "Surface stress was first defined by Josiah Willard Gibbs (1839–1903) as the amount of the reversible work per unit area needed to elastically stretch a pre-existing surface. Depending upon the convention used, the area is either the original, unstretched one which represents a constant number of atoms, or sometimes is the final area; these are atomistic versus continuum definitions. Some care is needed to ensure that the definition used is also consistent with the elastic strain energy, and misinterpretations and disagreements have occurred in the literature.\nA similar term called \"surface free energy\", the excess free energy per unit area needed to create a new surface, is sometimes confused with \"surface stress\". Although surface stress and surface free energy of liquid–gas or liquid–liquid interface are the same, they are very different in solid–gas or solid–solid interface. Both terms represent an energy per unit area, equivalent to a  force per unit length, so are sometimes referred to as \"surface tension\", which contributes further to the confusion in the literature.\n\nThermodynamics of surface stress\nThe continuum definition of surface free energy is the amount of reversible work \n  \n    \n      \n        d\n        w\n      \n    \n    {\\displaystyle dw}\n  \n performed to create new area \n  \n    \n      \n        d\n        A\n      \n    \n    {\\displaystyle dA}\n  \n of surface, expressed as:\n\n  \n    \n      \n        d\n        w\n        =\n        γ\n        d\n        A\n      \n    \n    {\\displaystyle dw=\\gamma dA}\n  \n\nIn this definition the number of atoms at the surface is proportional to the area. Gibbs was the first to define another surface quantity, different from the surface free energy \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n, that is associated with the reversible work per unit area needed to elastically stretch a pre-existing surface. In a continuum approach one can define a surface stress tensor \n  \n    \n      \n        \n          f\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle f_{ij}}\n  \n that relates the work associated with the variation in \n  \n    \n      \n        γ\n        A\n      \n    \n    {\\displaystyle \\gamma A}\n  \n, the total excess free energy of the surface due to a strain tensor \n  \n    \n      \n        \n          e\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle e_{ij}}\n  \n\n  \n    \n      \n        A\n        \n          f\n          \n            i\n            j\n          \n        \n        =\n        d\n        (\n        γ\n        A\n        )\n        \n          /\n        \n        d\n        \n          e\n          \n            i\n            j\n          \n        \n        =\n        A\n        d\n        γ\n        \n          /\n        \n        d\n        \n          e\n          \n            i\n            j\n          \n        \n        +\n        γ\n        d\n        A\n        \n          /\n        \n        d\n        \n          e\n          \n  ",
    "source": "wikipedia",
    "title": "Surface stress",
    "topic": "Physics"
  },
  {
    "id": "wiki_79850130",
    "text": "Synchronous lateral excitation is a dynamic phenomenon where pedestrians walking on a footbridge subconsciously synchronize their lateral footsteps with the bridge’s natural swaying motion, amplifying lateral vibrations. First widely recognized during the 2000 opening of the London Millennium Bridge, synchronous lateral excitation has since become a critical consideration in the design of lightweight pedestrian structures.\n\nMechanism\nSynchronous lateral excitation arises from two interrelated synchronization processes. The first is the pedestrian-structure synchronization, where slight lateral bridge movements (e.g., from wind or random pedestrian steps) prompt walkers to adjust their gait to match the bridge’s oscillation frequency, increasing lateral forces.\nThe second is pedestrian-pedestrian synchronization, where individuals unconsciously align their stepping patterns, further reinforcing the resonant force.\nKey cases\nThe London Millennium Bridge experienced lateral vibrations up to 70 mm due to synchronous lateral excitation, requiring a £5M retrofit with dampers.\nThe Auckland Harbour Bridge experienced a lateral frequency of 0.67 Hz during a 1975 demonstration.\nThe Birmingham NEC Link bridge experienced a lateral frequency of 0.7 Hz.\nThe Toda Park Bridge in Japan is an early documented case (1990s) studied by Fujino et al., informing later synchronous lateral excitation models.\nMitigation strategies\nSome ways to avoid synchronous lateral excitation are the implementation of tuned mass dampers, which were used in the Millennium Bridge to increase damping from 0.5% to 20% critical. Other strategies involve designing bridges with lateral frequencies outside the 0.5–1.1 Hz range as well as managing crows by limiting pedestrian density during events.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Synchronous lateral excitation",
    "topic": "Physics"
  },
  {
    "id": "wiki_467047",
    "text": "The term \"thermal energy\" is often used ambiguously in physics and engineering. It can denote several different physical concepts, including:\n\nInternal energy: The energy contained within a body of matter or radiation, excluding the potential energy of the whole system.\nHeat: Energy in transfer between a system and its surroundings by mechanisms other than thermodynamic work and transfer of matter.\nThe characteristic energy kBT, where T denotes temperature and kB denotes the Boltzmann constant; it is twice that associated with each degree of freedom.\nMark Zemansky (1970) has argued that the term \"thermal energy\" is best avoided due to its ambiguity. He suggests using more precise terms such as \"internal energy\" and \"heat\" to avoid confusion. The term is, however, used in some textbooks.\n\nRelation between heat and internal energy\nIn thermodynamics, heat is energy in transfer to or from a thermodynamic system by mechanisms other than thermodynamic work or transfer of matter, such as conduction, radiation, and friction. Heat refers to a quantity in transfer between systems, not to a property of any one system, or \"contained\" within it; on the other hand, internal energy and enthalpy are properties of a single system. Heat and work depend on the way in which an energy transfer occurs. In contrast, internal energy is a property of the state of a system and can thus be understood without knowing how the energy got there.\nMacroscopic thermal energy\nIn addition to the microscopic kinetic energies of its molecules, the internal energy of a body includes chemical energy belonging to distinct molecules, and the global joint potential energy involved in the interactions between molecules and suchlike. Thermal energy may be viewed as contributing to internal energy or to enthalpy.\nMicroscopic thermal energy\nIn a statistical mechanical account of an ideal gas, in which the molecules move independently between instantaneous collisions, the internal energy is just the sum total of the gas's independent particles' kinetic energies, and it is this kinetic motion that is the source and the effect of the transfer of heat across a system's boundary. For a gas that does not have particle interactions except for instantaneous collisions, the term \"thermal energy\" is effectively synonymous with \"internal energy\".\nIn many statistical physics texts, \"thermal energy\" refers to \n  \n    \n      \n        k\n        T\n      \n    \n    {\\displaystyle kT}\n  \n, the product of the Boltzmann constant and the absolute temperature, also written as ⁠\n  \n    \n      \n        \n          k\n          \n            B\n          \n        \n        T\n      \n    \n    {\\displaystyle k_{\\text{B}}T}\n  \n⁠.\n",
    "source": "wikipedia",
    "title": "Thermal energy",
    "topic": "Physics"
  },
  {
    "id": "wiki_74170779",
    "text": "The toroidal solenoid was an early 1946 design for a fusion power device designed by George Paget Thomson and Moses Blackman of Imperial College London. It proposed to confine a deuterium fuel plasma to a toroidal (donut-shaped) chamber using magnets, and then heating it to fusion temperatures using radio frequency energy in the fashion of a microwave oven. It is notable for being the first such design to be patented, filing a secret patent on 8 May 1946 and receiving it in 1948.\nA critique by Rudolf Peierls noted several problems with the concept. Over the next few years, Thomson continued to suggest starting an experimental effort to study these issues, but was repeatedly denied as the underlying theory of plasma diffusion was not well developed. When similar concepts were suggested by Peter Thonemann that included a more practical heating arrangement, John Cockcroft began to take the concept more seriously, establishing small study groups at Harwell. Thomson adopted Thonemann's concept, abandoning the radio frequency system.\nWhen the patent had still not been granted in early 1948, the Ministry of Supply inquired about Thomson's intentions. Thomson explained the problems he had getting a program started and that he did not want to hand off the rights until that was clarified. As the directors of the UK nuclear program, the Ministry quickly forced Harwell's hand to provide funding for Thomson's program. Thomson then released his rights the patent, which was granted late that year. Cockcroft also funded Thonemann's work, and with that, the UK fusion program began in earnest. After the news furor over the Huemul Project in February 1951, significant funding was released and led to rapid growth of the program in the early 1950s, and ultimately to the ZETA reactor of 1958.\n\nConceptual development\nThe basic understanding of nuclear fusion was developed during the 1920s as physicists explored the new science of quantum mechanics. George Gamow's 1928 work on quantum tunnelling demonstrated that nuclear reactions could take place at lower energies than classical theory predicted. Using this theory, in 1929 Fritz Houtermans and Robert Atkinson demonstrated that expected reaction rates in the core of the Sun supported Arthur Eddington's 1920 suggestion that the Sun is powered by fusion.\nIn 1934, Mark Oliphant, Paul Harteck and Ernest Rutherford were the first to achieve fusion on Earth, using a particle accelerator to shoot deuterium nuclei into a metal foil containing deuterium, lithium or other elements. This allowed them to measure the nuclear cross section of various fusion reactions, and determined that the deuterium-deuterium reaction occurred at a lower energy than other reactions, peaking at about 100,000 electronvolts (100 keV).\nThis energy corresponds to the average energy of particles in a gas heated to a billion Kelvin. Materials heated beyond a few tens of thousand Kelvin dissociate into their electrons and nuclei, producing a gas-like state",
    "source": "wikipedia",
    "title": "Toroidal solenoid",
    "topic": "Physics"
  },
  {
    "id": "wiki_79026168",
    "text": "The Wohlfarth Memorial Lecture, and the Wohlfarth Lectureship, is a lecture and prize given at the UK-based Institute of Physics' annual Magnetism Conference. It is named after Professor Erich Peter Wohlfarth, in honour of his \"outstanding contribution [...] to the field of magnetism\". It has been awarded since 1989.\n\nRecipients\n\nSee also\nInstitute of Physics\nInstitute of Physics Awards\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Wohlfarth Lectureship",
    "topic": "Physics"
  },
  {
    "id": "wiki_5180",
    "text": "Chemistry is the scientific study of the properties and behavior of matter. It is a physical science within the natural sciences that studies the chemical elements that make up matter and compounds made of atoms, molecules and ions: their composition, structure, properties, behavior and the changes they undergo during reactions with other substances. Chemistry also addresses the nature of chemical bonds in chemical compounds.\nIn the scope of its subject, chemistry occupies an intermediate position between physics and biology. It is sometimes called the central science because it provides a foundation for understanding both basic and applied scientific disciplines at a fundamental level. For example, chemistry explains aspects of plant growth (botany), the formation of igneous rocks (geology), how atmospheric ozone is formed and how environmental pollutants are degraded (ecology), the properties of the soil on the Moon (cosmochemistry), how medications work (pharmacology), and how to collect DNA evidence at a crime scene (forensics).\nChemistry has existed under various names since ancient times. It has evolved, and now chemistry encompasses various areas of specialisation, or subdisciplines, that continue to increase in number and interrelate to create further interdisciplinary fields of study. The applications of various fields of chemistry are used frequently for economic purposes in the chemical industry.\n\nEtymology\nThe word chemistry comes from a modification during the Renaissance of the word alchemy, which referred to an earlier set of practices that encompassed elements of chemistry, metallurgy, philosophy, astrology, astronomy, mysticism, and medicine. Alchemy is often associated with the quest to turn lead or other base metals into gold, though alchemists were also interested in many of the questions of modern chemistry.\nThe modern word alchemy in turn is derived from the Arabic word al-kīmīā (الكیمیاء). This may have Egyptian origins since al-kīmīā is derived from the Ancient Greek χημία, which is in turn derived from the word Kemet, which is the ancient name of Egypt in the Egyptian language. Alternately, al-kīmīā may derive from χημεία 'cast together'.\n",
    "source": "wikipedia",
    "title": "Chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_43180070",
    "text": "Actinide chemistry (or actinoid chemistry) is one of the main branches of nuclear chemistry that investigates the processes and molecular systems of the actinides. The actinides derive their name from the group 3 element actinium. The informal chemical symbol An is used in general discussions of actinide chemistry to refer to any actinide. All but one of the actinides are f-block elements, corresponding to the filling of the 5f electron shell; lawrencium, a d-block element, is also generally considered an actinide. In comparison with the lanthanides, also mostly f-block elements, the actinides show much more variable valence. The actinide series encompasses the 15 metallic chemical elements with atomic numbers from 89 to 103, actinium through lawrencium.\n\nMain branches\n\nNuclear reactions\nSome early evidence for nuclear fission was the formation of a short-lived radioisotope of barium which was isolated from neutron irradiated uranium (139Ba, with a half-life of 83 minutes and 140Ba, with a half-life of 12.8 days, are major fission products of uranium). At the time, it was thought that this was a new radium isotope, as it was then standard radiochemical practice to use a barium sulfate carrier precipitate to assist in the isolation of radium.\nPUREX\nThe PUREX process is a liquid–liquid extraction ion-exchange method used to reprocess spent nuclear fuel, in order to extract primarily uranium and plutonium, independent of each other, from the other constituents. The current method of choice is to use the PUREX liquid–liquid extraction process which uses a tributyl phosphate/hydrocarbon mixture to extract both uranium and plutonium from nitric acid. This extraction is of the nitrate salts and is classed as being of a solvation mechanism. For example, the extraction of plutonium by an extraction agent (S) in a nitrate medium occurs by the following reaction.\n\nPu4+(aq) + 4 NO−3(aq) + 2 S(organic) → [Pu(NO3)4S2](organic)\nA complex bond is formed between the metal cation, the nitrates and the tributyl phosphate, and a model compound of a dioxouranium(VI) complex with two nitrates and two triethyl phosphates has been characterised by X-ray crystallography. After the dissolution step it is normal to remove the fine insoluble solids, because otherwise they will disturb the solvent extraction process by altering the liquid-liquid interface. It is known that the presence of a fine solid can stabilize an emulsion. Emulsions are often referred to as third phases in the solvent extraction community.\nAn organic solvent composed of 30% tributyl phosphate (TBP) in a hydrocarbon solvent, such as kerosene, is used to extract the uranium as UO2(NO3)2·2TBP complexes, and plutonium as similar complexes, from other fission products, which remain in the aqueous phase. The transuranium elements americium and curium also remain in the aqueous phase. The nature of the organic soluble uranium complex has been the subject of some research. A series of complexes of uranium with ",
    "source": "wikipedia",
    "title": "Actinide chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_1839",
    "text": "Allotropy or allotropism (from Ancient Greek  ἄλλος (allos) 'other' and  τρόπος (tropos) 'manner, form') is the property of some chemical elements to exist in two or more different forms, in the same physical state, known as allotropes of the elements. Allotropes are different structural modifications of an element: the atoms of the element are bonded together in different manners.\nFor example, the allotropes of carbon include diamond (the carbon atoms are bonded together to form a cubic lattice of tetrahedra), graphite (the carbon atoms are bonded together in sheets of a hexagonal lattice), graphene (single sheets of graphite), and fullerenes (the carbon atoms are bonded together in spherical, tubular, or ellipsoidal formations).\nThe term allotropy is used for elements only, not for compounds. The more general term, used for any compound, is polymorphism, although its use is usually restricted to solid materials such as crystals. Allotropy refers only to different forms of an element within the same physical phase (the state of matter, i.e. plasmas, gases, liquids, or solids). The differences between these states of matter would not alone constitute examples of allotropy. Allotropes of chemical elements are frequently referred to as polymorphs or as phases of the element.\nFor some elements, allotropes have different molecular formulae or different crystalline structures, as well as a difference in physical phase; for example, two allotropes of oxygen (dioxygen, O2, and ozone, O3) can both exist in the solid, liquid and gaseous states. Other elements do not maintain distinct allotropes in different physical phases; for example, phosphorus has numerous solid allotropes, which all revert to the same P4 form when melted to the liquid state.\n\nHistory\nThe concept of allotropy was originally proposed in 1840 by the Swedish scientist Baron Jöns Jakob Berzelius (1779–1848). The term is derived from Greek  άλλοτροπἱα (allotropia) 'variability, changeableness'. After the acceptance of Avogadro's hypothesis in 1860, it was understood that elements could exist as polyatomic molecules, and two allotropes of oxygen were recognized as O2 and O3. In the early 20th century, it was recognized that other cases such as carbon were due to differences in crystal structure.\nBy 1912, Ostwald noted that the allotropy of elements is just a special case of the phenomenon of polymorphism known for compounds, and proposed that the terms allotrope and allotropy be abandoned and replaced by polymorph and polymorphism. Although many other chemists have repeated this advice, IUPAC and most chemistry texts still favour the usage of allotrope and allotropy for elements only.\n",
    "source": "wikipedia",
    "title": "Allotropy",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_1187",
    "text": "An alloy is a mixture of chemical elements of which in most cases at least one is a metallic element, although it is also sometimes used for mixtures of elements; herein only metallic alloys are described. Metallic alloys often have properties that differ from those of the pure elements from which they are made. The vast majority of metals used for commercial purposes are alloyed to improve their properties or behavior, such as increased strength, hardness or corrosion resistance. Metals may also be alloyed to reduce their overall cost, for instance alloys of gold and copper.\nIn an alloy, the atoms are joined by metallic bonding rather than by covalent bonds typically found in chemical compounds. The alloy constituents are usually measured by mass percentage for practical applications, and in atomic fraction for basic science studies. Alloys are usually classified as substitutional or interstitial alloys, depending on the atomic arrangement that forms the alloy. They can be further classified as homogeneous (consisting of a single phase), or heterogeneous (consisting of two or more phases) or intermetallic. An alloy may be a solid solution of metal elements (a single phase, where all metallic grains (crystals) are of the same composition) or a mixture of metallic phases (two or more solutions, forming a microstructure of different crystals within the metal).\nExamples of alloys include red gold (gold and copper), white gold (gold and silver), sterling silver (silver and copper), steel or silicon steel (iron with non-metallic carbon or silicon respectively), solder, brass, pewter, duralumin, bronze, and amalgams. Alloys are used in a wide variety of applications, from the steel alloys, used in everything from buildings to automobiles to surgical tools, to exotic titanium alloys used in the aerospace industry, to beryllium-copper alloys for non-sparking tools.\n\nCharacteristics\nAn alloy is a mixture of chemical elements which forms an impure substance (admixture) that retains the characteristics of a metal. Alloys are made by mixing two or more elements, at least one of which is a metal. This is usually called the primary metal or the base metal, and the name of this metal may also be the name of the alloy. The other constituents may or may not be metals but, when mixed with the molten base, they will be soluble and dissolve into the mixture.\nThe mechanical properties of alloys will often be quite different from those of its individual constituents. A metal that is normally very soft (malleable), such as aluminium, can be altered by alloying it with another soft metal, such as copper. Although both metals are very soft and ductile, the resulting aluminium–copper alloy will have much greater strength. Adding a small amount of non-metallic carbon to iron trades its great ductility for the greater strength of an alloy called steel. Due to its very-high strength, but still substantial toughness, and its ability to be greatly altered by heat treatment, st",
    "source": "wikipedia",
    "title": "Alloy",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_18504176",
    "text": "Amateur chemistry or home chemistry is the pursuit of chemistry as a private hobby. Amateur chemistry is usually done with whatever chemicals are available at disposal at the privacy of one's home. It should not be confused with clandestine chemistry, which involves the illicit production of controlled drugs.[a] Notable amateur chemists include Oliver Sacks and Sir Edward Elgar.\n\nHistory\n\nNotable amateur chemists\nInternet pioneer Vint Cerf, Intel co-founder Gordon Moore, and Hewlett Packard co-founder David Packard all used to practice amateur chemistry.\nBritish neurologist Oliver Sacks was a keen amateur chemist in his youth, as described in his memoir Uncle Tungsten: Memories of a Chemical Boyhood.\nNobel Prize winning chemist Linus Pauling practised amateur chemistry in his youth.\nWolfram Research co-founder Theodore Gray is a keen amateur chemist and element collector.  His exploits (most notably the construction of a wooden table in the shape of the periodic table, having compartments holding real samples of each element) earned him the 2002 Ig Nobel prize for chemistry, which he accepted as a great honor.  He writes a column for Popular Science magazine, featuring his home experiments.\nAmateur rocketeer (and later NASA engineer) Homer Hickham, together with his fellow Rocket Boys, experimented with a range of home-made rocket propellants.  These included \"Rocket Candy\" made from potassium nitrate and sugar, and \"Zincoshine\" made from zinc and sulfur held together with moonshine alcohol.\nComposer Sir Edward Elgar practised amateur chemistry from a laboratory erected in his back garden. The original manuscript of the prelude to The Kingdom is stained with chemicals.\nRobert Boyle is largely regarded today as the first modern chemist, and therefore one of the founders of modern chemistry, and one of the pioneers of modern experimental scientific method.\nMaurice Ward, a hairdresser and amateur chemist who invented the thermal insulating material called Starlite.\nRobert Cornelius, inventor, businessman and lamp manufacturer credited for creating the first photographic self-portrait in 1839.\n",
    "source": "wikipedia",
    "title": "Amateur chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_39488712",
    "text": "The Arens–van Dorp synthesis is a name reaction in organic chemistry. It describes the addition of lithiated ethoxyacetylenes to ketones to give propargyl alcohols, which can undergo further reaction to form α,β-unsaturated aldehydes, or esters. There is also a variation of this reaction called the Isler modification, where the acetylide anion is generated in situ from β-chlorovinyl ether using lithium amide.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Arens–van Dorp synthesis",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_71268",
    "text": "Astrochemistry is the interdisciplinary scientific study of the abundance and reactions of molecules in space and their interaction with radiation. The discipline overlaps with astronomy and chemistry. The term may refer to studies within both the Solar System and the interstellar medium. The investigation of elemental abundances and isotope ratios in Solar System materials, such as meteorites, is known as cosmochemistry, while the study of interstellar atoms and molecules and their interaction with radiation is sometimes called molecular astrophysics. The formation, composition, evolution and fate of molecular clouds is of particular interest, as these clouds are the birthplaces of planetary systems.\n\nHistory\nAs an offshoot of astronomy and chemistry, the history of astrochemistry follows the development of both fields. Advances in observational and experimental spectroscopy enabled the detection of an ever‑growing range of molecules within planetary systems and the surrounding interstellar medium. The expanding inventory of detected species, made possible by improvements in spectroscopy and related technologies, has in turn broadened the chemical space accessible to astrochemical research.\nSpectroscopy\nOne of the most important experimental tools in astrochemistry is spectroscopy, which uses telescopes to measure the absorption and emission of light from atoms and molecules in different astrophysical environments. By comparing astronomical observations with laboratory spectra, astrochemists can infer the elemental abundances, chemical composition and temperatures of stars and interstellar clouds. This is possible because ions, atoms and molecules have characteristic spectra: that is, they absorb and emit light at specific wavelengths, many of which are not visible to the human eye. Different regions of the electromagnetic spectrum (radio, infrared, visible, ultraviolet and others) probe different types of transitions and are therefore sensitive to different kinds of species.\n",
    "source": "wikipedia",
    "title": "Astrochemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_706999",
    "text": "Atmospheric chemistry is a branch of atmospheric science that studies the chemistry of the Earth's atmosphere and that of other planets. This multidisciplinary approach of research draws on environmental chemistry, physics, meteorology, computer modeling, oceanography, geology and volcanology, climatology and other disciplines to understand both natural and human-induced changes in atmospheric composition. Key areas of research include the behavior of trace gasses, the formation of pollutants, and the role of aerosols and greenhouse gasses. Through a combination of observations, laboratory experiments, and computer modeling, atmospheric chemists investigate the causes and consequences of atmospheric changes.\n\nAtmospheric composition\nThe composition and chemistry of the Earth's atmosphere is important for several reasons, but primarily because of the interactions between the atmosphere and living organisms. Natural processes such as volcano emissions, lightning and bombardment by solar particles from corona changes the composition of the Earth's atmosphere. It has also been changed by human activity and some of these changes are harmful to human health, crops and ecosystems.\nHistory\nThe first scientific studies of atmospheric composition began in the 18th century when chemists such as Joseph Priestley, Antoine Lavoisier and Henry Cavendish made the first measurements of the composition of the atmosphere.\nIn the late 19th and early 20th centuries, researchers shifted their interest towards trace constituents with very low concentrations. An important finding from this era was the discovery of ozone by Christian Friedrich Schönbein in 1840.\nIn the 20th century atmospheric science moved from studying the composition of air to consider how the concentrations of trace gasses in the atmosphere have changed over time and the chemical processes which create and destroy compounds in the air. Two important outcomes were the explanation by Sydney Chapman and Gordon Dobson of how the ozone layer is created and maintained, and Arie Jan Haagen-Smit’s explanation of photochemical smog. Further studies on ozone issues led to the 1995 Nobel Prize in Chemistry award shared between Paul Crutzen, Mario Molina and Frank Sherwood Rowland. \n\nIn the 21st century the focus is now shifting again. Instead of concentrating on atmospheric chemistry in isolation, it is now seen as one part of the Earth system with the rest of the atmosphere, biosphere and geosphere. A driving force for this link is the relationship between chemistry and climate. The changing climate and the recovery of the ozone hole and the interaction of the composition of the atmosphere with the oceans and terrestrial ecosystems are examples of the interdependent relationships between Earth's systems. A new field of extraterrestrial atmospheric chemistry has also recently emerged. Astrochemists analyze the atmospheric compositions of the Solar System and exoplanets to determine the formation of astronomical",
    "source": "wikipedia",
    "title": "Atmospheric chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_902",
    "text": "Atoms are the basic particles of the chemical elements and the fundamental building blocks of matter. An atom consists of a nucleus of protons and generally neutrons, surrounded by an electromagnetically bound swarm of electrons.  The chemical elements are distinguished from each other by the number of protons that are in their atoms. For example, any atom that contains 11 protons is sodium, and any atom that contains 29 protons is copper. Atoms with the same number of protons but a different number of neutrons are called isotopes of the same element.\nAtoms are extremely small, typically around 100 picometers across. A human hair is about a million carbon atoms wide. Atoms are smaller than the shortest wavelength of visible light, which means humans cannot see atoms with conventional microscopes. They are so small that accurately predicting their behavior using classical physics is not possible due to quantum effects.\nMore than 99.94% of an atom's mass is in the nucleus. Protons have a positive electric charge and neutrons have no charge, so the nucleus is positively charged. The electrons are negatively charged, and this opposing charge is what binds them to the nucleus. If the numbers of protons and electrons are equal, as they normally are, then the atom is electrically neutral as a whole. A charged atom is called an ion. If an atom has more electrons than protons, then it has an overall negative charge and is called a negative ion (or anion). Conversely, if it has more protons than electrons, it has a positive charge and is called a positive ion (or cation).\nThe electrons of an atom are attracted to the protons in an atomic nucleus by the electromagnetic force. The protons and neutrons in the nucleus are attracted to each other by the nuclear force. This force is usually stronger than the electromagnetic force that repels the positively charged protons from one another. Under certain circumstances, the repelling electromagnetic force becomes stronger than the nuclear force. In this case, the nucleus splits and leaves behind different elements. This is a form of nuclear decay.\nAtoms can attach to one or more other atoms by chemical bonds to form chemical compounds such as molecules or crystals. The ability of atoms to attach and detach from each other is responsible for most of the physical changes observed in nature. Chemistry is the science that studies these changes.\n\nHistory of atomic theory\n\n",
    "source": "wikipedia",
    "title": "Atom",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_62904359",
    "text": "Biliproteins are pigment protein compounds that are located in photosynthesising organisms such as algae, and sometimes also in certain insects. They refer to any protein that contains a bilin chromophore. In plants and algae, the main function of biliproteins is to make the process of light accumulation required for photosynthesis more efficient; while in insects they play a role in growth and development. Some of their properties: including light-receptivity, light-harvesting and fluorescence have made them suitable for applications in bioimaging and as indicators; while other properties such as anti-oxidation, anti-aging and anti-inflammation in phycobiliproteins have given them potential for use in medicine, cosmetics and food technology. While research on biliproteins dates back as far as 1950, it was hindered due to issues regarding biliprotein structure, lack of methods available for isolating individual biliprotein components, as well as limited information on lyase reactions (which are needed to join proteins with their chromophores). Research on biliproteins has also been primarily focused on phycobiliproteins; but advances in technology and methodology, along with the discovery of different types of lyases, has renewed interest in biliprotein research, allowing new opportunities for investigating biliprotein processes such as assembly/disassembly and protein folding.\n\nFunctions\n\nStructure\nThe structure of biliproteins is typically characterised by bilin chromophores arranged in linear tetrapyrrolic formation, and the bilins are covalently bound to apoproteins via thioether bonds. Each type of biliprotein has a unique bilin that belongs to it (e.g. phycoerythrobilin is the chromophore of phycoerythrin and phycocyanobilin is the chromophore of phycocyanin).  The bilin chromophores are formed by the oxidative cleavage of a haem ring and catalysed by haem oxygenases at one of four methine bridges, allowing four possible bilin isomers to occur.  In all organisms known to have biliproteins, cleavage usually occurs at the α-bridge, generating biliverdin IXα.\nPhycobiliproteins are grouped together in separate clusters, approximately 40nm in diameter, known as phycobilisomes.  The structural changes involved in deriving bilins from their biliverdin IXα isomer determine the spectral range of light absorption.\nThe structure of biliproteins in insects differ slightly than those in plants and algae; they have a crystal structure and their chromophores are not covalently bound to the apoproteins. Unlike phycobiliproteins whose chromophores are held in an extended arrangement by specific interactions between chromophores and proteins, the chromophore in insect biliproteins has a cyclic helical crystal structure in the protein-bound state, as found in studies of the biliprotein extracted from the large white butterfly.\n",
    "source": "wikipedia",
    "title": "Biliprotein",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_35741423",
    "text": "In aquatic toxicology, bioconcentration is the accumulation of a water-borne chemical substance in an organism exposed to the water.\nThere are several ways in which to measure and assess bioaccumulation and bioconcentration.  These include: octanol-water partition coefficients (KOW), bioconcentration factors (BCF), bioaccumulation factors (BAF) and biota-sediment accumulation factor (BSAF).  Each of these can be calculated using either empirical data or measurements, as well as from mathematical models. One of these mathematical models is a fugacity-based BCF model developed by Don Mackay.\nBioconcentration factor can also be expressed as the ratio of the concentration of a chemical in an organism to the concentration of the chemical in the surrounding environment. The BCF is a measure of the extent of chemical sharing between an organism and the surrounding environment.\nIn surface water, the BCF is the ratio of a chemical's concentration in an organism to the chemical's aqueous concentration.  BCF is often expressed in units of liter per kilogram (ratio of mg of chemical per kg of organism to mg of chemical per liter of water). BCF can simply be an observed ratio, or it can be the prediction of a partitioning model. A partitioning model is based on assumptions that chemicals partition between water and aquatic organisms as well as the idea that chemical equilibrium exists between the organisms and the aquatic environment in which it is found\n\nCalculation\nBioconcentration can be described by a bioconcentration factor (BCF), which is the ratio of the chemical concentration in an organism or biota to the concentration in water:\n\n  \n    \n      \n        B\n        C\n        F\n        =\n        \n          \n            \n              C\n              o\n              n\n              c\n              e\n              n\n              t\n              r\n              a\n              t\n              i\n              o\n              \n                n\n                \n                  B\n                  i\n                  o\n                  t\n                  a\n                \n              \n            \n            \n              C\n              o\n              n\n              c\n              e\n              n\n              t\n              r\n              a\n              t\n              i\n              o\n              \n                n\n                \n                  W\n                  a\n                  t\n                  e\n                  r\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle BCF={\\frac {Concentration_{Biota}}{Concentration_{Water}}}}\n  \n\nBioconcentration factors can also be related to the octanol-water partition coefficient, Kow. The octanol-water partition coefficient (Kow) is correlated with the potential for a chemical to bioaccumulate in organisms; the BCF can be predicted from log Kow, via computer programs based on structure activity relationship (SAR) or through the linear equatio",
    "source": "wikipedia",
    "title": "Bioconcentration",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_24568072",
    "text": "Biophysical chemistry is a physical science that uses the concepts of physics and physical chemistry for the study of biological systems. The most common feature of the research in this subject is to seek an explanation of the various phenomena in biological systems in terms of either the molecules that make up the system or the supra-molecular structure of these systems. Apart from the biological applications, recent research showed progress in the medical field as well.\n\nHistory\n\nTechniques\nBiophysical chemists employ various techniques used in physical chemistry to probe the structure of biological systems. These techniques include spectroscopic methods such as nuclear magnetic resonance (NMR) and other techniques like X-ray diffraction and cryo-electron microscopy. An example of research in biophysical chemistry includes the work for which the 2009 Nobel Prize in Chemistry was awarded. The prize was based on X-ray crystallographic studies of the ribosome that helped to unravel the physical basis of its biological function as a molecular machine that translates mRNA into polypeptides. Other areas in which biophysical chemists engage themselves are protein structure and the functional structure of cell membranes. For example, enzyme action can be explained in terms of the shape of a pocket in the protein molecule that matches the shape of the substrate molecule or its modification due to binding of a metal ion. The structures of many large protein assemblies, such as ATP synthase, also exhibit machine-like dynamics as they act on their substrates. Similarly, the structure and function of the biomembranes may be understood through the study of model supramolecular structures as liposomes or phospholipid vesicles of different compositions and sizes.\nApplications\nThere are several biological and medical applications that apply the knowledge of biophysical chemistry to benefit humankind.\nInstitutes\nThe oldest reputed institute for biophysical chemistry is the Max Planck Institute for Biophysical Chemistry in Göttingen.\n",
    "source": "wikipedia",
    "title": "Biophysical chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_13464959",
    "text": "Bittern (pl. bitterns), or nigari, is the salt solution formed when halite (table salt) precipitates from seawater or brines. Bitterns contain magnesium, calcium, and potassium ions as well as chloride, sulfate, iodide, and other ions.\nBittern is commonly formed in salt ponds where the evaporation of water prompts the precipitation of halite. These salt ponds can be part of a salt-producing industrial facility, or they can be used as a waste storage location for brines produced in desalination processes.\nBittern is a source of many useful salts. It is used as a natural source of Mg2+, and it can be used as a coagulant both in the production of tofu and in the treatment of industrial wastewater.\n\nHistory\nBittern has been extracted for a long time, at least several centuries. The Dutch chemist Petrus Jacobus Kipp (1808–1864) experimented with saturated solutions of bittern. The term for the solution is a modification of \"bitter\".\nUses\n\nEnvironmental impact\nIn some jurisdictions, most bitterns are used for other production instead of being directly discarded.  In other jurisdictions each tonne of salt produced can create 3+ tonnes of waste bitterns.\nAlthough bittern generally contains the same compounds as seawater, it is much more concentrated than seawater. If bittern is released directly into seawater, the ensuing salinity increase may harm marine life around the point of release. Even small increases in salinity can disrupt marine species' osmotic balances, which may result in the death of the organism in some cases.\nIn December 1997, 94 corpses of green sea turtles, Chelonia mydas, were found at the Ojo de Liebre Lagoon (OLL) in Mexico, adjacent to the industrial operation of Exportadora de Sal S.A. (ESSA), the largest saltworks in the world. The fluoride ion F− content in bitterns was 60.5-fold more than that in seawater. The bitterns osmolality was 11,000 mosm/kg of water, whereas the turtle's plasma osmolality was about 400 mosm/kg of water. Researchers concluded that the dumping of bitterns into the ocean should be avoided.\n\nThe lack of adequate disposal methods for bitterns and concerns of local commercial and recreational fishing associations about bitterns’ deleterious impacts upon local fish and prawn hatchery areas led the Western Australian EPA in 2008 to recommend against the proposed 4.2 million tonne per annum Straits Salt project in The Pilbara region of WA.  The EPA concluded that: ...the proposed solar salt farm is located in an area that presents unacceptably high risks of environmental harm to wetland values and unacceptable levels of uncertainty in relation to long term management of bitterns. [...] A high level of uncertainty in relation to the proponent’s ability to manage the ongoing production of over 1 million cubic metres per annum of bitterns C, which is toxic to marine biota and therefore likely to degrade wetland and biodiversity values should bitterns discharge occur either accidentally or be required to maintain sa",
    "source": "wikipedia",
    "title": "Bittern (salt)",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_52050131",
    "text": "Building block is a term in chemistry which is used to describe a virtual molecular fragment or a real chemical compound the molecules of which possess reactive functional groups. Building blocks are used for bottom-up modular assembly of molecular architectures: nano-particles, metal-organic frameworks, organic molecular constructs, supra-molecular complexes. Using building blocks ensures strict control of what a final compound or a (supra)molecular construct will be.\n\nBuilding blocks for medicinal chemistry\nIn medicinal chemistry, the term defines either imaginable, virtual molecular fragments or chemical reagents from which drugs or drug candidates might be constructed or synthetically prepared.\nExamples\nTypical examples of building block collections for medicinal chemistry are libraries of fluorine-containing building blocks. Introduction of the fluorine into a molecule has been shown to be beneficial for its pharmacokinetic and pharmacodynamic properties, therefore, the fluorine-substituted building blocks in drug design increase the probability of finding drug leads. Other examples include natural and unnatural amino acid libraries, collections of conformationally constrained bifunctionalized compounds and diversity-oriented  building block collections.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Building block (chemistry)",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_58076521",
    "text": "C1 chemistry is the chemistry of one-carbon molecules.  Although many compounds and ions contain only one carbon, stable and abundant C-1 feedstocks are the focus of research.  Four compounds are of major industrial importance: methane, carbon monoxide, carbon dioxide, and methanol.  Technologies that interconvert these species are often used massively to match supply to demand.\n\nIndustrial processes\nCarbon monoxide and methanol are important chemical feedstocks.  CO is utilized by myriad carbonylation reactions.  Together with hydrogen, it is the feed for the Fischer–Tropsch process, which affords liquid fuels.  Methanol is the precursor to acetic acid, dimethyl ether, formaldehyde, and many methyl compounds (esters, amines, halides). A larger-scale application is methanol to olefins, which produces ethylene and propylene.\nIn contrast to carbon monoxide and methanol, methane and carbon dioxide have limited uses as feedstocks for chemicals and fuels.  This disparity contrasts with the relative abundance of methane and carbon dioxide.  Methane is often partially converted to carbon monoxide for utilization in Fischer-Tropsch processes. Of interest for upgrading methane is its oxidative coupling:\n\n2CH4 + O2 → C2H4 + 2H2O\nConversion of carbon dioxide to unsaturated hydrocarbons via electrochemical reduction is a hopeful avenue of research. Still, no stable and economic technology has yet been developed.\nBiochemistry\n\nMethane, carbon monoxide, carbon dioxide, and methanol are substrates and products of enzymatic processes.  \nIn methanogenesis, carbon monoxide, carbon dioxide, and methanol are converted to methane. Methanogenesis by methanogenic archaea is reversible.\nIn photosynthesis, carbon dioxide and water are converted to sugars (and O2), the energy for this (thermally) uphill reaction being provided by sunlight.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "C1 chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_63498286",
    "text": "Calconcarboxylic acid (IUPAC name 3-hydroxy-4-[(2-hydroxy-4-sulfonaphthalen-1-yl)diazenyl]naphthalene-2-carboxylic acid; commonly called Patton and Reeder's Indicator) is an azo dye that is used as an indicator for complexometric titrations of calcium with ethylenediaminetetraacetic acid (EDTA) in the presence of magnesium. Structurally, it is similar to eriochrome blue black R, which is obtained from calconcarboxylic acid  by decarboxylation and reaction with sodium hydroxide.\n\nProperties\nCalconcarboxlic acid is soluble in water and a variety of other solvents, including sodium hydroxide, ethanol and methanol. It has a violet colour in dissolved form in ethanol. The melting point of calconcarboxylic acid is at approximately 300 °C, where it undergoes thermal decomposition.\nBackground\nThough the determination of calcium and magnesium by complexometric titration with standard solutions of disodium dihydrogen tetraacetate, utilising Eriochrome Black T as indicator is widely accepted and quite adequately understood, it, like other complexometric titration methods, suffers from the limitations of having an indistinct endpoint (where a photometric titrator is needed to provide acceptable accuracy) and/or having to separate the metals before titration can occur. Calconcarboxylic acid was thus adopted as a superior alternative due to its ability to give a good and visual endpoint and its rapid performance even with the presence of magnesium.\nSynthesis\nAs described by James Patton and Wendell Reeder in 1956, calconcarboxylic acid can be synthesised by coupling diazotized 1-amino-2-naphthol-4-sulfonic acid with 2-hydroxy-3-napthoic acid.\nApplications\nCalconcarboxylic acid is used for the determination of calcium ion concentration by complexometric titration. Free calconcarboxylic acid is blue colour, but changes to pink/red when it forms a complex with calcium ions. EDTA forms a more stable complex with calcium than calconcarboxylic acid does, so addition of EDTA to the Ca–calconcarboxylic acid complex causes formation of Ca-EDTA instead, leading to reversion to the blue colour of free calconcarboxylic acid.\nFor the complexometric titration, the indicator is first added to the titrant containing the calcium ions to form the calcium ion-indicator complex (Ca-PR) with a pink/red colour. This is then titrated against a standard solution of EDTA. The endpoint can be observed when the indicator produces a sharp, stable colour change from wine red to pure blue, which occurs at pH values between 12 and 14, this indicates the endpoint of the titration, as the Ca-PR complexes have been completely replaced by the Ca-EDTA complexes and hence the PR indicator reverts to its blue colour.\nThe reaction can be given by:\n\nCa-PR + EDTA4- → PR + [Ca-EDTA]2-\nThe Patton-Reeder Indicator is often used here in the form of a triturate.\nThis method of complexometric titration is dependent on the pH of the solution being sufficiently high to ensure that magnesium ions precipitate ",
    "source": "wikipedia",
    "title": "Calconcarboxylic acid",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_81646131",
    "text": "Carbodiphosphoranes are a class of organophosphorus compounds and a subtype of carbones with general formula C(PR3)2, consisting of a central carbon atom bound to two phosphine ligands by dative bonds. The central carbon atom has formal oxidation state zero and two high-energy lone pairs with σ and π-symmetry, making carbodiphosphoranes highly nucleophilic and strongly σ- and π-donating through the carbon atom. Carbodiphosphoranes have gained interest for their unique double-donating properties, and have been used as ligands in a number of main group and transition metal complexes with applications in catalysis. Carbodiphosphoranes generally have a bent molecular geometry, but observed P–C–P bond angles range from 100° to 180°.\nThe term “carbodiphosphorane” can also refer to hexaphenylcarbodiphosphorane, the most common carbodiphosphorane used in chemistry and the first species synthesized.\n\nStructure and Bonding\nThe electronic structure of carbodiphosphoranes was initially proposed alongside their original synthesis in 1961 to be a resonance hybrid of a double-bonded species isoelectronic to carbodiimides and a bisylide with the central carbon atom having a formal charge of -2. Kaska et al. in 1971 proposed an additional coordinatively unsaturated structure with two dative bonds from the phosphorus lone pairs and all four valence electrons of carbon unengaged in bonding, which has since been confirmed as the major resonance contributor to carbodiphosphoranes by computational studies. In 1976, the earliest synthesis of a geminal dimetallated carbodiphosphorane species was reported by Schmidbaur et al., demonstrating for the first time their ability to bind two metals at the central carbon, which provided experimental evidence consistent with the bisylide and zero-valent resonance structures. \n\nThe electronic structure of the central carbon atom can be compared to the central carbon of N-heterocyclic carbenes, which have a σ-symmetric lone pair as the HOMO and an unoccupied π-symmetric orbital as the LUMO, making them good σ-donors and π-acceptors. In contrast, carbodiphosphoranes have two lone pairs, the π-symmetric HOMO and the σ-symmetric HOMO-1, and no carbon-centered electrons engaged in bonding. For this reason, carbodiphosphoranes are more strongly Lewis basic than carbenes and have stronger electron-donating ability, which has been evaluated by the Tolman Electronic Parameter (TEP). TEP can also be used to compare relative donor strengths between carbodiphosphoranes, which differ based on the nature of the phosphine. Because of the presence of the lone pairs, the molecular geometry is typically bent, with common P–C–P bond angles being around 120°–145°. However, a bond angle of 180° has been observed in hexaphenylcarbodiphosphorane under some crystallization conditions, instead of its typical 136.9° bond angle. This linear structure is unique among carbones, and the authors attribute this structure to the low energy difference of 3.1 kcal/",
    "source": "wikipedia",
    "title": "Carbodiphosphoranes",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_12502695",
    "text": "The carryover effect is a term used in clinical chemistry to describe the transfer of unwanted material from one container or mixture to another. It describes the influence of one sample upon the following one. It may be from a specimen, or a reagent, or even the washing medium. The significance of carry over is that even a small amount can  lead to erroneous results.\n\nCarryover effect in clinical laboratory\nCarryover experiments are widely used for clinical chemistry and immunochemistry analyzers to evaluate and validate carryover effects.  The pipetting and washing systems in an automated analyzer are designed to continuously cycle between the aspiration of patient specimens and cleaning. An obvious concern is a potential for carryover of analyte from one patient specimen into one or more following patient specimens, which can falsely increase or decrease the measured analyte concentration. Specimen carryover is typically addressed by judicious choice of probe material, probe design, and an efficient probe washing system to flush the probe of residual patient specimens or reagents retained in their bores or clinging to the probe exterior surface before they are introduced into the next patient sample, reagent container, or cuvette/reaction vessel.\nSignificance in carryover assessment\nThe pathological range of measurement could be of several order to reference interval(e.g., Sex hormone, Tumor marker, Troponin...etc.).  A small portion of carryover could lead to erroneous results.\nCarryover assessment\n\nIUPAC made a recommendation in 1991 for the description and measurement of carryover effects in clinical chemistry.  The carryover ratio is the percentage of H3 carry to L1 constituting the carryover portion \"h\".  In a design of 3 high samples followed by 3 low samples, h can be calculated as (L1 - mean of L2&L3) / (H3 - mean of L2&L3)\nThe carry-over ratio's acceptance criteria depend on the measurement and the laboratory concerned.  For example, 1% carryover of plasma albumin would generally lead to a clinically insignificant effect, while 1% carryover of cardiac High sensitivity Troponin assay would be catastrophic.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Carryover effect",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_11092324",
    "text": "Chemistry is often called the central science because of its role in connecting the physical sciences, which include chemistry, with the life sciences, pharmaceutical sciences and applied sciences such as medicine and engineering. The nature of this relationship is one of the main topics in the philosophy of chemistry and in scientometrics. The phrase was popularized by its use in a textbook by Theodore L. Brown and H. Eugene LeMay, titled Chemistry: The Central Science, which was first published in 1977, with a fifteenth edition published in 2021.\nThe central role of chemistry can be seen in the systematic and hierarchical classification of the sciences by Auguste Comte. Each discipline provides a more general framework for the area it precedes (mathematics → astronomy → physics → chemistry → biology → social sciences). Balaban and Klein have more recently proposed a diagram showing the partial ordering of sciences in which chemistry may be argued is \"the central science\" since it provides a significant degree of branching. In forming these connections the lower field cannot be fully reduced to the higher ones. It is recognized that the lower fields possess emergent ideas and concepts that do not exist in the higher fields of science.\nThus chemistry is built on an understanding of laws of physics that govern particles such as atoms, protons, neutrons, electrons, thermodynamics, etc. although it has been shown that it has not been \"fully 'reduced' to quantum mechanics\". Concepts such as the periodicity of the elements and chemical bonds in chemistry are emergent in that they are more than the underlying forces defined by physics.\nIn the same way, biology cannot be fully reduced to chemistry, although the machinery that is responsible for life is composed of molecules. For instance, the machinery of evolution may be described in terms of chemistry by the understanding that it is a mutation in the order of genetic base pairs in the DNA of an organism. However, chemistry cannot fully describe the process since it does not contain concepts such as natural selection that are responsible for driving evolution. Chemistry is fundamental to biology since it provides a methodology for studying and understanding the molecules that compose cells.\nConnections made by chemistry are formed through various sub-disciplines that utilize concepts from multiple scientific disciplines. Chemistry and physics are both needed in the areas of physical chemistry, nuclear chemistry, and theoretical chemistry. Chemistry and biology intersect in the areas of biochemistry, medicinal chemistry, molecular biology, chemical biology, molecular genetics, and immunochemistry. Chemistry and the earth sciences intersect in areas like geochemistry and hydrology.\n\nSee also\nFundamental science\nHard and soft science\nPhilosophy of chemistry\nSpecial sciences\nUnity of science\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "The central science",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_47856613",
    "text": "The charged aerosol detector (CAD) is a detector used in conjunction with high-performance liquid chromatography (HPLC) and ultra high-performance liquid chromatography (UHPLC) to measure the amount of chemicals in a sample by creating charged aerosol particles which are detected using an electrometer. It is commonly used for the analysis of compounds that cannot be detected using traditional UV/Vis approaches due to their lack of a chromophore. The CAD can measure all non-volatile and many semi-volatile analytes including, but not limited to, antibiotics, excipients, ions, lipids, natural products, biofuels, sugars and surfactants. The CAD, like other aerosol detectors (e.g., evaporative light scattering detectors (ELSD) and condensation nucleation light scattering detectors (CNLSD)), falls under the category of destructive general-purpose detectors (see Chromatography detectors).\n\nHistory\nThe predecessor to the CAD, termed an evaporative electrical detector, was first described by Kaufman in 2002 at TSI Inc in US patent 6,568,245 and was based on the coupling of liquid chromatographic approaches to TSI's electrical aerosol measurement (EAM) technology.  At around the same time Dixon and Peterson at California State University were investigating the coupling of liquid chromatography to an earlier version of TSI's EAM technology, which they called an aerosol charge detector. Subsequent collaboration between TSI and ESA Biosciences Inc. (now part of Thermo Fisher Scientific), led to the first commercial instrument, the Corona CAD, which received both the Pittsburgh Conference Silver Pittcon Editor's Award (2005) and R&D 100 award (2005). Continued research and engineering improvements in product design resulted in CADs with ever increasing capabilities. The newest iterations of the CAD are the Thermo Scientific Corona Veo Charged Aerosol Detector, Corona Veo RS Charged Aerosol Detector and Thermo Scientific Vanquish Charged Aerosol Detectors.\nPrinciples of operation\nThe general detection scheme  involves:\n\nPneumatic nebulization of mobile phase from the analytical column forming an aerosol.\nAerosol conditioning to remove large droplets.\nEvaporation of solvent from the droplets to form dried particles.\nParticle charging using an ion jet formed via corona discharge.\nParticle selection – an ion trap is used to excess ions and high mobility charged particles.\nMeasurement of the aggregate charge of aerosol particles using a filter/electrometer.\nThe CAD like other aerosol detectors, can only be used with volatile mobile phases. For an analyte to be detected it must be less volatile than the mobile phase.\nMore detailed information on how CAD works can be found on the Charged Aerosol Detection for Liquid Chromatography Resource Center.\n",
    "source": "wikipedia",
    "title": "Charged aerosol detector",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_28254396",
    "text": "Chemical bath deposition, also called chemical solution deposition and CBD, is a method of thin-film deposition (solids forming from a solution or gas), using an aqueous precursor solution. Chemical bath deposition typically forms films using heterogeneous nucleation (deposition or adsorption of aqueous ions onto a solid substrate), to form homogeneous thin films of metal chalcogenides (mostly oxides, sulfides, and selenides) and many less common ionic compounds. Chemical bath deposition produces films reliably, using a simple process with little infrastructure, at low temperature (<100 ˚C), and at low cost. Furthermore, chemical bath deposition can be employed for large-area batch processing or continuous deposition. Films produced by CBD are often used in semiconductors, photovoltaic cells, and supercapacitors, and there is increasing interest in using chemical bath deposition to create nanomaterials.\n\nUses\nChemical bath deposition is useful in industrial applications because it is extremely cheap, simple, and reliable compared to other methods of thin-film deposition, requiring only aqueous solution at (relatively) low temperatures and minimal infrastructure. The chemical bath deposition process can easily be scaled up to large-area batch processing or continuous deposition. \nChemical bath deposition forms small crystals, which are less useful for semiconductors than the larger crystals created by other methods of thin-film deposition but are more useful for nano materials. However, films formed by chemical bath deposition often have better photovoltaic properties (band electron gap) than films of the same substance formed by other methods.\nProcess\nChemical bath deposition relies on creating a solution such that deposition (changing from an aqueous to a solid substance) will only occur on the substrate, using the method below:\n\nMetal salts and (usually) chalcogenide precursors are added to water to form an aqueous solution containing the metal ions and chalcogenide ions which will form the compound to be deposited.\nTemperature, pH, and concentration of salts are adjusted until the solution is in metastable supersaturation, that is until the ions are ready to deposit but can’t overcome the thermodynamic barrier to nucleation (forming solid crystals and precipitating out of the solution).\nA substrate is introduced, which acts as a catalyst to nucleation, and the precursor ions adhere to onto the substrate forming a thin crystalline film by one of the two methods described below.\nThat is, the solution is in a state where the precursor ions or colloidal particles are ‘sticky’, but can’t 'stick' to each other. When the substrate is introduced, the precursor ions or particles stick to it and aqueous ions stick to solid ions, forming a solid compound—depositing to form crystalline films. \nThe pH, temperature, and composition of the film affect crystal size, and can be used to control the rate of formation and the structure of the film. Other factors ",
    "source": "wikipedia",
    "title": "Chemical bath deposition",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_1686272",
    "text": "Chemical biology is a scientific field at the interface of chemistry and biology that applies chemical methods to the study of biological systems. It involves the use of chemical techniques, analytical approaches, and often small molecules produced through synthetic chemistry to probe, characterize, and manipulate biological processes and systems at the molecular level.\nAlthough it overlaps with biochemistry, which focuses on the chemistry of biomolecules and the regulation of biochemical pathways within and between cells, chemical biology is distinguished by its emphasis on the deliberate design and application of chemical tools to address biological questions.\n\nHistory\nAlthough considered a relatively new scientific field, the term \"chemical biology\" has been in use since the early 20th century, and has roots in scientific discovery from the early 19th century. The term 'chemical biology' can be traced back to an early appearance in a book published by Alonzo E. Taylor in 1907 titled \"On Fermentation\", and was subsequently used in John B. Leathes' 1930 article titled \"The Harveian Oration on The Birth of Chemical Biology\". However, it is unclear when the term was first used.\nFriedrich Wöhler's 1828 synthesis of urea is an early example of the application of synthetic chemistry to advance biology. It showed that biological compounds could be synthesized with inorganic starting materials and weakened the previous notion of vitalism, or that a 'living' source was required to produce organic compounds. Wöhler's work is often considered to be instrumental in the development of organic chemistry and natural product synthesis, both of which play a large part in modern chemical biology.\nFriedrich Miescher's work during the late 19th century investigating the cellular contents of human leukocytes led to the discovery of 'nuclein', which would later be renamed DNA. After isolating the nuclein from the nucleus of leukocytes through protease digestion, Miescher used chemical techniques such as elemental analysis and solubility tests to determine the composition of nuclein. This work would lay the foundations for Watson and Crick's discovery of the double-helix structure of DNA.\nThe rising interest in chemical biology has led to several journals dedicated to the field. Nature Chemical Biology, created in 2005, and ACS Chemical Biology, created in 2006, are two of the most well-known journals in this field, with impact factors of 14.8 and 4.0 respectively.\n",
    "source": "wikipedia",
    "title": "Chemical biology",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_21347411",
    "text": "A chemical compound is a chemical substance composed of many identical molecules (or molecular entities) containing atoms from more than one chemical element held together by chemical bonds. A molecule consisting of atoms of only one element is therefore not a compound. A compound can be transformed into a different substance by a chemical reaction, which may involve interactions with other substances. In this process, bonds between atoms may be broken or new bonds formed or both.\nThere are four major types of compounds, distinguished by how the constituent atoms are bonded together. Molecular compounds are held together by covalent bonds, ionic compounds are held together by ionic bonds, intermetallic compounds are held together by metallic bonds, and coordination complexes are held together by coordinate covalent bonds. Non-stoichiometric compounds form a disputed marginal case.\nA chemical formula specifies the number of atoms of each element in a compound molecule, using the standard chemical symbols with numerical subscripts. Many chemical compounds have a unique CAS number identifier assigned by the Chemical Abstracts Service. Globally, more than 350,000 chemical compounds (including mixtures of chemicals) have been registered for production and use.\n\nHistory of the concept\n\nDefinitions\nAny substance consisting of two or more different types of atoms (chemical elements) in a fixed stoichiometric proportion can be termed a chemical compound; the concept is most readily understood when considering pure chemical substances. It follows from their being composed of fixed proportions of two or more types of atoms that chemical compounds can be converted, via chemical reaction, into compounds or substances each having fewer atoms. A chemical formula is a way of expressing information about the proportions of atoms that constitute a particular chemical compound, using chemical symbols for the chemical elements, and subscripts to indicate the number of atoms involved. For example, water is composed of two hydrogen atoms bonded to one oxygen atom: the chemical formula is H2O. In the case of non-stoichiometric compounds, the proportions may be reproducible with regard to their preparation, and give fixed proportions of their component elements, but proportions that are not integral [e.g., for palladium hydride, PdHx (0.02 < x < 0.58)].\nChemical compounds have a unique and defined chemical structure held together in a defined spatial arrangement by chemical bonds. Chemical compounds can be molecular compounds held together by covalent bonds, salts held together by ionic bonds, intermetallic compounds held together by metallic bonds, or the subset of chemical complexes that are held together by coordinate covalent bonds. Pure chemical elements are generally not considered chemical compounds, failing the two or more atom requirement, though they often consist of molecules composed of multiple atoms (such as in the diatomic molecule H2, or the polyatomic m",
    "source": "wikipedia",
    "title": "Chemical compound",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_5659",
    "text": "A chemical element is a species of atom defined by its number of protons. The number of protons is called the atomic number of that element. For example, oxygen has an atomic number of 8: each oxygen atom has 8 protons in its nucleus. Atoms of the same element can have different numbers of neutrons in their nuclei, known as isotopes of the element. Atoms of one element can be transformed into atoms of a different element in nuclear reactions, which change an atom's atomic number. Almost all baryonic matter in the universe is composed of elements (among rare exceptions are neutron stars).\nThe term \"chemical element\" is also widely used to mean a pure chemical substance consisting of a single element. For example, oxygen gas consists only of atoms of oxygen.\nHistorically, the term \"chemical element\" meant a substance that cannot be broken down into constituent substances by chemical reactions, and for most practical purposes this definition still has validity. There was some controversy in the 1920s over whether isotopes deserved to be recognised as separate elements if they could be separated by chemical means. By November 2016, the International Union of Pure and Applied Chemistry (IUPAC) recognized a total of 118 elements. The first 94 occur naturally on Earth, and the remaining 24 are synthetic elements produced in nuclear reactions. Save for unstable radioactive elements (radioelements) which decay quickly, nearly all elements are available industrially in varying amounts. The discovery and synthesis of further new elements is an ongoing area of scientific study.\nThe history of the discovery and use of elements began with early human societies that discovered native minerals like carbon, sulfur, copper and gold (though the modern concept of an element was not yet understood). Attempts to classify materials such as these resulted in the concepts of classical elements, alchemy, and similar theories throughout history. Much of the modern understanding of elements developed from the work of Dmitri Mendeleev, a Russian chemist who published the first recognizable periodic table in 1869. This table organizes the elements by increasing atomic number into rows (\"periods\") in which the columns (\"groups\") share recurring (\"periodic\") physical and chemical properties. The periodic table summarizes various properties of the elements, allowing chemists to derive relationships between them and to make predictions about elements not yet discovered, and potential new compounds.\n\nDescription\nThe term \"(chemical) element\" is used in two different but closely related meanings: it can mean a chemical substance consisting of a single kind of atom (a free element), or it can mean that kind of atom as a component of various chemical substances. For example, water (H2O) consists of the elements hydrogen (H) and oxygen (O) even though it does not contain the chemical substances (di)hydrogen (H2) and (di)oxygen (O2), as H2O molecules are different from H2 and O2 molecu",
    "source": "wikipedia",
    "title": "Chemical element",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_199040",
    "text": "A chemical equation or chemistry notation is the symbolic representation of a chemical reaction in the form of symbols and chemical formulas. The reactant entities are given on the left-hand side, and the product entities are on the right-hand side with a plus sign between the entities in both the reactants and the products, and an arrow that points towards the products to show the direction of the reaction. The chemical formulas may be symbolic, structural (pictorial diagrams), or intermixed. The coefficients next to the symbols and formulas of entities are the absolute values of the stoichiometric numbers. The first chemical equation was diagrammed by Jean Beguin in 1615.\n\nStructure\nA chemical equation (see an example below) consists of a list of reactants (the starting substances) on the left-hand side, an arrow symbol, and a list of products (substances formed in the chemical reaction) on the right-hand side. Each substance is specified by its chemical formula, optionally preceded by a number called stoichiometric coefficient. The coefficient specifies how many entities (e.g. molecules) of that substance are involved in the reaction on a molecular basis. If not written explicitly, the coefficient is equal to 1. Multiple substances on any side of the equation are separated from each other by a plus sign.\nAs an example, the equation for the reaction of hydrochloric acid with sodium can be denoted:\n\n2HCl + 2Na  →  2NaCl + H2\nGiven the formulas are fairly simple, this equation could be read as \"two H-C-L plus two N-A yields two N-A-C-L and H two.\" Alternately, and in general for equations involving complex chemicals, the chemical formulas are read using IUPAC nomenclature, which could verbalise this equation as \"two hydrochloric acid molecules and two sodium atoms react to form two formula units of sodium chloride and a hydrogen gas molecule.\"\nBalancing chemical equations\nBecause no nuclear reactions take place in a chemical reaction, the chemical elements pass through the reaction unchanged. Thus, each side of the chemical equation must represent the same number of atoms of any particular element (or nuclide, if different isotopes are taken into account). The same holds for the total electric charge, as stated by the charge conservation law. An equation adhering to these requirements is said to be balanced.\nA chemical equation is balanced by assigning suitable values to the stoichiometric coefficients. Simple equations can be balanced by inspection, that is, by trial and error. Another technique involves solving a system of linear equations.\nBalanced equations are usually written with smallest natural-number coefficients. Yet sometimes it may be advantageous to accept a fractional coefficient, if it simplifies the other coefficients. The introductory example can thus be rewritten as\n\n  \n    \n      \n        \n          HCl\n          +\n          Na\n          ⟶\n          NaCl\n          +\n          \n            \n              1\n              2\n      ",
    "source": "wikipedia",
    "title": "Chemical equation",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_25162865",
    "text": "Chemical free is a term used in marketing to imply that a product is safe, healthy or environmentally friendly because it only contains natural ingredients. The term is a misnomer, as all substances and objects are composed entirely of chemicals and energy. The term chemical is roughly a synonym for matter, and all substances, such as water and air, are chemicals.\nThe use of the term chemical free in advertising to indicate that a product is free of synthetic chemicals, and the tolerance of its use in this fashion by the United Kingdom's Advertising Standards Authority, has been the subject of criticism.\nA study on American undergraduates' understanding of the term chemical, conducted by chemist Gayle Nicoll in 1997, noted that \"People may hold both a scientific and layman's definition of a chemical without linking the two together in any way. They may or may not consciously distinguish that the term 'chemical' has different connotations depending on the situation.\"\n\nSee also\nAppeal to nature\nChemophobia\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Chemical free",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_10257708",
    "text": "A chemical library or compound library is a collection of stored chemicals usually used ultimately in high-throughput screening or industrial manufacture. The chemical library can consist in simple terms of a series of stored chemicals. Each chemical has associated information stored in some kind of database with information such as the chemical structure, purity, quantity, and physiochemical characteristics of the compound.\n\nPurpose\nIn drug discovery high-throughput screening, it is desirable to screen a drug target against a selection of chemicals that try to take advantage of as much of the appropriate chemical space as possible. The chemical space of all possible chemical structures is extraordinarily large. Most stored chemical libraries do not typically have a fully represented or sampled chemical space mostly because of storage and cost concerns. However, since many molecular interactions cannot be predicted, the wider the chemical space that is sampled by the chemical library, the better the chance that high-throughput screening will find a \"hit\"—a chemical with an appropriate interaction in a biological model that might be developed into a drug.\nAn example of a chemical library in drug discovery would be a series of chemicals known to inhibit kinases, or in industrial processes, a series of catalysts known to polymerize resins.\nGeneration of chemical libraries\nChemical libraries are usually generated for a specific goal and larger chemical libraries could be made of several groups of smaller libraries stored in the same location.  In the drug discovery process for instance, a wide range of organic chemicals are needed to test against models of disease in high-throughput screening. Therefore, most of the chemical synthesis needed to generate chemical libraries in drug discovery is based on organic chemistry. A company that is interested in screening for kinase inhibitors in cancer may limit their chemical libraries and synthesis to just those types of chemicals known to have affinity for ATP binding sites or allosteric sites.\nGenerally, however, most chemical libraries focus on large groups of varied organic chemical series where an organic chemist can make many variations on the same molecular scaffold or molecular backbone. Sometimes chemicals can be purchased from outside vendors as well and included into an internal chemical library.\nDepending upon their scope and design, chemical libraries can also be classified as diverse oriented, Drug-like, Lead-like, peptide-mimetic, Natural Product-like, Targeted against a specific family of biological targets such Kinases, GPCRs, Proteases, PPI etc. These chemical libraries are often used in target based drug discovery (reverse pharmacology). Among the compound libraries should be annotated the Fragment Compound Libraries, which are mainly used for Fragment-based lead discovery.\n",
    "source": "wikipedia",
    "title": "Chemical library",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_6271",
    "text": "A chemical reaction is a process that leads to the chemical transformation of one set of chemical substances to another. When chemical reactions occur, the atoms are rearranged and the reaction is accompanied by an energy change as new products are generated. Classically, chemical reactions encompass changes that only involve the positions of electrons in the forming and breaking of chemical bonds between atoms, with no change to the nuclei (no change to the elements present), and can often be described by a chemical equation. Nuclear chemistry is a sub-discipline of chemistry that involves the chemical reactions of unstable and radioactive elements where both electronic and nuclear changes can occur.\nThe substance (or substances) initially involved in a chemical reaction are called reactants or reagents. Chemical reactions are usually characterized by a chemical change, and they yield one or more products, which usually have properties different from the reactants. Reactions often consist of a sequence of individual sub-steps, the so-called elementary reactions, and the information on the precise course of action is part of the reaction mechanism. Chemical reactions are described with chemical equations, which symbolically present the starting materials, end products, and sometimes intermediate products and reaction conditions.\nChemical reactions happen at a characteristic reaction rate at a given temperature and chemical concentration. Some reactions produce heat and are called exothermic reactions, while others may require heat to enable the reaction to occur, which are called endothermic reactions. Typically, reaction rates increase with increasing temperature because there is more thermal energy available to reach the activation energy necessary for breaking bonds between atoms.\nA reaction may be classified as redox in which oxidation and reduction occur or non-redox in which there is no oxidation and reduction occurring. Most simple redox reactions may be classified as a combination, decomposition, or single displacement reaction.\nDifferent chemical reactions are used during chemical synthesis in order to obtain the desired product. In biochemistry, a consecutive series of chemical reactions (where the product of one reaction is the reactant of the next reaction) form metabolic pathways. These reactions are often catalyzed by protein enzymes. Enzymes increase the rates of biochemical reactions, so that metabolic syntheses and decompositions impossible under ordinary conditions can occur at the temperature and concentrations present within a cell.\nThe general concept of a chemical reaction has been extended to reactions between entities smaller than atoms, including nuclear reactions, radioactive decays and reactions between elementary particles, as described by quantum field theory.\n\nHistory\nChemical reactions such as combustion in fire, fermentation and the reduction of ores to metals were known since antiquity. Initial theories of transfo",
    "source": "wikipedia",
    "title": "Chemical reaction",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_16090759",
    "text": "Chemical similarity (or molecular similarity) refers to the similarity of chemical elements, molecules or chemical compounds with respect to either structural or functional qualities, i.e. the effect that the chemical compound has on reaction partners in inorganic or biological settings. Biological effects and thus also similarity of effects are usually quantified using the biological activity of a compound. In general terms, function can be related to the chemical activity of compounds (among others).\n\nThe notion of chemical similarity (or molecular similarity) is one of the most important concepts in cheminformatics. It plays an important role in modern approaches to predicting the properties of chemical compounds, designing chemicals with a predefined set of properties and, especially, in conducting drug design studies by screening large databases containing structures of available (or potentially available) chemicals. These studies are based on the similar property principle of Johnson and Maggiora, which states: similar compounds have similar properties.\n\nSimilarity measures\nChemical similarity is often described as an inverse of a measure of distance in descriptor space. Examples for inverse distance measures are molecule kernels, that measure the structural similarity of chemical compounds.\nSimilarity search and virtual screening\nThe similarity-based virtual screening (a kind of ligand-based virtual screening) assumes that all compounds in a database that are similar to a query compound have similar biological activity. Although this hypothesis is not always valid, quite often the set of retrieved compounds is considerably enriched with actives. To achieve high efficacy of similarity-based screening of databases containing millions of compounds, molecular structures are usually represented by molecular screens (structural keys) or by fixed-size or variable-size molecular fingerprints. Molecular screens and fingerprints can contain both 2D- and 3D-information. However, the 2D-fingerprints, which are a kind of binary fragment descriptors, dominate in this area. Fragment-based structural keys, like MDL keys, are sufficiently good for handling small and medium-sized chemical databases, whereas processing of large databases is performed with fingerprints having much higher information density. Fragment-based Daylight, BCI, and UNITY 2D (Tripos) fingerprints are the best known examples. The most popular similarity measure for comparing chemical structures represented by means of fingerprints is the Tanimoto (or Jaccard) coefficient T. Two structures are usually considered similar if T > 0.85 (for Daylight fingerprints). However, it is a common misunderstanding that a similarity of T > 0.85 reflects similar bioactivities in general (\"the 0.85 myth\").\n",
    "source": "wikipedia",
    "title": "Chemical similarity",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_5736171",
    "text": "The chemical state of a chemical element is due to its electronic, chemical and physical properties as it exists in combination with itself or a group of one or more other elements. A chemical state is often defined as an \"oxidation state\" when referring to metal cations. When referring to organic materials, a chemical state is usually defined as a chemical group, which is a group of several elements bonded together. Material scientists, solid state physicists, analytical chemists, surface scientists and spectroscopists describe or characterize the chemical, physical and/or electronic nature of the surface or the bulk regions of a material as having or existing as one or more chemical states.\n\nOverview\nThe chemical state set comprises and encompasses these subordinate groups and entities: chemical species, functional group, anion, cation, oxidation state, chemical compound and elemental forms of an element.\nThis term or phrase is commonly used when interpreting data from analytical techniques such as: \n\nAuger electron spectroscopy (AES)\nEnergy-dispersive X-ray spectroscopy (EDS, EDX)\nInfrared spectroscopy (IR, FT-IR, ATR)\nLiquid chromatography (LC, HPLC)\nMass spectrometry (MS, ToF-SIMS, D-SIMS)\nNuclear magnetic resonance (NMR, H-NMR, C-NMR, X-NMR)\nPhotoemission spectroscopy (PES, UPS)\nRaman spectroscopy (FT-Raman)\nUltraviolet-visible spectroscopy (UV-Vis)\nX-ray photoelectron spectroscopy (XPS, ESCA)\nWavelength dispersive X-ray spectroscopy (WDX, WDS)\nSignificance\nThe chemical state of a group of elements, can be similar to, but not identical to, the chemical state of another similar group of elements because the two groups have different ratios of the same elements and exhibit different chemical, electronic, and physical properties that can be detected by various spectroscopic techniques.\nA chemical state can exist on or inside the surface of a solid state material and can often, but not always, be isolated or separated from the other chemical species found on the surface of that material. Surface scientists, spectroscopists, chemical analysts, and material scientists frequently describe the chemical nature of the chemical species, functional group, anion, or cation detected on the surface and near the surface of a solid state material as its chemical state.\nTo understand how a chemical state differs from an oxidation state, anion, or cation, compare sodium fluoride (NaF) to polytetrafluoroethylene (PTFE, Teflon). Both contain fluorine, the most electronegative element, but only NaF dissolves in water to form separate ions, Na+ and F−. The electronegativity of the fluorine strongly polarizes the electron density that exists between the carbon and the fluorine, but not enough to produce ions which would allow it to dissolve in the water. The carbon and fluorine in Teflon (PTFE) both have an electronic charge of zero since they form a covalent bond, but few scientists describe those elements as having an oxidation state of zero. On the other hand, ",
    "source": "wikipedia",
    "title": "Chemical state",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_85029",
    "text": "Chemical synthesis (chemical combination) is the artificial execution of chemical reactions to obtain one or more products. This occurs by physical and chemical manipulations, usually involving one or more reactions. In modern laboratory uses, the process is reproducible and reliable.\nA chemical synthesis involves one or more compounds (known as reagents or reactants) that will experience a transformation under certain conditions. Various reaction types can be applied to formulate a desired product. Many reactions require some form of processing (\"work-up\") or purification procedure to isolate the final product.\nThe amount produced by chemical synthesis is known as the reaction yield. Typically, yields are expressed as a mass in grams (in a laboratory setting) or as a percentage of the total theoretical quantity that could be produced based on the limiting reagent. A side reaction is an unwanted chemical reaction that can reduce the desired yield. The word synthesis was used first in a chemical context by the chemist Hermann Kolbe.\n\nStrategies\nChemical synthesis employs various strategies to achieve efficient and precise molecular transformations that are more complex than simply converting a reactant A to a reaction product B directly. These strategies can be grouped into approaches for managing reaction sequences.\nOrganic synthesis\nOrganic synthesis is a special type of chemical synthesis dealing with the synthesis of organic compounds. For the total synthesis of a complex product, multiple procedures in sequence may be required to synthesize the product of interest, needing a lot of time. A purely synthetic chemical synthesis begins with basic lab compounds. A semisynthetic process starts with natural products from plants or animals and then modifies them into new compounds.\nInorganic synthesis\nInorganic synthesis and organometallic synthesis are used to prepare compounds with significant non-organic content. An illustrative example is the preparation of the anti-cancer drug cisplatin from potassium tetrachloroplatinate.\n",
    "source": "wikipedia",
    "title": "Chemical synthesis",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_7746249",
    "text": "Chemical technologists and technicians (abbr. chem techs) are workers who provide technical support or services in chemical-related fields.  They may work under direct supervision or may work independently, depending on their specific position and duties.  Their work environments differ widely and include, but are not limited to, laboratories and industrial settings.  As such, it is nearly impossible to generalize the duties of chem techs as their individual jobs vary greatly.  Biochemical techs often do similar work in biochemistry.\n\nTechnologists\nChemical technologists are more likely than technicians to participate in the actual design of experiments, and may be involved in the interpretation of experimental data.  They may also be responsible for the operation of chemical processes in large plants, and may even assist chemical engineers in the design of the same.\nSome post-secondary education is generally required to be either a chemical technician or technologist.  Occasionally, a company may be willing to provide a high school graduate with training to become a chemical technician, but more often, a two-year degree will be required. Chemical technologists generally require completion of a specific college program—either two year or four year— in chemical, biochemical, or chemical engineering technology or a closely related discipline. \nThey usually work under or with a scientist such as a chemist or biochemist.\nTechnicians\nChemical or biochemical technicians often work in clinical (medical) laboratories conducting routine analyses of medical samples such as blood and urine.  Industries which employ chem techs include chemical, petrochemical, and pharmaceutical industries. Companies within these industries can be concerned with manufacturing, research and development (R&D), consulting, quality control, and a variety of other areas.  Also, chem techs working for these companies may be used to conduct quality control and other routine analyses, or assist in chemical and biochemical research including analyses, industrial chemistry, environmental protection, and even chemical engineering.\n",
    "source": "wikipedia",
    "title": "Chemical technologist",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_11960085",
    "text": "Chemophobia (or chemphobia or chemonoia) is an aversion to or prejudice against chemicals or chemistry. The phenomenon has been ascribed both to a reasonable concern over the potential adverse effects of synthetic chemicals, and to an irrational fear of these substances because of misconceptions about their potential for harm, particularly the possibility of certain exposures to some synthetic chemicals elevating an individual's risk of cancer.  Consumer products with labels such as \"natural\" and \"chemical free\" (the latter being impossible if taken literally, since all consumer products consist of chemical substances) appeal to chemophobic sentiments by offering consumers what appears to be a safer alternative (see appeal to nature).\n\nDefinition and uses\nThere are differing opinions on the proper usage of the word chemophobia.  The International Union of Pure and Applied Chemistry (IUPAC) defines chemophobia as an \"irrational fear of chemicals\".  According to the American Council on Science and Health, chemophobia is a fear of synthetic substances arising from \"scare stories\" and exaggerated claims about their dangers prevalent in the media.\nDespite containing the suffix -phobia, the majority of written work focusing on addressing chemophobia describes it as a non-clinical aversion or prejudice, and not as a phobia in the standard medical definition. Chemophobia is generally addressed by chemical education and public outreach despite the fact that much of chemophobia is economic or political in nature.\nMichelle Francl has written: \"We are a chemophobic culture. Chemical has become a synonym for something artificial, adulterated, hazardous, or toxic.\" She characterizes chemophobia as \"more like color blindness than a true phobia\" because chemophobics are \"blind\" to most of the chemicals that they encounter; every substance in the universe is a chemical. Francl proposes that such misconceptions are not innocuous, as demonstrated in one case by local statutes opposing the fluoridation of public water despite documented cases of tooth loss and nutritional deficit. In terms of risk perception, naturally occurring chemicals feel safer than synthetic ones to most people because of the involvement of humans.  Consequently, people fear man-made or \"unnatural\" chemicals, while accepting natural chemicals that are known to be dangerous or poisonous.\n\nThe Carcinogenic Potency Project, which is a part of the US EPA's Distributed Structure-Searchable Toxicity (DSSTox) Database Network, has been systemically testing the carcinogenicity of chemicals, both natural and synthetic, and building a publicly available database of the results since the 1980s.  Their work attempts to fill in the gaps in our scientific knowledge of the carcinogenicity of all chemicals, both natural and synthetic, as the scientists conducting the Project described in the journal, Science, in 1992: Toxicological examination of synthetic chemicals, without similar examination of chemicals t",
    "source": "wikipedia",
    "title": "Chemophobia",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_48999251",
    "text": "Chemoproteomics (also known as chemical proteomics) entails a broad array of techniques used to identify and interrogate protein-small molecule interactions. Chemoproteomics complements phenotypic drug discovery, a paradigm that aims to discover lead compounds on the basis of alleviating a disease phenotype, as opposed to target-based drug discovery (reverse pharmacology), in which lead compounds are designed to interact with predetermined disease-driving biological targets. As phenotypic drug discovery assays do not provide confirmation of a compound's mechanism of action, chemoproteomics provides valuable follow-up strategies to narrow down potential targets and eventually validate a molecule's mechanism of action. Chemoproteomics also attempts to address the inherent challenge of drug promiscuity in small molecule drug discovery by analyzing protein-small molecule interactions on a proteome-wide scale. A major goal of chemoproteomics is to characterize the interactome of drug candidates to gain insight into mechanisms of off-target toxicity and polypharmacology.\nChemoproteomics assays can be stratified into three basic types. Solution-based approaches involve the use of drug analogs that chemically modify target proteins in solution, tagging them for identification. Immobilization-based approaches seek to isolate potential targets or ligands by anchoring their binding partners to an immobile support. Derivatization-free approaches aim to infer drug-target interactions by observing changes in protein stability or drug chromatography upon binding. Computational techniques complement the chemoproteomic toolkit as parallel lines of evidence supporting potential drug-target pairs, and are used to generate structural models that inform lead optimization. Several targets of high profile drugs have been identified using chemoproteomics, and the continued improvement of mass spectrometer sensitivity and chemical probe technology indicates that chemoproteomics will play a large role in future drug discovery.\n\nBackground\n\n",
    "source": "wikipedia",
    "title": "Chemoproteomics",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_59274182",
    "text": "In modern valence bond (VB) theory calculations, Chirgwin–Coulson weights (also called Mulliken weights) are the relative weights of a set of possible VB structures of a molecule. Related methods of finding the relative weights of valence bond structures are the Löwdin and the inverse weights.\n\nBackground\nFor a wave function \n  \n    \n      \n        Ψ\n        =\n        \n          ∑\n          \n            i\n          \n        \n        \n          C\n          \n            i\n          \n        \n        \n          Φ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\Psi =\\sum \\limits _{i}C_{i}\\Phi _{i}}\n  \n where \n  \n    \n      \n        \n          Φ\n          \n            1\n          \n        \n        ,\n        \n          Φ\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          Φ\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\Phi _{1},\\Phi _{2},\\dots ,\\Phi _{n}}\n  \n are a linearly independent, orthogonal set of basis orbitals, the weight of a constituent orbital \n  \n    \n      \n        \n          Ψ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\Psi _{i}}\n  \n would be \n  \n    \n      \n        \n          C\n          \n            i\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle C_{i}^{2}}\n  \n since the overlap integral, \n  \n    \n      \n        \n          S\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle S_{ij}}\n  \n , between two wave functions \n  \n    \n      \n        \n          Ψ\n          \n            i\n          \n        \n        ,\n        \n          Ψ\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\Psi _{i},\\Psi _{j}}\n  \n would be 1 for \n  \n    \n      \n        i\n        =\n        j\n      \n    \n    {\\displaystyle i=j}\n  \n and 0 for \n  \n    \n      \n        i\n        ≠\n        j\n      \n    \n    {\\displaystyle i\\neq j}\n  \n . In valence bond theory, however, the generated structures are not necessarily orthogonal with each other, and oftentimes have substantial overlap between the two structures. As such, when considering non-orthogonal constituent orbitals (i.e. orbitals with non-zero overlap) the non-diagonal terms in the overlap matrix would be non-zero, and must be included in determining the weight of a constituent orbital. A method of computing the weight of a constituent orbital, \n  \n    \n      \n        \n          Φ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\Phi _{i}}\n  \n, proposed by Chirgwin and Coulson would be:\n\nApplication of the Chirgwin-Coulson formula to a molecular orbital yields the Mulliken population of the molecular orbital.\n",
    "source": "wikipedia",
    "title": "Chirgwin–Coulson weights",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_48248326",
    "text": "In chemistry, the term chromogen refers to a colourless (or faintly coloured) chemical compound that can be converted by chemical reaction into a compound which can be described as \"coloured\" (a chromophore). There is no universally agreed definition of the term. Various dictionaries give the following definitions:\n\nA substance capable of conversion into a pigment or dye.\nAny substance that can become a pigment or coloring matter, a substance in organic fluids that forms colored compounds when oxidized, or a compound, not itself a dye, that can become a dye.\nAny substance, itself without color, giving origin to a coloring matter.\nIn biochemistry the term has a rather different meaning. The following are found in various dictionaries.\n\nA precursor of a biochemical pigment\nA pigment-producing microorganism\nAny of certain bacteria that produce a pigment\nA strongly pigmented or pigment-generating organelle, organ, or microorganism.\n\nApplications in chemistry\nIn chromogenic photography, film or paper contains one or many layers of silver halide (AgX) emulsion, along with dye couplers that, in combination with processing chemistry, form visible dyes.\nApplications in biochemistry and medicine\nThe Runyon classification classifies mycobacteria by chromogenic properties.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Chromogen",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_610329",
    "text": "Clandestine chemistry is chemistry carried out in secret, and particularly in illegal drug laboratories. Larger labs are usually run by gangs or organized crime intending to produce for distribution on the black market. Smaller labs can be run by individual chemists working clandestinely in order to synthesize smaller amounts of controlled substances or simply out of a hobbyist interest in chemistry, often because of the difficulty in ascertaining the purity of other, illegally synthesized drugs obtained on the black market. The term clandestine lab is generally used in any situation involving the production of illicit compounds, regardless of whether the facilities being used qualify as a true laboratory.\n\nHistory\nAncient forms of clandestine chemistry included the manufacturing of explosives.\nFrom 1919 to 1933, the United States prohibited the sale, manufacture, or transportation of alcoholic beverages. This opened a door for brewers to supply their own town with alcohol. Just like modern-day drug labs, distilleries were placed in rural areas. The term moonshine generally referred to \"corn whiskey\", that is, a whiskey-like liquor made from corn. Today, American-made corn whiskey can be labeled or sold under that name, or as bourbon or Tennessee whiskey, depending on the details of the production process.\nPsychoactive substances\n\nExplosives\nClandestine chemistry is not limited to drugs; it is also associated with explosives, and other illegal chemicals. Of the explosives manufactured illegally, nitroglycerin and acetone peroxide are easiest to produce due to the ease with which the precursors can be acquired.\nUncle Fester is a writer who commonly writes about different aspects of clandestine chemistry. Secrets of Methamphetamine Manufacture is among his most popular books, and is considered required reading for DEA agents. More of his books deal with other aspects of clandestine chemistry, including explosives, and poisons. Fester is, however, considered by many to be a faulty and unreliable source for information in regard to the clandestine manufacture of chemicals.\n",
    "source": "wikipedia",
    "title": "Clandestine chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_49059554",
    "text": "Clay chemistry is an applied subdiscipline of chemistry which studies the chemical structures, properties and reactions of or involving clays and clay minerals. It is a multidisciplinary field, involving concepts and knowledge from inorganic and structural chemistry, physical chemistry, materials chemistry, analytical chemistry, organic chemistry, mineralogy, geology and others.\nThe study of the chemistry (and physics) of clays and clay minerals is of great academic and industrial relevance as they are among the most widely used industrial minerals, being employed as raw materials (ceramics, pottery, etc.), adsorbents, catalysts, additives, mineral charges, medicines, building materials and others.\nThe unique properties of clay minerals including: nanometric scale layered construction, presence of fixed and interchangeable charges, possibility of adsorbing and hosting (intercalating) molecules, ability of forming stable colloidal dispersions, possibility of tailored surface and interlayer chemical modification and others, make the study of clay chemistry a very important and extremely varied field of research.\nMany distinct fields and knowledge areas are impacted by the physico-chemical behavior of clay minerals, from environmental sciences to chemical process engineering, from pottery to nuclear waste management.\nTheir cation exchange capacity (CEC) is of great importance in the balance of the most common cations in soil (Na+, K+, NH4+, Ca2+, Mg2+) and pH control, with direct impact on the soil fertility. It also plays an important role in the fate of most Ca2+ arriving from land (river water) into the seas.\nThe ability to change and control the CEC of clay minerals offers a valuable tool in the development of selective adsorbents with applications as varied as chemical sensors or pollution cleaning substances for contaminated water, for example.\nThe understanding of the reactions of clay minerals with water (intercalation, adsorption, colloidal dispersion, etc.) are indispensable for the ceramic industry (plasticity and flow control of ceramic raw mixtures, for example). Those interactions also influence a great number of mechanical properties of soils, being carefully studied by building and construction engineering specialists.\nThe interactions of clay minerals with organic substances in the soil also plays a vital role in the fixation of nutrients and fertility, as well as in the fixation or leaching of pesticides and other contaminants. Some clay minerals (kaolinite) are used as carrier material for fungicides and insecticides.\nThe weathering of many rock types produce clay minerals as one of its last products. The understanding of these geochemical processes is also important for the understanding of geological evolution of landscapes and macroscopic properties of rocks and sediments. Presence of clay minerals in Mars, detected by the Mars Reconnaissance Orbiter in 2009 was another strong evidence of the existence of water on the planet in",
    "source": "wikipedia",
    "title": "Clay chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_37605500",
    "text": "The colloidal probe technique is commonly used to measure interaction forces acting between colloidal particles and/or planar surfaces in air or in solution. This technique relies on the use of an atomic force microscope (AFM). However, instead of a cantilever with a sharp AFM tip, one uses the colloidal probe. The colloidal probe consists of a colloidal particle of few micrometers in diameter that is attached to an AFM cantilever. The colloidal probe technique can be used in the sphere-plane or sphere-sphere geometries (see figure). One typically achieves a force resolution between 1 and 100 pN and a distance resolution between 0.5 and 2 nm.\nThe colloidal probe technique has been developed in 1991 independently by Ducker and Butt. Since its development this tool has gained wide popularity in numerous research laboratories, and numerous reviews are available in the scientific literature.\nAlternative techniques to measure force between surfaces involve the surface forces apparatus, total internal reflection microscopy, and optical tweezers techniques to with video microscopy.\n\nPurpose\nThe possibility to measure forces involving particles and surfaces directly is essential since such forces are relevant in a variety of processes involving colloidal and polymeric systems. Examples include particle aggregation, suspension rheology, particle deposition, and adhesion processes. One can equally study similar biological phenomena, such as deposition of bacteria or the infection of cells by viruses. Forces are equally most informative to investigate the mechanical properties of interfaces, bubbles, capsules, membranes, or cell walls. Such measurements permit to make conclusions about the elastic or plastic deformation or eventual rupture in such systems.\nThe colloidal probe technique provides a versatile tool to measure such forces between a colloidal particle and a planar substrate or between two colloidal particles (see figure above). The particles used in such experiments have typically a diameter between 1–10 μm. Typical applications involve measurements of electrical double layer forces and the corresponding surface potentials or surface charge, van der Waals forces, or forces induced by adsorbed polymers.\n",
    "source": "wikipedia",
    "title": "Colloidal probe technique",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_69164293",
    "text": "Compliance constants are the elements of an inverted Hessian matrix. The calculation of compliance constants provides an alternative description of chemical bonds in comparison with the widely used force constants explicitly ruling out the dependency on the coordinate system. They provide the unique description of the mechanical strength for covalent and non-covalent bonding. While force constants (as energy second derivatives) are usually given in aJ/Å2 or N/cm, compliance constants are given in Å2/aJ or Å / mdyn.\n\nHistory\nHitherto, recent publications that broke the wall of putative chemical understanding and presented detection/isolation of novel compounds with intriguing bonding characters can still be provocative at times. The stir in such discoveries arose partly from the lack of a universally accepted bond descriptor. While bond dissociation energies (BDE) and rigid force constants have been generally regarded as primary tools for such interpretation, they are prone to flawed definition of chemical bonds in certain scenarios whether simple or controversial.\nSuch reasons prompted the necessity to seek an alternative approach to describe covalent and non-covalent interactions more rigorously. Jörg Grunenberg, a German chemist at the TU Braunschweig and his Ph.D. student at the time, Kai Brandhorst, developed a program COMPLIANCE (freely available to the public), which harnesses compliance constants for tackling the aforementioned tasks. The authors use an inverted matrix of force constants, i.e., inverted Hessian matrix, originally introduced by W. T. Taylor and K. S. Pitzer. The insight in choosing the inverted matrix is from the realization that not all elements in the Hessian matrix are necessary—and thus redundant—for describing covalent and non-covalent interactions. Such redundancy is common for many molecules, and more importantly, it ushers in the dependence of the elements of the Hessian matrix on the choice of coordinate system. Therefore, the author claimed that force constants albeit more widely used are not an appropriate bond descriptor whereas non-redundant and coordinate system-independent compliance constants are.\n",
    "source": "wikipedia",
    "title": "Compliance constants",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_52625201",
    "text": "Compound Interest is a website launched in 2013 by Andy Brunning with infographics on everyday chemistry. The infographics describe, for example, how chemicals found in food and nature give them smell, taste, and color. The website has a monthly collaboration with the American Chemical Society. Content of the website is used as information source by various newspapers and media, including the Washington Post, Time, The Conversation, and Forbes.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Compound Interest (website)",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_38403118",
    "text": "In chemistry, congeners are chemical substances \"related to each other by origin, structure, or function\".\n\nCommon origin and structure\nAny significant quantity of a polyhalogenated compound is by default a blend of multiple molecule types because each molecule forms independently, and chlorine and bromine do not strongly select which site(s) they bond to.\n\nPolychlorinated biphenyls (PCBs) are a family of 209 congeners.\nPolybrominated biphenyls and polychlorinated diphenyl ethers are also families of 209 congeners.\nSimilarly polychlorinated dibenzodioxins, polychlorinated dibenzofurans, polychlorinated terphenyls, polychlorinated naphthalene, polychloro phenoxy phenol, and polybrominated diphenyl ethers (PBDEs) (pentabromodiphenyl ether, octabromodiphenyl ether, decabromodiphenyl ether), etc. are also groups of congeners.\nCommon origin\nCongener (alcohol), substances other than alcohol (desirable or undesirable) also produced during fermentation.\nCongeners of oleic acids can modify cell membrane behavior, protecting against tumors or having effects on blood pressure.\nCommon structure\nCongeners can refer to similar compounds that substitute other elements with similar valences, yielding molecules having similar structures. Examples:\n\npotassium chloride and sodium chloride may be considered congeners; also potassium chloride and potassium fluoride.\nhydrogen peroxide (HOOH), hydrogen thioperoxide (HSOH), and hydrogen disulfide (HSSH).\nStructural analogs are often isoelectronic.\nOther\nCongeners refer to the various oxidation states of a given element in a compound. For example, titanium(II) chloride (titanium dichloride), titanium(III) chloride (titanium trichloride), and titanium(IV) chloride (titanium tetrachloride) may be considered congeners.\nCongeners can refer to other elements in the same group in the periodic table. For example, congeners of the Group 11 element copper are silver and gold, sometimes found together in the same ores (porphyry copper deposit) due to their chemical similarity.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Congener (chemistry)",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_70333668",
    "text": "Cononsolvency is a phenomenon where two solvents that can typically readily dissolve a polymer, when mixed, at certain ratios of these two solvents, are no longer able to dissolve the polymer. This phenomenon is in contrast to cosolvency where two solvents that are both poor at dissolving a material, but when the two poor solvents admixed, can form a mixed solvent capable of dissolving the material.\nThe first works of both experimental and theoretical about the cononsolvency effect were published in the late 1970s. Since then, numerous studies focused on a manifold of different polymers that featured the cononsolvency effect in water and various organic cosolvents such as methanol, ethanol, and acetone. Typically poly(acrylamide)s such as poly(N-isopropylacrylamide) show the cononsolvency effect, while this effect is also known for other homopolymers and for more complex systems e.g., diblock copolymer, polyelectrolytes, crosslinked microgels, micelles, and grafted polymer brushes. Recently, it was also shown that thermo-responsive thin films exhibit the cononsolvency effect in a mixed solvent vapor phase, which can be explained by a decreased volume phase transition temperature, the thin-film analogy of a lower critical solution temperature. These experimental studies are supported by a growing number of simulation studies.\nAfter 45 years of research, the origin of the molecular mechanism behind the cononsolvency effect in a mixture of solvents remains not fully resolved yet. To date, researchers have considered various interactions between polymer and solvent/cosolvent as possible factors leading to the cononsolvency effect, such as competitive hydrogen bonding of the solvent and cosolvent with the polymer, hydrophobic hydration of particular functional groups of the polymer,  cosolvent induced geometric frustration, excluded-volume interactions due to the surfactant-like behavior of amphiphilic cosolvents, as well as the three body effects, i.e., temporary bridging of one or more individual polymer chains by the cosolvent.\nIn literature, cononsolvency was reported almost exclusively for polymers in aqueous solution. This, however, does not mean that cononsolvency cannot happen in non-aqueous solutions. For example, poly(methyl methacrylate) shows the cononsolvency effect in the binary mixtures of two organic solvents (chlorobutane and amyl acetate).  \n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Cononsolvency",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_33177861",
    "text": "Core–shell semiconducting nanocrystals (CSSNCs) are a class of materials which have properties intermediate between those of small, individual molecules and those of bulk, crystalline semiconductors. They are unique because of their easily modular properties, which are a result of their size. These nanocrystals are composed of a quantum dot semiconducting core material and a shell of a distinct semiconducting material.  The core and the shell are typically composed of type II–VI, IV–VI, I-III-VI, and III–V semiconductors, with configurations such as CdS/ZnS, CdSe/ZnS, CuInZnSe/ZnS, CdSe/CdS, and InAs/CdSe (typical notation is: core/shell) Organically passivated quantum dots have low fluorescence quantum yield due to surface related trap states. CSSNCs address this problem because the shell increases quantum yield by passivating the surface trap states.  In addition, the shell provides protection against environmental changes, photo-oxidative degradation, and provides another route for modularity. Precise control of the size, shape, and composition of both the core and the shell enable the emission wavelength to be tuned over a wider range of wavelengths than with either individual semiconductor.  These materials have found applications in biological systems and optics.\n\nBackground\nColloidal semiconductor nanocrystals, which are also called quantum dots (QDs), consist of ~1–10 nm diameter semiconductor nanoparticles that have organic ligands bound to their surface.  These nanomaterials have found applications in nanoscale photonic, photovoltaic, and light-emitting diode (LED) devices due to their size-dependent optical and electronic properties.  Quantum dots are popular alternatives to organic dyes as fluorescent labels for biological imaging and sensing due to their small size, tuneable emission, and photostability.\nThe luminescent properties of quantum dots arise from exciton decay (recombination of electron hole pairs) which can proceed through a radiative or nonradiative pathway.  The radiative pathway involves electrons relaxing from the conduction band to the valence band by emitting photons with wavelengths corresponding to the semiconductor's bandgap. Nonradiative recombination can occur through energy release via phonon emission or Auger recombination. In this size regime, quantum confinement effects lead to a size dependent increasing bandgap with observable, quantized energy levels.  The quantized energy levels observed in quantum dots lead to electronic structures that are intermediate between single molecules which have a single HOMO-LUMO gap and bulk semiconductors which have continuous energy levels within bands \n\nSemiconductor nanocrystals generally adopt the same crystal structure as their extended solids. At the surface of the crystal, the periodicity abruptly stops, resulting in surface atoms having a lower coordination number than the interior atoms. This incomplete bonding (relative to the interior crystal structure) results ",
    "source": "wikipedia",
    "title": "Core–shell semiconductor nanocrystal",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_74173478",
    "text": "Corrosion inhibitors are substances used in the oil industry to protect equipment and pipes against corrosion. Corrosion is a common problem in the oil industry due to the presence of water, gases, and other corrosive contaminants in the production environment.\nAnodic inhibitors and cathodic inhibitors are the two main categories of corrosion inhibitors. While cathodic inhibitors act as catalysts to slow down corrosion, anodic inhibitors protect metal surfaces by acting as physical barriers. They can also be divided into organic and inorganic corrosion inhibitors based on their chemical composition.\nCorrosion inhibitors are used in the petroleum industry in several steps, including drilling, production, transportation, and storage of oil and gas. They can mitigate different types of corrosion in the petroleum industry, such as generalized corrosion, pitting corrosion, erosion corrosion, stress corrosion, galvanic corrosion, cavitation corrosion, and hydrogen blister.\n\nCorrosion Inhibitor Families\nThere are different chemical families of corrosion inhibitors used in the oil industry, among them are the following:\nFatty Imidazolines: These are imidazole-based compounds, usually with a long unsaturated chain length, derived mainly from oleic acid. They are very effective in preventing acid corrosion of carbon steel (Figure 1).\n\nFatty amines: These corrosion inhibitors are organic compounds that contain an amino group and an alkyl group. They act as cathodic inhibitors and form a protective layer on the metal surface.They work efficiently against corrosion brought about by carbon dioxide (CO2) and hydrogen sulfide (H2S). Also, ethoxylated amines are widely applied for the same purpose (Figure 2).\n\nOrganic Acids: Organic acids such as acetic acid, formic acid and citric acid are used as corrosion inhibitors. These acids react with metal ions to form insoluble compounds that protect the metal surface. These inhibitors are often used in combination with other corrosion inhibitors and techniques, such as cathodic protection and coatings, to provide comprehensive corrosion protection. CO2 and H2S are regularly seen in oilfields and are notorious for causing corrosion of metal sections. Fortunately, they can be kept under control with measures that have been found to be effective (Figure 3).\n\nPyridines: Some studies have shown that certain pyridines can inhibit corrosion caused by the presence of acid gases, such as carbon dioxide and hydrogen sulfide, which are common in the oil industry. Pyridine and its derivatives have been shown to be effective inhibitors for a wide range of metals, such as carbon steel, stainless steel, and copper alloys. They act by adsorbing to the metal surface and forming a protective film, which can be physical or chemical in nature. Pyridine and its derivatives are also effective in inhibiting localized corrosion, such as pitting and crevice corrosion (Figure 4).\n\nAzoles: Azoles, such as triazole and benzotriazole, oxazole and ",
    "source": "wikipedia",
    "title": "Corrosion inhibitors for the petroleum industry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_40913419",
    "text": "In chemistry, a crossover experiment is a method used to study the mechanism of a chemical reaction. In a crossover experiment, two similar but distinguishable reactants simultaneously undergo a reaction as part of the same reaction mixture. The products formed will either correspond directly to one of the two reactants (non-crossover products) or will include components of both reactants (crossover products). The aim of a crossover experiment is to determine whether or not a reaction process involves a stage where the components of each reactant have an opportunity to exchange with each other.\nThe results of crossover experiments are often straightforward to analyze, making them one of the most useful and most frequently applied methods of mechanistic study. In organic chemistry, crossover experiments are most often used to distinguish between intramolecular and intermolecular reactions.\nInorganic and organometallic chemists rely heavily on crossover experiments, and in particular isotopic labeling experiments, for support or contradiction of proposed mechanisms.  When the mechanism being investigated is more complicated than an intra- or intermolecular substitution or rearrangement, crossover experiment design can itself become a challenging question. A well-designed crossover experiment can lead to conclusions about a mechanism that would otherwise be impossible to make. Many mechanistic studies include both crossover experiments and measurements of rate and kinetic isotope effects.\n\nPurpose\nCrossover experiments allow for experimental study of a reaction mechanism. Mechanistic studies are of interest to theoretical and experimental chemists for a variety of reasons including prediction of stereochemical outcomes, optimization of reaction conditions for rate and selectivity, and design of improved catalysts for better turnover number, robustness, etc. Since a mechanism cannot be directly observed or determined solely based on the reactants or products, mechanisms are challenging to study experimentally. Only a handful of experimental methods are capable of providing information about the mechanism of a reaction, including crossover experiments, studies of the kinetic isotope effect, and rate variations by substituent. The crossover experiment has the advantage of being conceptually straightforward and relatively easy to design, carry out, and interpret. In modern mechanistic studies, crossover experiments and KIE studies are commonly used in conjunction with computational methods.\n",
    "source": "wikipedia",
    "title": "Crossover experiment (chemistry)",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_18388283",
    "text": "Crystal chemistry is the study of the principles of chemistry behind crystals and their use in describing structure-property relations in solids, as well as the chemical properties of periodic structures. The principles that govern the assembly of crystal and glass structures are described, models of many of the technologically important crystal structures (alumina, quartz, perovskite) are studied, and the effect of crystal structure on the various fundamental mechanisms responsible for many physical properties are discussed.\nThe objectives of the field include:\n\nidentifying important raw materials and minerals as well as their names and chemical formulae.\ndescribing the crystal structure of important materials and determining their atomic details\nlearning the systematics of crystal and glass chemistry.\nunderstanding how physical and chemical properties are related to crystal structure and microstructure.\nstudying the engineering significance of these ideas and how they relate to foreign products: past, present, and future.\nTopics studied are:\n\nChemical bonding, Electronegativity\nFundamentals of crystallography: crystal systems, Miller Indices, symmetry elements, bond lengths and radii, theoretical density\nCrystal and glass structure prediction: Pauling's and Zachariasen’s rules\nPhase diagrams and crystal chemistry (including solid solutions)\nImperfections (including defect chemistry and line defects)\nPhase transitions\nStructure – property relations: Neumann's law, melting point, mechanical properties (hardness, slip, cleavage, elastic moduli), wetting, thermal properties (thermal expansion, specific heat, thermal conductivity), diffusion, ionic conductivity, refractive index, absorption, color, Dielectrics and Ferroelectrics, and Magnetism\nCrystal structures of representative metals, semiconductors, polymers, and ceramics\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Crystal chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_7794",
    "text": "Crystallography is the branch of science devoted to the study of molecular and crystalline structure and properties. The word crystallography is derived from the Ancient Greek word κρύσταλλος (krústallos; \"clear ice, rock-crystal\"), and γράφειν (gráphein; \"to write\"). In July 2012, the United Nations recognised the importance of the science of crystallography by proclaiming 2014 the International Year of Crystallography.\nCrystallography is a broad topic, and many of its subareas, such as X-ray crystallography, are themselves important scientific topics. Crystallography ranges from the fundamentals of crystal structure to the mathematics of crystal geometry, including those that are not periodic or quasicrystals. At the atomic scale it can involve the use of X-ray diffraction to produce experimental data that the tools of X-ray crystallography can convert into detailed positions of atoms, and sometimes electron density. At larger scales it includes experimental tools such as orientational imaging to examine the relative orientations at the grain boundary in materials. Crystallography plays a key role in many areas of biology, chemistry, and physics, as well as in emerging developments in these fields.\n\nHistory and timeline\nBefore the 20th century, the study of crystals was based on physical measurements of their geometry using a goniometer. This involved measuring the angles of crystal faces relative to each other and to theoretical reference axes (crystallographic axes), and establishing the symmetry of the crystal in question. The position in 3D space of each crystal face is plotted on a stereographic net such as a Wulff net or Lambert net. The pole to each face is plotted on the net. Each point is labelled with its Miller index. The final plot allows the symmetry of the crystal to be established.\nThe discovery of X-rays and electrons in the last decade of the 19th century enabled the determination of crystal structures on the atomic scale, which brought about the modern era of crystallography. The first X-ray diffraction experiment was conducted in 1912 by Max von Laue, while electron diffraction was first realized in 1927 in the Davisson–Germer experiment and parallel work by George Paget Thomson and Alexander Reid. These developed into the two main branches of crystallography, X-ray crystallography and electron diffraction. The quality and throughput of solving crystal structures greatly improved in the second half of the 20th century, with the developments of customized instruments and phasing algorithms. Nowadays, crystallography is an interdisciplinary field, supporting theoretical and experimental discoveries in various domains. Modern-day scientific instruments for crystallography vary from laboratory-sized equipment, such as diffractometers and electron microscopes, to dedicated large facilities, such as photoinjectors, synchrotron light sources and free-electron lasers.\n",
    "source": "wikipedia",
    "title": "Crystallography",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_68615793",
    "text": "Cyclosiloxanes are a class of silicone material. They are volatile and often used as a solvent. The three main commercial varies are octamethylcyclotetrasiloxane (D4), decamethylcyclopentasiloxane (D5) and dodecamethylcyclohexasiloxane (D6). They evaporate and degrade in air under sunlight.\n\nOctamethylcyclotetrasiloxane (D4)\nThe octamethylcyclotetrasiloxane silicone liquid has no odor and consists of four repeating units of silicon (Si) and oxygen (O) atoms in a closed loop giving it a circular structure. Each silicon atom has two methyl groups attached (CH3).\nDecamethylcyclopentasiloxane (D5)\nDecamethylcyclopentasiloxane silicone liquid has no odor and consists of five repeating units of silicon (Si) and oxygen (O) atoms in a closed loop giving it a circular structure. Each silicon atom has two methyl groups attached (CH3). Typically it is used as an ingredient in antiperspirant, skin cream, sun protection lotion and make-up. With a low surface tension of 18 mN/m this material has good spreading properties.\nDodecamethylcyclohexasiloxane (D6)\nThe dodecamethylcyclohexasiloxane silicone liquid has no odor and consists of six repeating units of silicon (Si) and oxygen (O) atoms in a closed loop giving it a circular structure. Each silicon atom has two methyl groups attached (CH3).\nCASRN: 540-97-6. D6 is also contained under the CAS No. (69430-24-6 ) which is associated with the names cyclopolydimethylsiloxane, cyclopolydimethylsiloxane (DX), cyclosiloxanes di-Me, dimethylcyclopolysiloxane, polydimethyl siloxy cyclics, polydimethylcyclosiloxane, cyclomethicone and mixed cyclosiloxane.\nSee also\nPolydimethylsiloxane\nCyclomethicone\nSiloxane and other organosilicon compounds\nLiterature\nCyclosiloxanes (pdf-file), Materials for the December 4-5, 2008 Meeting of the California Environmental Contaminant Biomonitoring Program (CECBP) Scientific Guidance Panel (SGP)\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Cyclosiloxane",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_77419359",
    "text": "Dark oxygen production refers to the generation of molecular oxygen (O2) through processes that do not involve light-dependent oxygenic photosynthesis. The name therefore uses a different sense of 'dark' than that used in the phrase \"biological dark matter\" (for example) which indicates obscurity to scientific assessment rather than the photometric meaning. While the majority of Earth's oxygen is produced by plants and photosynthetically active microorganisms via photosynthesis, dark oxygen production occurs via a variety of abiotic and biotic processes and may support aerobic metabolism in dark, anoxic environments.\nThe metallic nodule theory for dark oxygen production in particular is controversial, with scientists disagreeing about their validity.\n\nAbiotic production\nAbiotic production of dark oxygen can occur through several mechanisms, such as:\n\nWater radiolysis: This process typically takes place in dark geological ecosystems, such as aquifers, where the decay of radioactive elements in surrounding rock leads to the breakdown of water molecules, producing O2.\nOxidation of surface-bound radicals: On silicon-bearing minerals like quartz, surface-bound radicals can undergo oxidation, contributing to O2 production.\nIn addition to direct O2 formation, these processes often produce reactive oxygen species (ROS), such as hydroxyl radicals (OH•), superoxide (O2•-), and hydrogen peroxide (H2O2). These ROS can be converted into O2 and water either biotically, through enzymes like superoxide dismutase and catalase, or abiotically, via reactions with ferrous iron and other reduced metals.\nBiotic production\nBiotic production of dark oxygen is performed by microorganisms through distinct microbial processes, including:\n\nChlorite dismutation: This involves the dismutation of chlorite (ClO2−) into O2 and chloride ions.\nNitric oxide dismutation: This involves the dismutation of nitric oxide (NO) into O2 and dinitrogen gas (N2) or nitrous oxide (N2O).\nWater lysis via methanobactins: Methanobactins can lyse water molecules to produce O2.\nThese processes enable microbial communities to sustain aerobic metabolism in environments that lack oxygen.\n",
    "source": "wikipedia",
    "title": "Dark oxygen",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_60575040",
    "text": "In chemistry, the decay technique is a method to generate chemical species such as radicals, carbocations, and other potentially unstable covalent structures by radioactive decay of other compounds. For example, decay of a tritium-labeled molecule yields an ionized helium atom, which might then break off to leave a cationic molecular fragment.\nThe technique was developed in 1963 by the Italian chemist Fulvio Cacace at the University of Rome.  It has allowed the study of a vast number of otherwise inaccessible compounds and reactions.  It has also provided much of our current knowledge about the chemistry of the helium hydride ion [HeH]+.\n\nCarbocation generation\nIn the basic method, a molecule (R,R′,R″)C−T is prepared where the vacant bond of the desired radical or ion is satisfied by an atom of tritium 3H, the radioactive isotope of hydrogen with mass number 3.  As the tritium undergoes beta decay (with a half-life of 12.32 years), it is transformed into an ion of helium-3, creating the cation (R,R′,R″)C−[3He]+.\nIn the decay, an electron and an antineutrino are ejected at great speed from the tritium nucleus, changing one of the neutrons into a proton with the release of 18,600 electronvolts (eV) of energy.  The neutrino escapes the system; the electron is generally captured within a short distance, but far enough away from the site of the decay that it can be considered lost from the molecule.  Those two particles carry away most of the released energy, but their departure causes the nucleus to recoil, with about 1.6 eV of energy. This recoil energy is larger than the bond strength of the carbon–helium bond (about 1 eV), so this bond breaks.  The helium atom almost always leaves as a neutral 3He, leaving behind the carbocation [(R,R′,R″)C]+.\nThese events happen very quickly compared to typical molecular relaxation times, so the carbocation is usually created in the same conformation and electronic configuration as the original neutral molecule.  For example, decay of tritiated methane, CH3T (R = R′ = R″ = H) produces the carbenium ion H3C+ in a tetrahedral conformation, with one of the orbitals having a single unpaired electron and the other three forming a trigonal pyramid.  The ion then relaxes to its more favorable trigonal planar form, with release of about 30 kcal/mol of energy—that goes into vibrations and rotation of the ion.\nThe carbocation then can interact with surrounding molecules in many reactions that cannot be achieved by other means.  When formed within a rarefied gas, the carbocation and its reactions can be studied by mass spectrometry techniques.  However the technique can be used also in condensed matter (liquids and solids).  In liquid phase, the carbocation is initially formed in the same solvation state as the parent molecule, and some reactions may happen before the solvent shells around it have time to rearrange.  In a crystalline solid, the cation is formed in the same crystalline site; and the nature, position, and ori",
    "source": "wikipedia",
    "title": "Decay technique",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_48623453",
    "text": "DePriester Charts provide an efficient method to find the vapor-liquid equilibrium ratios for different substances at different conditions of pressure and temperature. The original chart was put forth by C.L. DePriester in an article in Chemical Engineering Progress in 1953. These nomograms have two vertical coordinates, one for pressure, and another for temperature. \"K\" values, representing the tendency of a given chemical species to partition itself preferentially between liquid and vapor phases, are plotted in between. Many DePriester charts have been printed for simple hydrocarbons.\n\nExample\nFor example, to find the K value of methane at 100 psia and 60 °F.\n\nOn the left-hand vertical axis, locate and mark the point containing the pressure 100 psia.\nOn the right-hand vertical axis, locate and mark the point containing the temperature 60°F.\nConnect the points with a straight line.\nNote where the line crosses the methane axis. Read this K-value off the chart (approximately 21.3).\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "DePriester chart",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_74926555",
    "text": "In the iron and steel industry, direct reduction is a set of processes for obtaining iron from iron ore, by reducing iron oxides without melting the metal. The resulting product is pre-reduced iron ore.\nHistorically, direct reduction was used to obtain a mix of iron and slag called a bloom in a bloomery. At the beginning of the 20th century, this process was abandoned in favor of the blast furnace, which produces iron in two stages (reduction-melting to produce cast iron, followed by refining in a converter).\nHowever, various processes were developed in the course of the 20th century and, since the 1970s, the production of pre-reduced iron ore has undergone remarkable industrial development, notably with the rise of the Midrex process. Designed to replace the blast furnace, these processes have so far only proved profitable in certain economic contexts, which still limits this sector to less than 5% of world steel production.\n\nHistory\n\nChemical reactions\n\nProcedures\nPlants for the production of pre-reduced iron ore are known as direct reduction plants. The principle involves exposing iron ore to the reducing action of a high-temperature gas (around 1000 °C). This gas is composed of carbon monoxide and dihydrogen, the proportions of which depend on the production process.\nGenerally speaking, there are two main types of processes:\n\nprocesses where the reducing gas is obtained from natural gas. In this case, the ore is reduced in tanks;\nprocesses where the reducing gas is obtained from coal. The reactor is generally an inclined rotary kiln, similar to those used in cement plants, in which coal is mixed with limestone and ore, then heated.\nAnother way of classifying processes is to distinguish between those where the reducing gases are produced in specific facilities separate from the reduction reactor - which characterizes most processes using natural gas - and those where the gases are produced inside the fusion reactor: coal-fired processes generally fall into this category. However, many \"gas-fired\" processes can be fed by gasification units producing a reducing gas from coal.\nIn addition, since the melting stage is necessary to obtain alloys, reduction-melting processes have been developed which, like blast furnaces, produce a more or less carburized liquid metal. Finally, many more or less experimental processes have been developed.\n",
    "source": "wikipedia",
    "title": "Direct reduction",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_37732235",
    "text": "Double layer forces occur between charged objects across liquids, typically water. This force acts over distances that are comparable to the Debye length, which is on the order of one to a few tenths of nanometers. The strength of these forces increases with the magnitude of the surface charge density (or the electrical surface potential). For two similarly charged objects, this force is repulsive and decays exponentially at larger distances, see figure. For unequally charged objects and eventually at shorted distances, these forces may also be attractive. The theory due to Derjaguin, Landau, Verwey, and Overbeek (DLVO) combines such double layer forces together with Van der Waals forces in order to estimate the actual interaction potential between colloidal particles.\nAn electrical double layer develops near charged surfaces (or another charged objects) in aqueous solutions. Within this double layer, the first layer corresponds to the charged surface. These charges may originate from tightly adsorbed ions, dissociated surface groups, or substituted ions within the crystal lattice. The second layer corresponds to the diffuse layer, which contains the neutralizing charge consisting of accumulated counterions and depleted coions. The resulting potential profile between these two objects leads to differences in the ionic concentrations within the gap between these objects with respect to the bulk solution. These differences generate an osmotic pressure, which generates a force between these objects.\nThese forces are easily experienced when hands are washed with soap. Adsorbing soap molecules make the skin negatively charged, and the slippery feeling is caused by the strongly repulsive double layer forces. These forces are further relevant in many colloidal or biological systems, and may be responsible for their stability, formation of colloidal crystals, or their rheological properties.\n\nPoisson–Boltzmann model\nThe most popular model to describe the electrical double layer is the Poisson-Boltzmann (PB) model. This model can be equally used to evaluate double layer forces. Let us discuss this model in the case of planar geometry as shown in the figure on the right. In this case, the electrical potential profile ψ(z) near a charged interface will only depend on the position z. The corresponding Poisson's equation reads in SI units\n\n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              ψ\n            \n            \n              d\n              \n                z\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        −\n        \n          \n            ρ\n            \n              \n                ϵ\n                \n                  0\n                \n              \n              ϵ\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {d^{2}\\psi }{dz^{2}}}=-{\\frac {\\",
    "source": "wikipedia",
    "title": "Double layer forces",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_59891440",
    "text": "Made up of primary carbon, carbon black is spherical in shape and arranged into aggregates and agglomerates. It differs from other carbon forms (diamond, graphite, coke) in its complex configuration, colloid dimensions and quasi-graphitic structure. Carbon black's purity and composition are practically free of inorganic pollutants and extractable organic substances.\nA distinction is made between these two terms:\n\nCarbon black – a specially produced type of carbon using the process of incomplete combustion with restricted oxygen access. The article addresses this type of carbon.\nSoot – auxiliary fuel (coal, hydrocarbons, crude oil) combustion product, which is considered to be a hazardous substance with carcinogenic properties.\nCarbon black can be characterized as a substance with over 97% amorphous carbon content. It is used extensively in many areas of industrial chemistry. It is often used in the plastic and rubber manufacturing industries, where it improves electrical conductivity and electromagnetic or thermo-conductive characteristics of plastic materials and rubbers. By virtue of its pigmentation capabilities, it is also used for the production of special printing inks, paints and varnishes. Thanks to its advanced porous structure, it is also used as a catalyst carrier, and its notable sorption attributes are used for, in example, catching gaseous pollutants at waste incinerator plants.\nCarbon black predominantly includes a conductive type of carbon, which combines an extremely high specific surface and extensively developed structure – microporosity. At the same time, it consists of primary carbon particles and boasts a high degree of aggregation. Carbon black's grouping facilitates the formation of a conductive structure in plastics, rubbers and other composites. These characteristics predetermine electroconductive carbon black's primary area of application, i.e. electrical conductivity modification of nearly all types of plastic materials by adding a relatively low volume of carbon black. Such modifications can be used for numerous purposes, from establishing antistatic properties to adjusting polymer conductivity. Another valuable property of electroconductive carbon black is its excellent ability to absorb UV radiation on the visible spectrum, i.e. as a UV stabilizer for plastic materials, pigment in printer inks, paints and varnishes, or for coloring plastics, rubbers and sealants.\n\nProduction\nCarbon black begins as a byproduct of what is referred to as partial oxidation, a process during which crude oil residues, such as vacuum residues from crude oil distillation or residues from the thermic cracking process, split due to the effects of the mixture of oxygen and water steam under high temperatures around 1,300 °C.\nPartial oxidation of various raw materials always creates a gaseous mixture containing CO, CO2, H2O, H2, CH4 and H2S and COS formed from sulfurous compounds. Carbon black is formed as an undesired byproduct. The amount of ",
    "source": "wikipedia",
    "title": "Electroconductive carbon black",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_8339883",
    "text": "Electrolysed water (also electrolyzed water, EOW, electrolyzed oxidizing water, electro-activated water, super-oxidized solution or electro-chemically activated water solution) is produced by the electrolysis of water containing dissolved sodium chloride. The electrolysis of salt solutions produces a solution of hypochlorous acid and sodium hydroxide. The hypochlorous acid and sodium hydroxide (essentially, bleach) generated by electrolysis can be used as a disinfectant, if the solution is used immediately before the solution degrades.\n\nCreation\nThe electrolysis occurs in a vessel with separation of the cathodic and anodic solutions. \nAt the cathode, hydrogen gas and hydroxide ions are produced, leading to an alkaline solution that consists essentially of sodium hydroxide. \nAt the anode, chloride ions can be oxidized to elemental chlorine, which is present in acidic solution and can be corrosive to metals. If the solution near the anode is acidic then it will contain elemental chlorine.  \nThe key to delivering a powerful sanitizing agent is to form hypochlorous acid without elemental chlorine. This occurs at around neutral pH .  Hypochlorous is a weak acid and an oxidizing agent. This \"acidic electrolyzed water\" can be raised in pH by mixing in the desired amount of hydroxide ion solution from the cathode compartment, yielding a solution of Hypochlorous acid (HOCl) and sodium hydroxide (NaOH).  A solution at pH 7.3 will contain equal concentrations of hypochlorous acid and hypochlorite ion; reducing the pH will shift the balance toward the hypochlorous acid.  At a pH between 5.5 and 6.0 approximately 90% of the ions are in the form of hypochlorous acid.\nProposed use as a disinfectant\nBoth sodium hydroxide and hypochlorous acid can be disinfecting agents; The key to effective sanitation is to have a high proportion of hypochlorous acid present, this happens between acidic and neutral pH conditions. \nUnder some controlled circumstances, EOW can kill bacteria and inactivate viruses. Freshly made EOW (used within 2 minutes of creation) was shown to achieve a 5-log reduction in pathogens.\nThe disinfectant claims of EOW are based on a formulation containing a mixed oxidant with a  corrosive pH of 2.53.\n",
    "source": "wikipedia",
    "title": "Electrolysed water",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_70995834",
    "text": "In chemistry and particularly biochemistry, an energy-rich species (usually energy-rich molecule) or high-energy species (usually high-energy molecule) is a chemical species which reacts, potentially with other species found in the environment, to release chemical energy.\nIn particular, the term is often used for:\n\nadenosine triphosphate (ATP) and similar molecules called high-energy phosphates, which release inorganic phosphate into the environment in an exothermic reaction with water:\nATP + H2O → ADP + Pi   ΔG°' = −30.5 kJ/mol (−7.3 kcal/mol)\nfuels such as hydrocarbons, carbohydrates, lipids, proteins, and other organic molecules which react with oxygen in the environment to ultimately form carbon dioxide, water, and sometimes nitrogen, sulfates, and phosphates\nmolecular hydrogen\nmonatomic oxygen, ozone, hydrogen peroxide, singlet oxygen and other metastable or unstable species which spontaneously react without further reactants\nin particular, the vast majority of free radicals\nexplosives such as nitroglycerin and other substances which react exothermically without requiring a second reactant\nmetals or metal ions which can be oxidized to release energy\nThis is contrasted to species that are either part of the environment (this sometimes includes diatomic triplet oxygen) or do not react with the environment (such as many metal oxides or calcium carbonate); those species are not considered energy-rich or high-energy species.\n\nAlternative definitions\nThe term is often used without a definition. Some authors define the term \"high-energy\" to be equivalent to \"chemically unstable\", while others reserve the term for high-energy phosphates, such as the Great Soviet Encyclopedia which defines the term \"high-energy compounds\" to refer exclusively to those.\nThe IUPAC glossary of terms used in ecotoxicology defines a primary producer as an \"organism capable of using the energy derived from light or a chemical substance in order to manufacture energy-rich organic compounds\". However, IUPAC does not formally define the meaning of \"energy-rich\".\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Energy-rich species",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_656979",
    "text": "Environmental chemistry is the scientific study of the chemical and biochemical phenomena that occur in natural places. It should not be confused with green chemistry, which seeks to reduce potential pollution at its source. It can be defined as the study of the sources, reactions, transport, effects, and fates of chemical species in the air, soil, and water environments; and the effect of human activity and biological activity on these. Environmental chemistry is an interdisciplinary science that includes atmospheric, aquatic and soil chemistry, as well as heavily relying on analytical chemistry and being related to environmental and other areas of science.\nEnvironmental chemistry involves first understanding how the uncontaminated environment works, which chemicals in what concentrations are present naturally, and with what effects. Without this it would be impossible to accurately study the effects humans have on the environment through the release of chemicals.\nEnvironmental chemists draw traditional chemical concepts as well as sampling and analytical techniques.\n\nContaminant\nA contaminant is a substance present in nature at a level higher than fixed levels or that would not otherwise be there.   This may be due to human activity and bioactivity. The term contaminant is often used interchangeably with pollutant, which is a substance that detrimentally impacts the surrounding environment.  While a contaminant is sometimes a substance in the environment as a result of human activity, but without harmful effects, it sometimes the case that toxic or harmful effects from contamination only become apparent at a later date.\nThe \"medium\" such as  soil or organism such as  fish affected by the pollutant or contaminant is called a receptor, whilst a sink is a chemical medium or species that retains and interacts with the pollutant such as carbon sink and its effects by microbes.\nEnvironmental indicators\nChemical measures of water quality include dissolved oxygen (DO), chemical oxygen demand (COD), biochemical oxygen demand (BOD), total dissolved solids (TDS), pH, nutrients (nitrates and phosphorus), heavy metals, soil chemicals (including copper, zinc, cadmium, lead and mercury), and pesticides.\n",
    "source": "wikipedia",
    "title": "Environmental chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_1631889",
    "text": "In chemistry, equivalent weight (more precisely, equivalent mass) is the mass of one equivalent, that is the mass of a given substance which will combine with or displace a fixed quantity of another substance. The equivalent weight of an element is the mass which combines with or displaces 1.008 gram of hydrogen or 8.0 grams of oxygen or 35.5 grams of chlorine. The corresponding unit of measurement is sometimes expressed as \"gram equivalent\".\nThe equivalent weight of an element is the mass of a mole of the element divided by the element's valence. That is, in grams, the atomic weight of the element divided by the usual valence.  For example, the equivalent weight of oxygen is 16.0/2 = 8.0 grams.\nFor acid–base reactions, the equivalent weight of an acid or base is the mass which supplies or reacts with one mole of hydrogen cations (H+). For redox reactions, the equivalent weight of each reactant supplies or reacts with one mole of electrons (e−) in a redox reaction.\nEquivalent weight has the units of mass, unlike atomic weight, which is now used as a synonym for relative atomic mass and is dimensionless. Equivalent weights were originally determined by experiment, but (insofar as they are still used) are now derived from molar masses. The equivalent weight of a compound can also be calculated by dividing the molecular mass by the number of positive or negative electrical charges that result from the dissolution of the compound.\n\nIn history\nThe first equivalent weights were published for acids and bases by Carl Friedrich Wenzel in 1777. A larger set of tables was prepared, possibly independently, by Jeremias Benjamin Richter, starting in 1792. However, neither Wenzel nor Richter had a single reference point for their tables, and so had to publish separate tables for each pair of acid and base.\nJohn Dalton's first table of atomic weights (1808) suggested a reference point, at least for the elements: taking the equivalent weight of hydrogen to be one unit of mass. However, Dalton's atomic theory was far from universally accepted in the early 19th century. One of the greatest problems was the reaction of hydrogen with oxygen to produce water. One gram of hydrogen reacts with eight grams of oxygen to produce nine grams of water, so the equivalent weight of oxygen was defined as eight grams. Since Dalton supposed (incorrectly) that a water molecule consisted of one hydrogen and one oxygen atom, this would imply an atomic weight of oxygen equal to eight. However, expressing the reaction in terms of gas volumes following Gay-Lussac's law of combining gas volumes, two volumes of hydrogen react with one volume of oxygen to produce two volumes of water, suggesting (correctly) that the atomic weight of oxygen is sixteen. The work of Charles Frédéric Gerhardt (1816–56), Henri Victor Regnault (1810–78) and Stanislao Cannizzaro (1826–1910) helped to rationalise this and many similar paradoxes, but the problem was still the subject of debate at the Karlsruhe Cong",
    "source": "wikipedia",
    "title": "Equivalent weight",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_37125569",
    "text": "Estimated maximum possible concentration (EMPC) is a term used in dioxin concentration determination for a concentration between limit of quantification and limit of detection.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Estimated maximum possible concentration",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_152969",
    "text": "A eutectic system or eutectic mixture ( yoo-TEK-tik) is a type of a homogeneous mixture that has a melting point lower than those of the constituents. The lowest possible melting point over all of the mixing ratios of the constituents is called the eutectic temperature. On a phase diagram, the eutectic temperature is seen as the eutectic point (see plot).\nNon-eutectic mixture ratios have different melting temperatures for their different constituents, since one component's lattice will melt at a lower temperature than the other's. Conversely, as a non-eutectic mixture cools down, each of its components solidifies into a lattice at a different temperature, until the entire mass is solid.  A non-eutectic mixture thus does not have a single melting/freezing point temperature at which it changes phase, but rather a temperature at which it changes between liquid and slush (known as the liquidus) and a lower temperature at which it changes between slush and solid (the solidus).\nIn the real world, eutectic properties can be used to advantage in such processes as eutectic bonding, where silicon chips are bonded to gold-plated substrates with ultrasound, and eutectic alloys prove valuable in such diverse applications as soldering, brazing, metal casting, electrical protection, fire sprinkler systems, and nontoxic mercury substitutes.\nThe term eutectic was coined in 1884 by the British physicist and chemist Frederick Guthrie (1833–1886). The word originates from Greek  εὐ- (eû) 'well' and  τῆξῐς (têxis) 'melting'. Before his studies, chemists assumed \"that the alloy of minimum fusing point must have its constituents in some simple atomic proportions\", but he showed that that is not always true.\n\nEutectic phase transition\nThe eutectic solidification is defined as follows:\n\n  \n    \n      \n        \n          Liquid\n        \n        \n        \n          \n            →\n            \n              \n                cooling\n              \n            \n            \n              \n                \n                  eutectic\n                \n                \n                  temperature\n                \n              \n            \n          \n        \n        \n        α\n        \n           solid solution\n        \n         \n        +\n         \n        β\n        \n           solid solution\n        \n      \n    \n    {\\displaystyle {\\text{Liquid}}\\quad {\\xrightarrow[{\\text{cooling}}]{{\\text{eutectic}} \\atop {\\text{temperature}}}}\\quad \\alpha {\\text{ solid solution}}\\ +\\ \\beta {\\text{ solid solution}}}\n  \n\nThis type of reaction is an invariant reaction, because it is in thermal equilibrium; another way to define this is the change in Gibbs free energy equals zero. Tangibly, this means the liquid and two solid solutions all coexist at the same time and are in chemical equilibrium. There is also a thermal arrest for the duration of the phase change during which the temperature of the system does not change.\nThe resulting solid macrostructure from a eutectic reaction depends on ",
    "source": "wikipedia",
    "title": "Eutectic system",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_56824574",
    "text": "A field effect is the polarization of a molecule through space. The effect is a result of an electric field produced by charge localization in a molecule. This field, which is substituent and conformation dependent, can influence structure and reactivity by manipulating the location of electron density in bonds and/or the overall molecule. The polarization of a molecule through its bonds is a separate phenomenon known as induction. Field effects are relatively weak, and diminish rapidly with distance, but have still been found to alter molecular properties such as acidity.\n\nField sources\nField effects can arise from the electric dipole field of a bond containing an electronegative atom or electron-withdrawing substituent, as well as from an atom or substituent bearing a formal charge. The directionality of a dipole, and concentration of charge, can both define the shape of a molecule's electric field which will manipulate the localization of electron density toward or away from sites of interest, such as an acidic hydrogen. Field effects are typically associated with the alignment of a dipole field with respect to a reaction center. Since these are through space effects, the 3D structure of a molecule is an important consideration. A field may be interrupted by other bonds or atoms before propagating to a reactive site of interest. Atoms of differing electronegativities can move closer together resulting in bond polarization through space that mimics the inductive effect through bonds. Bicycloheptane and bicyclooctane (seen left) are \npounds in which the change in acidity with substitution was attributed to the field effect. The C-X dipole is oriented away from the carboxylic acid group, and can draw electron density away because the molecule center is empty, with a low dielectric constant, so the electric field is able to propagate with minimal resistance.\nUtility of effect\nA dipole can align to stabilize or destabilize the formation or loss of a charge, thereby decreasing (if stabilized) or increasing (if destabilized) the activation barrier to a chemical event. Field effects can therefore tune the acidity or basicity of bonds within their fields by donating or withdrawing charge density. With respect to acidity, a common trend to note is that, inductively, an electron-withdrawing substituent in the vicinity of an acidic proton will lower the pKa (i.e. increase the acidity) and, correspondingly, an electron-donating substituent will raise the pKa. The reorganization of charge due to field effects will have the same result. An electric dipole field propagated through the space around, or in the middle of, a molecule in the direction of an acidic proton will decrease the acidity, while a dipole pointed away will increase the acidity and  concomitantly elongate the X-H bond. These effects can therefore help to tune the acidity/basicity of a molecule to protonate/deprotonate a specific compound, or enhance hydrogen bond-donor ability for molecular ",
    "source": "wikipedia",
    "title": "Field effect (chemistry)",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_2275867",
    "text": "Forensic chemistry is the application of chemistry and its subfield, forensic toxicology, in a legal setting.  A forensic chemist can assist in the identification of unknown materials found at a crime scene.  Specialists in this field have a wide array of methods and instruments to help identify unknown substances.  These include high-performance liquid chromatography, gas chromatography-mass spectrometry, atomic absorption spectroscopy, Fourier transform infrared spectroscopy, and thin layer chromatography.  The range of different methods is important due to the destructive nature of some instruments and the number of possible unknown substances that can be found at a scene. Forensic chemists prefer using nondestructive methods first, to preserve evidence and to determine which destructive methods will produce the best results.\nAlong with other forensic specialists, forensic chemists commonly testify in court as expert witnesses regarding their findings. Forensic chemists follow a set of standards that have been proposed by various agencies and governing bodies, including the Scientific Working Group on the Analysis of Seized Drugs.  In addition to the standard operating procedures proposed by the group, specific agencies have their own standards regarding the quality assurance and quality control of their results and their instruments.  To ensure the accuracy of what they are reporting, forensic chemists routinely check and verify that their instruments are working correctly and are still able to detect and measure various quantities of different substances.\n\nRole in investigations\nForensic chemists' analysis can provide leads for investigators, and they can confirm or refute their suspicions.  The identification of the various substances found at the scene can tell investigators what to look for during their search.  During fire investigations, forensic chemists can determine if an accelerant such as gasoline or kerosene was used; if so, this suggests that the fire was intentionally set.  Forensic chemists can also narrow down the suspect list to people who would have access to the substance used in a crime.  For example, in explosive investigations, the identification of RDX or C-4 would indicate a military connection as those substances are military grade explosives.  On the other hand, the identification of TNT would create a wider suspect list, since it is used by demolition companies as well as in the military.  During poisoning investigations, the detection of specific poisons can give detectives an idea of what to look for when they are interviewing potential suspects. For example, an investigation that involves ricin would tell investigators to look for ricin's precursors, the seeds of the castor oil plant.\nForensic chemists also help to confirm or refute investigators' suspicions in drug or alcohol cases.  The instruments used by forensic chemists can detect minute quantities, and accurate measurement can be important in crimes such a",
    "source": "wikipedia",
    "title": "Forensic chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_31295654",
    "text": "In chemistry, a free element is a chemical element that is not combined with or chemically bonded to other elements. These may either be chemically inert, or may form bonds with atoms of the same element. \nMetals, non-metals, and noble gases can all be found as free elements. Noble gases such as helium and argon are found in the monoatomic state due to the low reactivity of these atoms. Similarly, noble metals such as gold and platinum are also found in the pure state naturally. Non-metals are rarely found as free elements in the solid state — carbon is a notable exception, as it may be found as diamond and graphite. However, they commonly exist as gases, examples of which include molecular oxygen, ozone, and nitrogen, which together make up approximately 99% of the atmosphere. Because of their reactivity, the halogens do not naturally occur in the free elemental state, but they are both widespread and abundant in the form of their halide ions. They are, however, stable in their diatomic forms.\n\nSee also\nNative metal\nNoble metal\nNative element mineral\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Free element",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_46644932",
    "text": "In coordination chemistry and crystallography, the geometry index or structural parameter (τ) is a number ranging from 0 to 1 that indicates what the geometry of the coordination center is. The first such parameter for 5-coordinate compounds was developed in 1984. Later, parameters for 4-coordinate compounds were developed.\n\n5-coordinate compounds\nTo distinguish whether the geometry of the coordination center is trigonal bipyramidal or square pyramidal, the τ5 (originally just τ) parameter was proposed by Addison et al.:\n\n  \n    \n      \n        \n          τ\n          \n            5\n          \n        \n        =\n        \n          \n            \n              β\n              −\n              α\n            \n            \n              60\n              \n                ∘\n              \n            \n          \n        \n        ≈\n        −\n        0.01667\n        α\n        +\n        0.01667\n        β\n      \n    \n    {\\displaystyle \\tau _{5}={\\frac {\\beta -\\alpha }{60^{\\circ }}}\\approx -0.01667\\alpha +0.01667\\beta }\n  \n\nwhere: β > α are the two greatest valence angles of the coordination center.\nWhen τ5 is close to 0 the geometry is similar to square pyramidal, while if τ5 is close to 1 the geometry is similar to trigonal bipyramidal:\n4-coordinate compounds\nIn 2007 Houser et al. developed the analogous τ4 parameter to distinguish whether the geometry of the coordination center is square planar or tetrahedral. The formula is:\n\n  \n    \n      \n        \n          τ\n          \n            4\n          \n        \n        =\n        \n          \n            \n              \n                360\n                \n                  ∘\n                \n              \n              −\n              (\n              α\n              +\n              β\n              )\n            \n            \n              \n                360\n                \n                  ∘\n                \n              \n              −\n              2\n              θ\n            \n          \n        \n        ≈\n        −\n        0.00709\n        α\n        −\n        0.00709\n        β\n        +\n        2.55\n      \n    \n    {\\displaystyle \\tau _{4}={\\frac {360^{\\circ }-(\\alpha +\\beta )}{360^{\\circ }-2\\theta }}\\approx -0.00709\\alpha -0.00709\\beta +2.55}\n  \n\nwhere: α and β are the two greatest valence angles of coordination center; θ = cos−1(− 1⁄3) ≈ 109.5° is a tetrahedral angle.\nWhen τ4 is close to 0 the geometry is similar to square planar, while if τ4 is close to 1 then the geometry is similar to tetrahedral. However, in contrast to the τ5 parameter, this does not distinguish α and β angles, so structures of significantly different geometries can have similar τ4 values. To overcome this issue, in 2015 Okuniewski et al. developed parameter τ4′ that adopts values similar to τ4 but better differentiates the examined structures:\n\n  \n    \n      \n        \n          τ\n          \n            4\n          \n          ′\n        \n        =\n        \n          \n            \n              β\n              −\n              α\n ",
    "source": "wikipedia",
    "title": "Geometry index",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_28704496",
    "text": "The Gilchrist–Thomas process or Thomas process is a historical process for refining pig iron, derived from the Bessemer converter. It is named after its inventors who patented it in 1877: Percy Carlyle Gilchrist and his cousin Sidney Gilchrist Thomas. By allowing the exploitation of phosphorous iron ore, the most abundant, this process allowed the rapid expansion of the steel industry outside the United Kingdom and the United States.\n\nThe process differs essentially from the Bessemer process in the refractory lining of the converter. The latter, being made of dolomite ((Ca,Mg)(CO3)2) fired with tar, is basic (MgO giving O2− anions), whereas the Bessemer lining, made of packed sand, is acidic (SiO2 accepting O2− anions) according to the Lux-Flood theory of molten oxides. Phosphorus, by migrating from liquid iron to molten slag, allows both the production of a steel of satisfactory quality, and of phosphates sought after as fertilizer, known as \"Thomas meal\". The disadvantages of the basic process includes larger iron loss and more frequent relining of the converter vessel.\nAfter having favored the spectacular growth of the Lorraine iron and steel industry, the process progressively faded away in front of the Siemens-Martin Open-hearth furnace, which also used the benefit of basic refractory lining, before disappearing in the mid-1960s: with the development of gas liquefaction and the cryogenic separation of O2 from air, the use of pure oxygen became economically viable. Even if modern pure oxygen converters all operate with a basic medium, their performance and operation have little to do with their ancestor.\n\nBibliographic sources\n\nG. Reginald Bashforth, The manufacture of iron and steel, vol. 2: Steel production, London, Chapman & Hall Ltd, 1951, 461 p.\nThomas Turner (dir.), The metallurgy of iron: By Thomas Turner...: Being one of a series of treatises on metallurgy written by associates of the Royal school of mines, C. Griffin & company, limited, coll. \"Griffin's metallurgical series\", 1908, 3rd ed., 463 p. ISBN 978-1-177-69287-8\nWalter MacFarlane, The principles and practice of iron and steel manufacture, Longmans, Green, and Co, 1917, 5th ed.\nR.W. Burnie, Memoir and letters of Sidney Gilchrist Thomas, Inventor, John Murray, 1891\nWilliam Tulloch Jeans, The Creators of the Age of Steel, 1884, 356 p. ISBN 978-1-4179-5381-3\nHermann Wedding (translated from German by: William B. Phillips, Ph.D. & Ernst Prochaska), Wedding's basic Bessemer process [\"Basische Bessemer - oder Thomas-Process\"], New York Scientific Publishing Company, 1891, 224 p.\nJean Duflot, Encyclopædia Universalis, \"Sidérurgie\"\n",
    "source": "wikipedia",
    "title": "Gilchrist–Thomas process",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_13849906",
    "text": "This glossary of chemistry terms is a list of terms and definitions relevant to chemistry, including chemical laws, diagrams and formulae, laboratory tools, glassware, and equipment. Chemistry is a physical science concerned with the composition, structure, and properties of matter, as well as the changes it undergoes during chemical reactions; it features an extensive vocabulary and a significant amount of jargon.\nNote: All periodic table references refer to the IUPAC Style of the Periodic Table.\n\nA\nabsolute zero\nA theoretical condition concerning a system at the lowest limit of the thermodynamic temperature scale, or zero kelvins, at which the system does not emit or absorb energy (i.e. all atoms are at rest). By extrapolating the ideal gas law, the internationally agreed-upon value for absolute zero has been determined as −273.15 °C (−459.67 °F; 0.00 K).\n\nabsorbance\n\nabsorption\n1.  The physical or chemical process by which a substance in one state becomes incorporated into and retained by another substance of a different state. Absorption differs from adsorption in that the first substance permeates the entire bulk of the second substance, rather than just adhering to the surface.\n2.  The process by which matter (typically electrons bound in atoms) takes up the energy of electromagnetic radiation and transforms it into any of various types of internal energy, such as thermal energy. This type of absorption is the principle on which spectrophotometry is based.\n\nabundance\n\naccuracy\nHow close a measured value is to the actual or true value. Compare precision.\n\nacetyl\n\nachiral\n(of a molecule) Having the geometric symmetry of being indistinguishable from its own mirror image; lacking chirality.\n\nacid\n1.  (Brønsted–Lowry acid) Any chemical species or molecular entity that acts as a proton donor when reacting with another species, because it loses at least one proton (H+) which is then transferred or 'donated' to the other species, which by definition is a Brønsted–Lowry base. When dissolved in an aqueous solution, a proton donor which increases the concentration of hydronium ion (H3O+) by transferring protons to water molecules may also be called an Arrhenius acid. The term \"acid\", when not otherwise qualified, often refers implicitly to a Brønsted–Lowry acid.\n2.  (Lewis acid) Any chemical species or molecular entity that acts as an electron pair acceptor when reacting with another species, forming a covalent bond by accepting a lone pair of electrons donated by the other species, which is known as a Lewis base. This definition was intended as a generalization of the Brønsted–Lowry definition by proposing that acid-base reactions are best viewed as reorganizations of electrons rather than transfers of protons, with the acid being a species that accepts electron pairs from another species either directly or by releasing protons (H+) into the solution, which then accept electron pairs from the other species. The Lewis definition is inclusive of many B",
    "source": "wikipedia",
    "title": "Glossary of chemistry terms",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_47772755",
    "text": "C-glycosyltryptophan is an indolyl carboxylic amino acid with the structural formula C17H22N2O7. This sugar-loaded amino acid strongly correlates with age.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "C-glycosyl tryptophan",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_936085",
    "text": "Green chemistry, similar to sustainable chemistry or circular chemistry, is an area of chemistry and chemical engineering focused on the design of products and processes that minimize or eliminate the use and generation of hazardous substances. While environmental chemistry focuses on the effects of polluting chemicals on nature, green chemistry focuses on the environmental impact of chemistry, including lowering consumption of nonrenewable resources and technological approaches for preventing pollution.\nThe overarching goals of green chemistry—namely, more resource-efficient and inherently safer design of molecules, materials, products, and processes—can be pursued in a wide range of contexts.\n\nDefinition\nGreen chemistry (also called sustainable chemistry) is the design of chemical products and processes that reduce or eliminate the use and generation of hazardous substances. The concept integrates pollution-prevention and process-intensification approaches at laboratory and industrial scales to improve resource efficiency and minimize waste and risk across the life cycle of chemicals and materials.\nHistory\nGreen chemistry evolved and emerged from a variety of existing ideas and research efforts (such as Pollution Prevention, atom economy and catalysis) in the period leading up to the 1990s, in the context of increasing attention to problems of chemical pollution and resource depletion. The development of green chemistry in Europe and the United States was proceeded by a shift in environmental problem-solving strategies: a movement from command and control regulation and mandated lowering of industrial emissions at the \"end of the pipe,\" toward the broad interdisciplinary concept of prevention of pollution through the innovative design of production technologies themselves. The narrower set of concepts later recognized and re-named as green chemistry coalesced in the mid- to late-1990s, along with broader adoption of the new term in the Academic literature (which prevailed over earlier competing terms such as \"clean\" and \"sustainable\" chemistry).\nIn the United States, the Environmental Protection Agency played a significant supporting role in evolving green chemistry out of its earlier pollution prevention programs, funding, and cooperative coordination with industry. At the same time in the United Kingdom, researchers at the University of York, who used the term \"clean technology\" in the early 1990s, contributed to the establishment of the Green Chemistry Network within the Royal Society of Chemistry, and the launch of the journal Green Chemistry. In 1991, in the Netherlands, a special issue called 'green chemistry' [groene chemie] was published in Chemisch Magazine. In the Dutch context, the umbrella term green chemistry was associated with the exploitation of biomass as a renewable feedstock.\n",
    "source": "wikipedia",
    "title": "Green chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_79146569",
    "text": "Group Fertiberia (in English: Fertiberia Group) is a Spanish business conglomerate in the chemical industry. It has been operating since 1995 and has its origins in the historical company Fertiberia, whose activities have expanded throughout Spain, France and Portugal. Today, the group is one of the leading producers of fertilizers, ammonia and its byproducts in the European Union.\n\nHistory\nIn 1995, the Villar Mir Group took control of the assets of Fesa-Enfersa, which would form the basis for the re-foundation of the historic company Fertiberia. After a period of internal reorganization, which included plant closures and workforce reductions, Fertiberia began a period of expansion. This strategy included the creation of several subsidiary companies and the acquisition of other companies in the sector, such as Sefanitro, ASUR, Química del Estroncio, Fercampo, etc. This has led to Grupo Fertiberia being considered “the leading company in the fertilizer sector in Spain”, although its presence has subsequently extended to Portugal and France. In 2020, the Swedish-German group Triton Partners took control of Grupo Fertiberia.\nAffiliates\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Grupo Fertiberia",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_13335826",
    "text": "The International Chemical Identifier (InChI, pronounced  IN-chee) is a textual identifier for chemical substances, designed to provide a standard way to encode molecular information and to facilitate the search for such information in databases and on the web. Initially developed by the International Union of Pure and Applied Chemistry (IUPAC) and National Institute of Standards and Technology (NIST) from 2000 to 2005, the format and algorithms are non-proprietary. Since May 2009, it has been developed by the InChI Trust, a nonprofit charity from the United Kingdom which works to implement and promote the use of InChI.\nThe identifiers describe chemical substances in terms of layers of information — the atoms and their bond connectivity, tautomeric information, isotope information, stereochemistry, and electronic charge information.\nNot all layers have to be provided; for instance, the tautomer layer can be omitted if that type of information is not relevant to the particular application. The InChI algorithm converts input structural information into a unique InChI identifier in a three-step process: normalization (to remove redundant information), canonicalization (to generate a unique number label for each atom), and serialization (to give a string of characters).\nInChIs differ from the widely used CAS registry numbers in three respects: firstly, they are freely usable and non-proprietary; secondly, they can be computed from structural information and do not have to be assigned by some organization; and thirdly, most of the information in an InChI is human readable (with practice). InChIs can thus be seen as akin to a general and extremely formalized version of IUPAC names. They can express more information than the simpler SMILES notation and, in contrast to SMILES strings, every structure has a unique InChI string, which is important in database applications. Information about the 3-dimensional coordinates of atoms is not represented in InChI; for this purpose a format such as PDB can be used.\nThe InChIKey, sometimes referred to as a hashed InChI, is a fixed length (27 character) condensed digital representation of the InChI that is not human-understandable. The InChIKey specification was released in September 2007 in order to facilitate web searches for chemical compounds, since these were problematic with the full-length InChI. Unlike the InChI, the InChIKey is not unique: though collisions are expected to be extremely rare, there are known collisions.\nIn January 2009 the 1.02 version of the InChI software was released. This provided a means to generate so called standard InChI, which does not allow for user selectable options in dealing with the stereochemistry and tautomeric layers of the InChI string. The standard InChIKey is then the hashed version of the standard InChI string. The standard InChI will simplify comparison of InChI strings and keys generated by different groups, and subsequently accessed via diverse sources such as databa",
    "source": "wikipedia",
    "title": "International Chemical Identifier",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_78626621",
    "text": "Intrinsic DNA fluorescence is the fluorescence emitted directly by DNA when it absorbs ultraviolet (UV) radiation. It contrasts to that stemming from fluorescent labels that are either simply bound to DNA or covalently attached to it, widely used in biological applications; such labels may be chemically modified, not naturally occurring, nucleobases.\nThe intrinsic DNA fluorescence was discovered in the 1960s by studying nucleic acids in low temperature glasses. Since the beginning of the 21st century, the much weaker emission of nucleic acids in fluid solutions is being studied at room temperature by means sophisticated spectroscopic techniques, using as UV source femtosecond laser pulses, and following the evolution of the emitted light from femtoseconds to nanoseconds. The development of specific experimental protocols has been crucial for obtaining reliable results.  \nFluorescence studies combined to theoretical computations and transient absorption measurements bring information about the relaxation of the electronic excited states and, thus, contribute to understanding the very first steps of a complex series of events triggered by UV radiation, ultimately leading to DNA damage. The principles governing the behavior of the intrinsic RNA fluorescence, to which only a few studies have been dedicated,\n are the same as those described for DNA.\nThe knowledge of the fundamental processes underlying the DNA fluorescence paves the way for the development of label-free biosensors. The development of such optoelectronic devices for certain applications would have the advantage of bypassing the step of chemical synthesis or avoiding the uncertainties due to non-covalent biding of fluorescent dyes to nucleic acids.\n\nConditions for measuring the intrinsic DNA fluorescence\nDue to the weak intensity of the intrinsic DNA fluorescence, specific cautions are necessary in order to perform correct measurements and obtain reliable results. A first requirement concerns the purity of both the DNA samples and that of the chemicals and the water used to the preparation of the buffered solutions. The  buffer emission must be systematically recorded and, in certain cases, subtracted in an appropriate way. A second requirement is associated with the DNA damage provoked by the exciting UV light which alters its fluorescence. In order to overcome these difficulties, continuous stirring of the solution is needed. For measurements using laser excitation, the circulation of the DNA solution by means of a peristaltic pump is recommended; the reproducibility of successive fluorescence signal needs to be checked.\n",
    "source": "wikipedia",
    "title": "Intrinsic DNA fluorescence",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_53076456",
    "text": "Ioliomics (from a portmanteau of ions and liquids) is the study of ions in liquids (or liquid phases) and stipulated with fundamental differences of ionic interactions. Ioliomics covers a broad research area concerning structure, properties and applications of ions involved in various biological and chemical systems. The concept of this research discipline is related to other comprehensive research fields, such as genomics, proteomics, glycomics, petroleomics, etc., where the suffix -omics is used for describing the comprehensiveness of data.\n\nFundamental nature\nThe nature of chemical reactions and their description is one of the most fundamental problems in chemistry. The concepts of covalent and ionic bonds which emerged in the beginning of the 20th century specify the profound differences between their electronic structures. These differences, in turn, lead to dramatically different behavior of covalent and ionic compounds both in the solution and solid phase.  In the solid phase, ionic compounds, e.g. salts, are prone to formation of crystal lattices; in polar solvents, they dissociate into ions surrounded by solvate shells, thus rendering the solution highly ionic conductive.  In contrast to covalent bonds, ionic interactions demonstrate flexible, dynamic behavior, which allows tuning ionic compounds to obtain desired properties.\nImportance\nIonic compounds interact strongly with the solvent medium; therefore, their impact on chemical and biochemical processes involving ions can be significant. Even in the case of simplest ions and solvents, the presence of the former can lead to rearrangement and restructuring of the latter.  It is established that ionic reactions are involved in numerous phenomena at the scales of whole galaxies or single living cells. To name a few, in living cells, metal ions bind to metalloenzymes and other proteins therefore modulating their activity; ions are involved in the control of neuronal functioning during sleep – wakefulness cycles;  anomalous activity of ion channels results in the development of various disorders, such as Parkinson's and Alzheimer's diseases,  etc. Thus, despite the problems associated with the studies on properties and activities of ions in various chemical and biological systems, this research field is among the most urgent ones.\n",
    "source": "wikipedia",
    "title": "Ioliomics",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_6756239",
    "text": "Landolt–Börnstein is a collection of property data in materials science and the closely related fields of chemistry, physics and engineering published by Springer Nature.\n\nHistory\nOn July 28, 1882, Dr. Hans Heinrich Landolt and Dr. Richard Börnstein, both professors at the \"Landwirtschaftliche Hochschule\" (Agricultural College) at Berlin, signed a contract with the publisher Ferdinand Springer on the publication of a collection of tables with physical-chemical data. The title of this book \"Physikalisch-chemische Tabellen\" (Physical-Chemical Tables) published in 1883 was soon forgotten. Owing to its success the data collection has been known for more than a hundred years by each scientist only as \"The Landolt-Börnstein\".\n1250 copies of the 1st Edition were printed and sold. In 1894, the 2nd Edition was published, in 1905 the 3rd Edition, in 1912 the 4th Edition, and finally in 1923 the 5th Edition. Supplementary volumes of the latter were printed until as late as 1936. New Editions saw changes in large expansion of volumes, number of authors, updated structure, additional tables and coverage of new areas of physics and chemistry.\nThe 5th Edition was eventually published in 1923, consisting of two volumes and comprising a total of 1,695 pages. Sixty three authors had contributed to it. The growth that had already been noticed in previous editions, continued. It was clear, that \"another edition in approximately 10 years\" was no solution. A complete conceptual change of the Landolt–Börnstein had thus become necessary. For the meantime supplementary volumes in two-year intervals should be provided to fill in the blanks and add the latest data. The first supplementary volume of the 5th Edition was published in 1927, the second in 1931 and the third in 1935/36. The latter consisted of three sub-volumes with a total of 3,039 pages and contributions from 82 authors.\nThe 6th Edition (1950) was published in line with the revised general frame. The basic idea was to have four volumes instead of one, each of which was to cover different fields of the Landolt–Börnstein under different editors. Each volume was given a detailed table of contents. Two major restrictions were also imposed. The author of a contribution was asked to choose a \"Bestwert\" (optimum value) from the mass of statements of an experimental value in the publications of different authors, or derive a \"wahrscheinlichster Wert” (most possible value). The other change of importance was that not only diagrams became as important as tables, but that text also became necessary to explain the presented data.\n",
    "source": "wikipedia",
    "title": "Landolt–Börnstein",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_65216890",
    "text": "Organoantimony chemistry is the chemistry of compounds containing a carbon to antimony (Sb) chemical bond. Relevant oxidation states are SbV and SbIII. The toxicity of antimony limits practical application in organic chemistry.\n\nSyntheses\n\nReactions\nStibine oxides undergo a sort of polarized-olefin metathesis.  For example, they mediate a carbonyl-imine exchange (Ar is any activated arene):Ph3Sb=NSO2Ar + PhC=O → Ph3Sb=O + PhC=NSO2ArThe effect may extend vinylically: \n  \n    \n      \n        \n          \n            R\n            \n              2\n            \n            \n              \n            \n          \n          C\n          \n            =\n          \n          O\n          \n\n          \n          +\n          \n            HBrCHCO\n            \n              2\n            \n            \n              \n            \n          \n          R\n          \n            \n              →\n              \n                \n                  \n                    Bu\n                    \n                      3\n                    \n                    \n                      \n                    \n                  \n                  Sb\n                \n              \n            \n          \n          \n            R\n            \n              2\n            \n            \n              \n            \n          \n          C\n          \n            =\n          \n          \n            CHCO\n            \n              2\n            \n            \n              \n            \n          \n          R\n          \n\n          \n          +\n          HBr\n        \n      \n    \n    {\\displaystyle {\\ce {R2C=O{}+ HBrCHCO2R ->[{\\ce {Bu3Sb}}] R2C=CHCO2R{}+ HBr}}}\n  \nIn contrast, unstabilized ylides (R3Sb=CR'2; R' not electron-withdrawing) form only with difficulty (e.g. diazo reagents).\nLike other metals, stibanes vicinal to a leaving group can eliminate before a proton.  For example, diphenyl(β-hydroxyphenethyl)stibine decomposes in heat or acid to styrene:\n\nPh2SbCH2CH(OH)Ph → CH2=CHPh + Ph2SbOH\nAs tertiary stibines also insert into haloalkyl bonds, tertiary stibines are powerful dehalogenating agents.  However, stibanes poorly imitate active metal organometallics: only with difficulty do their ligands add to carbonyls or they power noble-metal cross couplings.\nStiboranes are gentle oxidants, converting acyloins to diketones and thiols to disulfides.  In air, tris(thiophenyl)stibine catalyzes a Hunsdiecker-like decarboxylative oxidation of anhydrides to alcohols.\nIn ultraviolet light, distibines radicalize; the resulting radicals can displace iodide.\n",
    "source": "wikipedia",
    "title": "Organoantimony chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_52754183",
    "text": "Liquid nitrogen wash is a process mainly used for the production of ammonia synthesis gas within fertilizer production plants. It is usually the last purification step in the ammonia production process sequence upstream of the actual ammonia production.\n\nCompeting technologies\nThe purpose of the final purification step upstream of the actual ammonia production is to remove all components that are poisonous for the sensitive ammonia synthesis catalyst. This can be done with the following concepts:\n\nMethanation, formally the standard concept with the disadvantage, that the methane content is not removed, but even increased, since in this process, the carbon oxides (carbon monoxide and carbon dioxide) are converted to methane.\nPressure Swing Adsorption, which can replace the low temperature shift, the carbon dioxide removal and the methanation, since this process produces pure hydrogen, which can be mixed with pure nitrogen.\nLiquid Nitrogen Wash, which produces an ammonia syngas for a so-called \"inert free\" ammonia synthesis loop, that can be operated without the withdrawal of a purge gas stream.\nFunctions\nThe liquid nitrogen wash has two principle functions: \n\nRemoval of impurities such as carbon monoxide, argon and methane from the crude hydrogen gas\nAddition of the required stoichiometric amount of nitrogen to the hydrogen stream to achieve the correct ammonia synthesis gas ratio of hydrogen to nitrogen of 3 : 1\nThe carbon monoxide must be removed completely from the synthesis gas (i.e. syngas) since it is poisonous for the sensitive ammonia synthesis catalyst. \nThe components argon and methane are inert gases within the ammonia synthesis loop, but would enrich there and call for a purge gas system with synthesis gas losses or additional expenditures for a purge gas separation unit. \nThe main sources for the supply of feed gases are partial oxidation processes.\nUpstream syngas preparations\nSince the synthesis gas exiting the partial oxidation process consists mainly of carbon monoxide and hydrogen, usually a sulfur tolerant CO shift (i.e. water-gas shift reaction) is installed in order to convert as much carbon monoxide into hydrogen as possible.\nShifting carbon monoxide and water into hydrogen also produces carbon dioxide, usually this is removed in an acid gas scrubbing process together with other sour gases as e.g. hydrogen sulfide (e.g. in a Rectisol Wash Unit).\n",
    "source": "wikipedia",
    "title": "Liquid nitrogen wash",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_30897833",
    "text": "Magnetochemistry is concerned with the magnetic properties of chemical compounds and elements. Magnetic properties arise from the spin and orbital angular momentum of the electrons contained in a compound. Compounds are diamagnetic when they contain no unpaired electrons. Molecular compounds that contain one or more unpaired electrons are paramagnetic. The magnitude of the paramagnetism is expressed as an effective magnetic moment, μeff. For first-row transition metals the magnitude of μeff is, to a first approximation, a simple function of the number of unpaired electrons, the spin-only formula. In general, spin–orbit coupling causes μeff to deviate from the spin-only formula. For the heavier transition metals, lanthanides and actinides, spin–orbit coupling cannot be ignored. Exchange interaction can occur in clusters and infinite lattices, resulting in ferromagnetism, antiferromagnetism or ferrimagnetism depending on the relative orientations of the individual spins.\n\nMagnetic susceptibility\nThe primary measurement in magnetochemistry is magnetic susceptibility. This measures the strength of interaction on placing the substance in a magnetic field. The volume magnetic susceptibility, represented by the symbol \n  \n    \n      \n        \n          χ\n          \n            v\n          \n        \n      \n    \n    {\\displaystyle \\chi _{v}}\n  \n is defined by the relationship\n\n  \n    \n      \n        \n          \n            \n              M\n              →\n            \n          \n        \n        =\n        \n          χ\n          \n            v\n          \n        \n        \n          \n            \n              H\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {M}}=\\chi _{v}{\\vec {H}}}\n  \n\nwhere, \n  \n    \n      \n        \n          \n            \n              M\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {M}}}\n  \n is the magnetization of the material (the magnetic dipole moment per unit volume), measured in amperes per meter (SI units), and \n  \n    \n      \n        \n          \n            \n              H\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {H}}}\n  \n is the magnetic field strength, also measured in amperes per meter. Susceptibility is a dimensionless quantity. For chemical applications the molar magnetic susceptibility (χmol) is the preferred quantity. It is measured in m3·mol−1 (SI) or cm3·mol−1 (CGS) and is defined as\n\n  \n    \n      \n        \n          χ\n          \n            mol\n          \n        \n        =\n        M\n        \n          χ\n          \n            v\n          \n        \n        \n          /\n        \n        ρ\n      \n    \n    {\\displaystyle \\chi _{\\text{mol}}=M\\chi _{v}/\\rho }\n  \n\nwhere ρ is the density in kg·m−3 (SI) or g·cm−3 (CGS) and M is molar mass in kg·mol−1 (SI) or g·mol−1 (CGS).\n\nA variety of methods are available for the measurement of magnetic susceptibility.\n\nWith the Gouy balance the weight change of the",
    "source": "wikipedia",
    "title": "Magnetochemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_1570072",
    "text": "Mathematical chemistry is the area of research engaged in novel applications of mathematics to chemistry; it concerns itself principally with the mathematical modeling of chemical phenomena. Mathematical chemistry has also sometimes been called computer chemistry, but should not be confused with computational chemistry.\nMajor areas of research in mathematical chemistry include chemical graph theory, which deals with topology such as the mathematical study of isomerism and the development of topological descriptors or indices which find application in quantitative structure-property relationships; and chemical aspects of group theory, which finds applications in stereochemistry and quantum chemistry. Another important area is molecular knot theory and circuit topology that describe the topology of folded linear molecules such as proteins and nucleic acids.\nThe history of the approach may be traced back to the 19th century. Georg Helm published a treatise titled \"The Principles of Mathematical Chemistry: The Energetics of Chemical Phenomena\" in 1894. Some of the more contemporary periodical publications specializing in the field are MATCH Communications in Mathematical and in Computer Chemistry, first published in 1975, and the Journal of Mathematical Chemistry, first published in 1987. In 1986 a series of annual conferences MATH/CHEM/COMP taking place in Dubrovnik was initiated by the late Ante Graovac.\nThe basic models for mathematical chemistry are molecular graph and topological index.\nIn 2005 the International Academy of Mathematical Chemistry (IAMC) was founded in Dubrovnik (Croatia) by Milan Randić. The Academy has 82 members (2009) from all over the world, including six scientists awarded with a Nobel Prize.\n\nSee also\nChemical reaction network theory – Area of applied mathematics\nCheminformatics – Computational chemistry\nCombinatorial chemistry – Compound library-based chemical synthesis method\nMolecular descriptor\nMolecular modelling – Discovering chemical properties by physical simulations\nList of quantum chemistry and solid state physics software\nList of software for molecular mechanics modeling\nRandom graph theory of gelation – Mathematical theory for sol–gel processes\n",
    "source": "wikipedia",
    "title": "Mathematical chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_710202",
    "text": "Mechanochemistry (or mechanical chemistry) is the initiation of chemical reactions by mechanical phenomena. Mechanochemistry thus represents a fourth way to cause chemical reactions, complementing thermal reactions in fluids, photochemistry, and electrochemistry. Conventionally mechanochemistry focuses on the transformations of covalent bonds by mechanical force. Not covered by the topic are many phenomena: phase transitions, dynamics of biomolecules (docking, folding), and sonochemistry. Mechanochemistry also encompasses mechanophores which are molecules that undergo predictable changes in response to applied stress. Two types of mechanophores are mechanochromic ones in which a force causes a change in molecular structure and subsequently color and acid releasing mechanophores that release small amounts of an acid such as HCl in response to an applied force. \nMechanochemistry is not the same as mechanosynthesis, which refers specifically to the machine-controlled construction of complex molecular products.\nIn natural environments, mechanochemical reactions are frequently induced by physical processes such as earthquakes, glacier movement or hydraulic action of rivers or waves. In extreme environments such as subglacial lakes, hydrogen generated by mechnochemical reactions involving crushed silicate rocks and water can support methanogenic microbial communities. And mechanochemistry may have generated oxygen in the ancient Earth by water splitting on fractured mineral surfaces at high temperatures, potentially influencing life's origin or early evolution.\n\nHistory\nThe primal mechanochemical project was to make fire by rubbing pieces of wood against each other, creating friction and hence heat, triggering combustion at the elevated temperature. Another method involves the use of flint and steel, during which a spark (a small particle of pyrophoric metal) spontaneously combusts in air, starting fire instantaneously.\nIndustrial mechanochemistry began with the grinding of two solid reactants. Mercuric sulfide (the mineral cinnabar) and copper metal thereby react to produce mercury and copper sulfide:\n\nHgS + 2Cu → Hg + Cu2S\nA special issue of Chemical Society Review was dedicated to mechanochemistry.\nScientists recognized that mechanochemical reactions occur in environments naturally due to various processes, and the reaction products have the potential to influence microbial communities in tectonically active regions. The field has garnered increasing attention recently as mechanochemistry has the potential to generate diverse molecules capable of supporting extremophilic microbes, influencing the early evolution of life, developing the systems necessary for the origin of life, or supporting alien life forms. The field has now inspired the initiation of a special research topic in the journal Frontiers in Geochemistry.\n",
    "source": "wikipedia",
    "title": "Mechanochemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_71234088",
    "text": "A mental gland is a part of the body found in many species of amphibians and reptiles.  Mental glands produce chemicals that conspecific animals use to communicate.\n\nLocation\nThe mental glands appear in pairs, one on each side of the head. They are located behind the end of the mandible.\nFunction\nMental glands produce hormones that are secreted through the skin. The secretions from mental glands have been implicated in mate selection, species identification, and other functions.\nScientists believe that the head bobbing behavior observed in turtles encountering another member of their own species may serve to disperse the chemicals from the mental glands through the air.  Certain courtship behaviors observed in salamanders, such as snapping, only appear in salamanders that have mental glands, so scientists believe they are also meant to spread the chemicals through the air.\nOrigins and evolution\nNot all reptiles and amphibians have mental glands. It is not unusual for some species in the same family to have mental glands while others do not.\nIn 2021, one team of scientists found that most turtles that have mental glands are aquatic.  They concluded that mental glands developed once in turtles, in the ancestor of the family Testudinoidea, and that all turtles that have mental glands develop them from tissue of homologous origin.  They inferred that turtles that do not have mental glands lost them.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Mental gland",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_61854053",
    "text": "Metal Assisted Chemical Etching (also known as MACE) is the process of wet chemical etching of semiconductors (mainly silicon) with the use of a metal catalyst, usually deposited on the surface of a semiconductor in the form of a thin film or nanoparticles. The semiconductor, covered with the metal, is then immersed in an etching solution containing an oxidizing agent and hydrofluoric acid. The metal on the surface catalyzes the reduction of the oxidizing agent and therefore in turn also the dissolution of silicon. In the majority of the conducted research this phenomenon of increased dissolution rate is also spatially confined, such that it is increased in close proximity to a metal particle at the surface. Eventually this leads to the formation of straight pores that are etched into the semiconductor (see figure to the right). This means that a pre-defined pattern of the metal on the surface can be directly transferred to a semiconductor substrate.\n\nHistory of development\nMACE is a relatively new technology in semiconductor engineering and therefore it has yet to be a process that is used in industry. The first attempts of MACE consisted of a silicon wafer that was partially covered with aluminum and then immersed in an etching solution. This material combination led to an increased etching rate compared to bare silicon. Often this very first attempt is also called galvanic etching instead of metal assisted chemical etching.\nFurther research showed that a thin film of a noble metal deposited on a silicon wafer's surface can also locally increase the etching rate. In particular, it was observed that noble metal particles sink down into the material when the sample is immersed in an etching solution containing an oxidizing agent and hydrofluoric acid (see image in the introduction).  This method is now commonly called the metal assisted chemical etching of silicon.   \nOther semiconductors were also successfully etched with MACE, such as silicon carbide or gallium nitride. However, the main portion of research is dedicated to MACE of silicon.\nIt has been shown that both noble metals such as gold, platinum, palladium, and silver, and base metals such as iron, nickel, copper, and aluminium can act as a catalyst in the process.\n",
    "source": "wikipedia",
    "title": "Metal assisted chemical etching",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_49072305",
    "text": "Methane functionalization is the process of converting methane in its gaseous state to another molecule with a functional group, typically methanol or acetic acid, through the use of transition metal catalysts.\nIn the realm of carbon-hydrogen bond activation and functionalization (C-H activation/functionalization), many recent efforts have been made in order to catalytically functionalize  the C-H bonds in methane. The large abundance of methane in natural gas or shale gas deposits presents a large potential for its use as a feedstock in modern chemistry. However, given its gaseous natural state, it is quite difficult to transport economically. Its ideal use would be as a raw starting material for methanol or acetic acid synthesis, with plants built at the source to eliminate the issue of transportation. Methanol, in particular, would be of great use as a potential fuel source, and many efforts have been applied to researching the feasibilities of a methanol economy.\nThe challenges of C-H activation and functionalization present themselves when several factors are taken into consideration. Firstly, the C-H bond is extremely inert and non-polar, with a high bond dissociation energy, making methane a relatively unreactive starting material. Secondly, any products formed from methane would likely be more reactive than the starting product, which would be detrimental to the selectivity and yield of the reaction. \n\nThe main strategy currently used to increase the reactivity of methane uses transition metal complexes to activate the carbon-hydrogen bonds. In a typical C-H activation mechanism, a transition metal catalyst coordinates to the C-H bond to cleave it, and convert it into a bond with a lower bond dissociation energy. By doing so, the product can be used in further downstream reactions, since it will usually have a new functional group attached to the carbon. It is also important to note the difference between the terms \"activation\" and \"functionalization,\" since both terms are often used interchangeably, but should be held distinct from each other. Activation refers to the coordination of a metal center to the C-H bond, whereas functionalization occurs when the coordinated metal complex is further reacted with a group \"X\" to result in the functionalized product.\n\nMethane activation\nThe four most common methods of transition metal catalyzed methane activation are the Shilov system, sigma bond metathesis, oxidative addition, and 1,2 addition reactions.\n\nThe Shilov system involves platinum based complexes to produce metal alkyls. It was first discovered when a hydrogen-deuterium exchanged was observed in a deuterated solution with the platinum tetrachloride anion. Shilov et al. then was able to catalytically convert methane into methanol or methyl chloride when a Pt(IV) salt was used as a stoichiometric oxidant. The process is simplified down into three main steps: (1) C-H activation, (2) a redox reaction to form an octahedral intermediate, foll",
    "source": "wikipedia",
    "title": "Methane functionalization",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_19780855",
    "text": "Micro x-ray fluorescence (μXRF) is an elemental analysis technique that relies on the same principles as x-ray fluorescence (XRF). Synchrotron X-rays may be used to provide elemental imaging with biological samples.    \nThe spatial resolution diameter of micro x-ray fluorescence is many orders of magnitude smaller than that of conventional XRF. While a smaller excitation spot can be achieved by restricting x-ray beam using a pinhole aperture, this method blocks much of the x-ray flux which has an adverse effect on the sensitivity of trace elemental analysis. Two types of x-ray optics, polycapillary and doubly curved crystal focusing optics, are able to create small focal spots of just a few micrometers in diameter. By using x-ray optics, the irradiation of the focal spot is much more intense and allows for enhanced trace element analysis and better resolution of small features. Micro x-ray fluorescence using x-ray optics has been used in applications such as forensics, small feature evaluations, elemental mapping, mineralogy, electronics, multi-layered coating analysis, micro-contamination detection, film and plating thickness, biology and environment.\n\nApplication in forensic science\nMicro x-ray fluorescence is among the newest technologies used to detect fingerprints. It is a new visualization technique which rapidly reveals the elemental composition of a sample by irradiating it with a thin beam of X-rays without disturbing the sample. It was discovered recently by scientists at the Los Alamos National Laboratory. The newly discovered technique was then first revealed at the 229th national meeting of the American Chemical Society (March, 2005). This new discovery could prove to be very beneficial to the law enforcement world, because it is expected that μXRF will be able to detect the most complex molecules in fingerprints.\nMichael Bernstein of the American Chemical Society describes how the process works \"Salts such as sodium chloride and potassium chloride excreted in sweat are sometimes present in detectable quantities in fingerprints. Using μXRF, the researchers showed that they could detect the sodium, potassium and chlorine from such salts. And since these salts are deposited along the patterns present in a fingerprint, an image of the fingerprint can be visualized producing an elemental image for analysis.\" This basically means that we can “see” a fingerprint because the salts are deposited mainly along the patterns present in a fingerprint.\nSince μXRF technology uses X-ray technology to detect fingerprints, instead of traditional techniques, the image comes out much clearer. Traditional fingerprints are performed by a technique using powders, liquids or vapors to add color to the fingerprint so it can be distinguished. But sometimes this process may alter the fingerprint or may not be able to detect some of the more complex molecules.\nAnother μXRF application in forensics is GSR (gunshot residue) determination. Some specific elements, ",
    "source": "wikipedia",
    "title": "Micro-X-ray fluorescence",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_50311973",
    "text": "Microfluidic cell culture integrates knowledge from biology, biochemistry, engineering, and physics to develop devices and techniques for culturing, maintaining, analyzing, and experimenting with cells at the microscale. It merges microfluidics, a set of technologies used for the manipulation of small fluid volumes (μL, nL, pL) within artificially fabricated microsystems, and cell culture, which involves the maintenance and growth of cells in a controlled laboratory environment. Microfluidics has been used for cell biology studies as the dimensions of the microfluidic channels are well suited for the physical scale of cells (in the order of magnitude of 10 micrometers). For example, eukaryotic cells have linear dimensions between 10 and 100 μm which falls within the range of microfluidic dimensions. A key component of microfluidic cell culture is being able to mimic the cell microenvironment which includes soluble factors that regulate cell structure, function, behavior, and growth. Another important component for the devices is the ability to produce stable gradients that are present in vivo as these gradients play a significant role in understanding chemotactic, durotactic, and haptotactic effects on cells.\n\nFabrication\nSome considerations for microfluidic devices relating to cell culture include:\n\nfabrication material (e.g., polydimethylsiloxane (PDMS), polystyrene)\nculture region geometry\ncontrol system for delivering and removing media when needed using either passive methods (e.g., gravity-driven flow, capillary pumps, or Laplace pressure based 'passive pumping') or a flow-rate controlled device (i.e., perfusion system)\nFabrication material is crucial as not all polymers are biocompatible, with some materials such as PDMS causing undesirable adsorption or absorption of small molecules. Additionally, uncured PDMS oligomers can leach into the cell culture media, which can harm the microenvironment. As an alternative to commonly used PDMS, there have been advances in the use of thermoplastics (e.g., polystyrene) as a replacement material.\nSpatial organization of cells in microscale devices largely depends on the culture region geometry for cells to perform functions in vivo. For example, long, narrow channels may be desired to culture neurons. The perfusion system chosen might also affect the geometry chosen. For example, in a system that incorporates syringe pumps, channels for perfusion inlet, perfusion outlet, waste, and cell loading would need to be added for the cell culture maintenance. Perfusion in microfluidic cell culture is important to enable long culture periods on-chip and cell differentiation.\nOther critical aspects for controlling the microenvironment include: cell seeding density, reduction of air bubbles as they can rupture cell membranes, evaporation of media due to an insufficiently humid environment, and cell culture maintenance (i.e. regular, timely media changes).\nCell's health is defined as the collective equilibrium act",
    "source": "wikipedia",
    "title": "Microfluidic cell culture",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_7981894",
    "text": "Microscale chemistry (often referred to as small-scale chemistry, in German: Chemie im Mikromaßstab) is an analytical method and also a teaching method widely used at school and at university levels, working with small quantities of chemical substances. While much of traditional chemistry teaching centers on multi-gramme preparations, milligrammes of substances are sufficient for microscale chemistry. In universities, modern and expensive lab glassware is used and modern methods for detection and characterization of the produced substances are very common. In schools and in many countries of the Southern hemisphere, small-scale working takes place with low-cost and even no-cost material. There has always been a place for small-scale working in qualitative analysis, but the new developments can encompass much of chemistry a student is likely to meet.\n\nHistory\nThere are two main strands of the modern approach. \nOne is based on the idea that many of the experiments associated with general chemistry (acids and bases, oxidation and reduction, electrochemistry, etc.) can be carried out in equipment much simpler (injection bottles, dropper bottles, syringes, wellplates, plastic pipettes) and therefore cheaper than the traditional glassware in a laboratory, thus enabling the expansion of the laboratory experiences of students in large classes and to introduce laboratory work into institutions too poorly equipped for standard-type work. Pioneering development in this area was carried out by  Egerton C. Grey (1928), Mahmoud K. El-Marsafy (1989) in Egypt, Stephen Thompson in the US and others. A further application of these ideas was the devising by Bradley of the Radmaste kits in South Africa, designed to make effective chemical experiments possible in developing countries in schools that lack the technical services (electricity, running water) taken for granted in many places.\nThe other strand is the introduction of this approach into synthetic work, mainly in organic chemistry. Here the crucial breakthrough was achieved by Mayo, Pike and Butcher and by Williamson who demonstrated that inexperienced students were able to carry out organic syntheses on a few tens of milligrams, a skill previously thought to require years of training and experience. These approaches were accompanied by the introduction of some specialised equipment, which was subsequently simplified by Breuer without great loss of versatility.\nThere is a great deal of published material available to help in the introduction of such a scheme, providing advice on choice of equipment, techniques and preparative experiments and the flow of such material is continuing through a column in the Journal of Chemical Education called 'The Microscale Laboratory' that has been running for many years.\nScaling down experiments, when combined with modern projection technology, opened up the possibility of carrying out lecture demonstrations of the most hazardous kind in total safety.\nThe approach has been ",
    "source": "wikipedia",
    "title": "Microscale chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_66400985",
    "text": "Microsegregation is a non-uniform chemical separation and concentration of elements or impurities in alloys after they have solidified.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Microsegregation",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_47305230",
    "text": "A mixed oxidant solution (MOS) is a type of disinfectant that has many uses including disinfecting, sterilizing, and eliminating pathogenic microorganisms in water. An MOS may have advantages such as a higher disinfecting power, stable residual chlorine in water, elimination of biofilm, and safety. The main components of an MOS are chlorine and its derivatives (ClO− and HClO), which are produced by electrolysis of sodium chloride. It may also contain high amounts of hydroxy radicals, chlorine dioxide, dissolved ozone, hydrogen peroxide and oxygen from which the name \"mixed oxidant\" is derived.\n\nPerformance\n\nComparisons\n\nApplications\nMixed oxidant solutions for water treatment may improve safety, lower general corrosion rates, increase performance, and save money. MOS may be more effective than bleach and can be used for a variety of applications. Some of these applications are cited below.\nCooling water treatment: An MOS for industrial cooling water treatment and disinfection improves safety and thermal efficiency, lowers general corrosion rates, increases performance, and saves money, resulting in a reduction of downtime, maintenance, and expense. Additionally, it can improve workplace safety by eliminating the handling and storage of hazardous chemicals while maintaining steady microbiological control.\nCooling tower water treatment: An MOS improves cooling tower efficiency, safety, and cost compared to conventional biocide treatment methods for legionella prevention, biofilm removal, and inactivation of other performance-inhibiting waterborne organisms.\nIndustrial process water and wastewater treatment: As the lowest cost supplier of chlorine for disinfection and oxidation of process water and wastewater prior to discharge, an MOS is used in industrial wastewater treatment. MOS chemistry is more effective at biofilm control. Biochemical and Chemical oxygen demand removal, breakpoint chlorination of ammonia and hydrogen sulfide removal.\nMunicipal wastewater: As one of the world's most precious natural resources, the reuse of water is becoming increasingly important. MOS is both the most cost-effective solution and the preferred technology for disinfection and oxidation of wastewater for reuse or reintroduction into the environment eliminating many of the negative problems associated with traditional chlorine disinfection.\nDrinking water & beverage facilities: An MOS is a proven disinfectant for improving the quality and safety of drinking water with significant economic savings. For providing clean, safe drinking water ranges from rural communities to large cities. Also providing clean, safe water at food and beverage facilities. It is ideally suited for carbonated soft drinks bottling, brewing, dairy farms and dairy and food processing applications.\nAquatics and pools: An alternative to chlorine for pool cleaning is an MOS. It can reduce skin and eye irritation, and skin redness and dryness often associated with chlorine. An MOS can also reduce",
    "source": "wikipedia",
    "title": "Mixed oxidant",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_286069",
    "text": "In chemistry, a mixture is a material made up of two or more different chemical substances which can be separated by physical method. It is an impure substance made up of 2 or more elements or compounds mechanically mixed together in any proportion. A mixture is the physical combination of two or more substances in which the identities are retained and are mixed in the form of solutions, suspensions or colloids.\nMixtures are one product of mechanically blending or mixing chemical substances such as elements and compounds, without chemical bonding or other chemical change, so that each ingredient substance retains its own chemical properties and makeup. Despite the fact that there are no chemical changes to its constituents, the physical properties of a mixture, such as its melting point, may differ from those of the components. Some mixtures can be separated into their components by using physical (mechanical or thermal) means. Azeotropes are one kind of mixture that usually poses considerable difficulties regarding the separation processes required to obtain their constituents (physical or chemical processes or, even a blend of them).\n\nCharacteristics of mixtures\nAll mixtures can be characterized as being separable by mechanical means (e.g. purification, distillation, electrolysis, chromatography, heat, filtration, gravitational sorting, centrifugation). Mixtures differ from chemical compounds in the following ways:\n\nThe substances in a mixture can be separated using physical methods such as filtration, freezing, and distillation.\nThere is little or no energy change when a mixture forms (see Enthalpy of mixing).\nThe substances in a mixture keep their separate properties.\nIn the example of sand and water, neither one of the two substances changed in any way when they are mixed. Although the sand is in the water it still keeps the same properties that it had when it was outside the water.\n\nmixtures have variable compositions, while compounds have a fixed, definite formula.\nwhen mixed, individual substances keep their properties in a mixture, while if they form a compound their properties can change.\nThe following table shows the main properties and examples for all possible phase combinations of the three \"families\" of mixtures :\n",
    "source": "wikipedia",
    "title": "Mixture",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_19555",
    "text": "A molecule is a group of two or more atoms that are held together by attractive forces known as chemical bonds; depending on context, the term may or may not include ions that satisfy this criterion. In quantum physics, organic chemistry, and biochemistry, the distinction from ions is dropped and molecule is often used when referring to polyatomic ions.\nA molecule may be homonuclear, that is, it consists of atoms of one chemical element, e.g. two atoms in the oxygen molecule (O2); or it may be heteronuclear, a chemical compound composed of more than one element, e.g. water (two hydrogen atoms and one oxygen atom; H2O). In the kinetic theory of gases, the term molecule is often used for any gaseous particle regardless of its composition. This relaxes the requirement that a molecule contains two or more atoms, since the noble gases are individual atoms. Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are typically not considered single molecules.\nConcepts similar to molecules have been discussed since ancient times, but modern investigation into the nature of molecules and their bonds began in the 17th century. Refined over time by scientists such as Robert Boyle, Amedeo Avogadro, Jean Perrin, and Linus Pauling, the study of molecules is today known as molecular physics or molecular chemistry.\n\nEtymology\nAccording to Merriam-Webster and the Online Etymology Dictionary, the word \"molecule\" derives from the Latin \"moles\" or small unit of mass. The word is derived from French molécule (1678), from Neo-Latin molecula, diminutive of Latin moles \"mass, barrier\". The word, which until the late 18th century was used only in Latin form, became popular after being used in works of philosophy by Descartes.\nHistory\nThe definition of the molecule has evolved as knowledge of the structure of molecules has increased. Earlier definitions were less precise, defining molecules as the smallest particles of pure chemical substances that still retain their composition and chemical properties. This definition often breaks down since many substances in ordinary experience, such as rocks, salts, and metals, are composed of large crystalline networks of chemically bonded atoms or ions, but are not made of discrete molecules.\nThe modern concept of molecules can be traced back towards pre-scientific and Greek philosophers such as Leucippus and Democritus who argued that all the universe is composed of atoms and voids. Circa 450 BC Empedocles imagined fundamental elements (fire (), earth (), air (), and water ()) and \"forces\" of attraction and repulsion allowing the elements to interact.\nA fifth element, the incorruptible quintessence aether, was considered to be the fundamental building block of the heavenly bodies. The viewpoint of Leucippus and Empedocles, along with the aether, was accepted by Aristotle and passed to medieval and renaissance Europe.\nIn a more concrete manner, however, the concept of aggregates or units of",
    "source": "wikipedia",
    "title": "Molecule",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_4653948",
    "text": "Nanochemistry is an emerging sub-discipline of the chemical and material sciences that deals with the development of new methods for creating nanoscale materials. The term \"nanochemistry\" was first used by Ozin in 1992 as 'the uses of chemical synthesis to reproducibly afford nanomaterials from the atom \"up\", contrary to the nanoengineering and nanophysics approach that operates from the bulk \"down\"'. Nanochemistry focuses on solid-state chemistry that emphasizes synthesis of building blocks that are dependent on size, surface, shape, and defect properties, rather than the actual production of matter. Atomic and molecular properties mainly deal with the degrees of freedom of atoms in the periodic table. However, nanochemistry introduced other degrees of freedom that controls material's behaviors by transformation into solutions. Nanoscale objects exhibit novel material properties, largely as a consequence of their finite small size. Several chemical modifications on nanometer-scaled structures approve size dependent effects.\n\nNanochemistry is used in chemical, materials and physical science as well as engineering, biological, and medical applications. Silica, gold, polydimethylsiloxane, cadmium selenide, iron oxide, and carbon are materials that show its transformative power. Nanochemistry can make the most effective contrast agent of MRI out of iron oxide (rust) which can detect cancers and kill them at their initial stages. Silica (glass) can be used to bend or stop lights in their tracks. Developing countries also use silicone to make circuits for the fluids used in pathogen detection. Nano-construct synthesis leads to the self-assembly of the building blocks into  functional structures that may be useful for electronic, photonic, medical, or bioanalytical problems. Nanochemical methods can be used to create carbon nanomaterials such as carbon nanotubes, graphene, and fullerenes which have gained attention in recent years due to their remarkable mechanical and electrical properties.\n\nHistory\nOne of the first scientific reports is the colloidal gold particles synthesized by Michael Faraday as early as 1857. By the early 1940's, precipitated and fumed silica nanoparticles were being manufactured and sold in USA and Germany as substitutes for ultrafine carbon black for rubber reinforcements.\n",
    "source": "wikipedia",
    "title": "Nanochemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_1209760",
    "text": "A natural product is a natural compound or substance produced by a living organism—that is, found in nature. In the broadest sense, natural products include any substance produced by life. Natural products can also be prepared by chemical synthesis (both semisynthesis and total synthesis and have played a central role in the development of the field of organic chemistry by providing challenging synthetic targets). The term natural product has also been extended for commercial purposes to refer to cosmetics, dietary supplements, and foods produced from natural sources without added artificial ingredients.\nWithin the field of organic chemistry, the definition of natural products is usually restricted to organic compounds isolated from natural sources that are produced by the  pathways of primary or secondary metabolism.  Within the field of medicinal chemistry, the definition is often further restricted to secondary metabolites. Secondary metabolites (or specialized metabolites) are not essential for survival, but nevertheless provide organisms that produce them an evolutionary advantage. Many secondary metabolites are cytotoxic and have been selected and optimized through evolution for use as \"chemical warfare\" agents against prey, predators, and competing organisms. Secondary or specialized metabolites are often unique to specific species, whereas primary metabolites are commonly found across multiple kingdoms. Secondary metabolites are marked by chemical complexity which is why they are of such interest to chemists.\nNatural sources may lead to basic research on potential bioactive components for commercial development as lead compounds in drug discovery. Although natural products have inspired numerous  drugs, drug development from natural sources has received declining attention in the 21st century by pharmaceutical companies, partly due to unreliable access and supply, intellectual property, cost, and profit concerns, seasonal or environmental variability of composition, and loss of sources due to rising extinction rates. Despite this, natural products and their derivatives still accounted for about 10% of new drug approvals between 2017 and 2019.\n\nClasses\nThe broadest definition of natural product is anything that is produced by life, and includes the likes of biotic materials (e.g. wood, silk), bio-based materials (e.g. bioplastics, cornstarch), bodily fluids (e.g. milk, plant exudates), and other natural materials (e.g. soil, coal).\nNatural products may be classified according to their biological function, biosynthetic pathway, or source. Depending on the sources, the number of known natural product molecules ranges between 300,000 and 400,000.\n",
    "source": "wikipedia",
    "title": "Natural product",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_242001",
    "text": "Nuclear chemistry is the sub-field of chemistry dealing with radioactivity, nuclear processes, and transformations in the nuclei of atoms, such as nuclear transmutation and nuclear properties.\nIt is the chemistry of radioactive elements such as the actinides, radium and radon together with the chemistry associated with equipment (such as nuclear reactors) which are designed to perform nuclear processes. This includes the corrosion of surfaces and the behavior under conditions of both normal and abnormal operation (such as during an accident). An important area is the behavior of objects and materials after being placed into a nuclear waste storage or disposal site.\nIt includes the study of the chemical effects resulting from the absorption of radiation within living animals, plants, and other materials. The radiation chemistry controls much of radiation biology as radiation has an effect on living things at the molecular scale. To explain it another way, the radiation alters the biochemicals within an organism, the alteration of the bio-molecules then changes the chemistry which occurs within the organism; this change in chemistry then can lead to a biological outcome. As a result, nuclear chemistry greatly assists the understanding of medical treatments (such as cancer radiotherapy) and has enabled these treatments to improve.\nIt includes the study of the production and use of radioactive sources for a range of processes. These include radiotherapy in medical applications; the use of radioactive tracers within industry, science and the environment, and the use of radiation to modify materials such as polymers.\nIt also includes the study and use of nuclear processes in non-radioactive areas of human activity. For instance, nuclear magnetic resonance (NMR) spectroscopy is commonly used in synthetic organic chemistry and physical chemistry and for structural analysis in macro-molecular chemistry.\n\nHistory\nAfter Wilhelm Röntgen discovered X-rays in 1895, many scientists began to work on ionizing radiation. One of these was Henri Becquerel, who investigated the relationship between phosphorescence and the blackening of photographic plates. When Becquerel (working in France) discovered that, with no external source of energy, the uranium generated rays which could blacken (or fog) the photographic plate, radioactivity was discovered. Marie Skłodowska-Curie (working in Paris) and her husband Pierre Curie isolated two new radioactive elements from uranium ore. They used radiometric methods to identify which stream the radioactivity was in after each chemical separation; they separated the uranium ore into each of the different chemical elements that were known at the time, and measured the radioactivity of each fraction. They then attempted to separate these radioactive fractions further, to isolate a smaller fraction with a higher specific activity (radioactivity divided by mass). In this way, they isolated polonium and radium. It was noticed in about ",
    "source": "wikipedia",
    "title": "Nuclear chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_22208",
    "text": "Organic chemistry is a subdiscipline within chemistry involving the scientific study of the structure, properties, and reactions of organic compounds and organic materials, i.e., matter in its various forms that contain carbon atoms. Study of structure determines their structural formula. Study of properties includes physical and chemical properties, and evaluation of chemical reactivity to understand their behavior. The study of organic reactions includes the chemical synthesis of natural products, drugs, and polymers, and study of individual organic molecules in the laboratory and via theoretical (in silico) study.\nThe range of chemicals studied in organic chemistry includes hydrocarbons (compounds containing only carbon and hydrogen) as well as compounds based on carbon, but also containing other elements, especially oxygen, nitrogen, sulfur, phosphorus (included in many biochemicals) and the halogens. Organometallic chemistry is the study of compounds containing carbon–metal bonds.\nOrganic compounds form the basis of all earthly life and constitute the majority of known chemicals. The bonding patterns of carbon, with its valence of four—formal single, double, and triple bonds, plus structures with delocalized electrons—make the array of organic compounds structurally diverse, and their range of applications enormous. They form the basis of, or are constituents of, many commercial products including pharmaceuticals; petrochemicals and agrichemicals, and products made from them including lubricants, solvents; plastics; fuels and explosives. The study of organic chemistry overlaps organometallic chemistry and biochemistry, but also with medicinal chemistry, polymer chemistry, and materials science.\n\nEducational aspects\nOrganic chemistry is typically taught at the college or university level. It is considered a very challenging course but has also been made accessible to students.\nHistory\nBefore the 18th century, chemists generally believed that compounds obtained from living organisms were endowed with a vital force that distinguished them from inorganic compounds. According to the concept of vitalism (vital force theory), organic matter was endowed with a \"vital force\".  During the first half of the nineteenth century,  some of the first systematic studies of organic compounds were reported. Around 1816 Michel Chevreul started a study of soaps made from various fats and alkalis. He separated the acids that, in combination with the alkali, produced the soap.  Since these were all individual compounds, he demonstrated that it was possible to make a chemical change in various fats (which traditionally come from organic sources), producing new compounds, without \"vital force\". In 1828 Friedrich Wöhler produced the organic chemical urea (carbamide), a constituent of urine, from inorganic starting materials (the salts potassium cyanate and ammonium sulfate), in what is now called the Wöhler synthesis.  Although Wöhler himself was cautious about claim",
    "source": "wikipedia",
    "title": "Organic chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_5751599",
    "text": "Organolithium chemistry is the science of organolithium compounds describing their physical properties, synthesis, and reactions. Organolithium compounds in organometallic chemistry contain carbon-lithium chemical bonds. A major subset and perhaps the most used organolithium compounds are organolithium reagents, such as butyllithium\n\nClassification\n\n",
    "source": "wikipedia",
    "title": "Organolithium chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_4062960",
    "text": "Particle agglomeration refers to the formation of assemblages in a suspension and represents a mechanism leading to the functional destabilization of colloidal systems. During this process, particles dispersed in the liquid phase stick to each other, and spontaneously form irregular particle assemblages, flocs, or agglomerates. This phenomenon is also referred to as coagulation or flocculation and such a suspension is also called unstable. Particle agglomeration can be induced by adding salts or other chemicals referred to as coagulant or flocculant.\n\nParticle agglomeration can be a reversible or irreversible process. Particle agglomerates defined as \"hard agglomerates\" are more difficult to redisperse to the initial single particles. In the course of agglomeration, the agglomerates will grow in size, and as a consequence they may settle to the bottom of the container, which is referred to as sedimentation. Alternatively, a colloidal gel may form in concentrated suspensions which changes its rheological properties. The reverse process whereby particle agglomerates are re-dispersed as individual particles, referred to as peptization, hardly occurs spontaneously, but may occur under stirring or shear.\nColloidal particles may also remain dispersed in liquids for long periods of time (days to years). This phenomenon is referred to as colloidal stability and such a suspension is said to be functionally stable. Stable suspensions are often obtained at low salt concentrations or by addition of chemicals referred to as stabilizers or stabilizing agents. The stability of particles, colloidal or otherwise, is most commonly evaluated in terms of zeta potential. This parameter provides a readily quantifiable measure of interparticle repulsion, which is the key inhibitor of particle aggregation.\nSimilar agglomeration processes occur in other dispersed systems too. In emulsions, they may also be coupled to droplet coalescence, and not only lead to sedimentation but also to creaming. In aerosols, airborne particles may equally aggregate and form larger clusters (e.g., soot).\n\nEarly stages\nA well dispersed colloidal suspension consists of individual, separated particles and is stabilized by repulsive inter-particle forces. When the repulsive forces weaken or become attractive through the addition of a coagulant, particles start to aggregate. Initially, particle doublets A2 will form from singlets A1 according to the scheme\n\n  \n    \n      \n        \n          \n            A\n            \n              1\n            \n            \n              \n            \n          \n          +\n          \n            A\n            \n              1\n            \n            \n              \n            \n          \n          ⟶\n          \n            A\n            \n              2\n            \n            \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\ce {A1 + A1 -> A2}}}\n  \n\nIn the early stage of the aggregation process, the suspension mainly contain",
    "source": "wikipedia",
    "title": "Particle aggregation",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_35696465",
    "text": "Particle deposition is the spontaneous attachment of particles to surfaces. The particles in question are normally colloidal particles, while the surfaces involved may be planar, curved, or may represent particles much larger in size than the depositing ones (e.g., sand grains). Deposition processes may be triggered by appropriate hydrodynamic flow conditions and favorable particle-surface interactions. Depositing particles may just form a monolayer which further inhibits additional particle deposition, and thereby one refers to surface blocking. Initially attached particles may also serve as seeds for further particle deposition, which leads to the formation of thicker particle deposits, and this process is termed as surface ripening or fouling. While deposition processes are normally irreversible, initially deposited particles may also detach. The latter process is known as particle release and is often triggered by the addition of appropriate chemicals or a modification in flow conditions.\nMicroorganisms may deposit to surfaces in a similar fashion as colloidal particles. When macromolecules, such as proteins, polymers or polyelectrolytes attach to surfaces, one rather calls this process adsorption. While adsorption of macromolecules largely resembles particle deposition, macromolecules may substantially deform during adsorption. The present article mainly deals with particle deposition from liquids, but similar process occurs when aerosols or dust deposit from the gas phase.\n\nInitial stages\nA particle may diffuse to a surface in quiescent conditions, but this process is inefficient as a thick depletion layer develops, which leads to a progressive slowing down of the deposition. When particle deposition is efficient, it proceeds almost exclusively in a system under flow. In such conditions, the hydrodynamic flow will transport the particles close to the surface. Once a particle is situated close to the surface, it will attach spontaneously, when the particle-surface interactions are attractive. In this situation, one refers to favorable deposition conditions. When the interaction is repulsive at larger distances, but attractive at shorter distances, deposition will still occur but it will be slowed down. One refers to unfavorable deposition conditions here. The initial stages of the deposition process can be described with the rate equation \n\n  \n    \n      \n        \n          \n            \n              d\n              Γ\n            \n            \n              d\n              t\n            \n          \n        \n        =\n        k\n        c\n      \n    \n    {\\displaystyle {d\\Gamma  \\over dt}=kc}\n  \n\nwhere \n  \n    \n      \n        Γ\n      \n    \n    {\\displaystyle \\Gamma }\n  \n; is the number density of deposited particles, \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n is the time, \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n the particle number concentration, and \n  \n    \n      \n        k\n      \n    \n    {\\displ",
    "source": "wikipedia",
    "title": "Particle deposition",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_54537321",
    "text": "Random sequential adsorption (RSA) refers to a process where particles are randomly introduced in a system, and if they do not overlap any previously adsorbed particle, they adsorb and remain fixed for the rest of the process. RSA can be carried out in computer simulation, in a mathematical analysis, or in experiments. It was first studied by one-dimensional models: the attachment of pendant groups in a polymer chain by Paul Flory, and the car-parking problem by Alfréd Rényi. Other early works include those of Benjamin Widom. In two and higher dimensions many systems have been studied by computer simulation, including in 2d, disks, randomly oriented squares and rectangles, aligned squares and rectangles, various other shapes, etc.\nAn important result is the maximum surface coverage, called the saturation coverage or the packing fraction. On this page we list that coverage for many systems.\n\nThe blocking process has been studied in detail in terms of the random sequential adsorption (RSA) model. The simplest RSA model related to deposition of spherical particles considers irreversible adsorption of circular disks. One disk after another is placed randomly at a surface. Once a disk is placed, it sticks at the same spot, and cannot be removed. When an attempt to deposit a disk would result in an overlap with an already deposited disk, this attempt is rejected. Within this model, the surface is initially filled rapidly, but the more one approaches saturation the slower the surface is being filled. Within the RSA model, saturation is sometimes referred to as jamming. For circular disks, saturation occurs at a coverage of 0.547. When the depositing particles are polydisperse, much higher surface coverage can be reached, since the small particles will be able to deposit into the holes in between the larger deposited particles. On the other hand, rod like particles may lead to much smaller coverage, since a few misaligned rods may block a large portion of the surface.\nFor the one-dimensional parking-car problem, Renyi has shown that the maximum coverage is equal to\n\n  \n    \n      \n        \n          θ\n          \n            1\n          \n        \n        =\n        \n          ∫\n          \n            0\n          \n          \n            ∞\n          \n        \n        exp\n        ⁡\n        \n          (\n          \n            −\n            2\n            \n              ∫\n              \n                0\n              \n              \n                x\n              \n            \n            \n              \n                \n                  1\n                  −\n                  \n                    e\n                    \n                      −\n                      y\n                    \n                  \n                \n                y\n              \n            \n            d\n            y\n          \n          )\n        \n        d\n        x\n        =\n        0.7475979202534\n        …\n      \n    \n    {\\displaystyle \\theta _{1}=\\int _{0}^{\\infty }\\exp \\le",
    "source": "wikipedia",
    "title": "Random sequential adsorption",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_63488647",
    "text": "Phenol sulfur transferase deficiency, in short PST deficiency, is the lack or the reduced activity of the functional enzyme phenol sulfur transferase, which is crucial in the detoxification of mainly phenolic compounds by catalysing the sulfate conjugation of the hydroxyl groups in the toxic phenolic compounds to result in more hydrophilic forms for more efficient excretion. This metabolic disorder was first discovered in the late 1990s by Dr. Rosemary Waring during her researches with autistic children, which also made this deficiency commonly associated to the topics of autism. Mutations in the PST genes account for the genetic causes of the deficiency, of which single nucleotide polymorphism and methylation of promoters are two examples of mutations that respectively cause conformational abnormalities and diminished expressions to the enzyme, resulting in the reduced detoxification of phenolic compounds and regulation of phenolic neurotransmitter. The deficiency may cause symptoms like flushing, tachycardia, and depression, and be a risk factor for disorders like autism, migraine, and cancer, while it also limits the use of phenolic drugs in PST deficient patients. There is currently no drug available for treating PST deficiency. However, some people suffering from PST deficiency have found taking a digestive enzyme supplement containing Xylanase 10 minutes before eating to greatly reduce symptoms.\n\nPhenol sulfur transferase\nPhenol sulfur transferase, in short PST or SULT1, is a subfamily of the enzyme cytosolic sulfotransferases (SULTs) consisting of at least 8 isoforms in humans that catalyze the transfer of sulfuryl group from 3′-phosphoadenosine 5′-phosphosulfate (PAPS) to phenolic compounds, resulting in more hydrophilic products that can be more easily expelled from tissues for excretion. At high concentration, PST could also catalyze the sulfate conjugation of amino groups. This enzyme subfamily, which exists in nearly all human tissues, is important for the detoxification of phenol-containing xenobiotics or endogenous compounds, including the biotransformation of neurotransmitters and drugs.  Its expression is controlled by the PST genes located on chromosomes 2, 4, and 16 depending on the isoform, for example the genes for the predominant isoform throughout the body of human adults, SULT1A1, which is highly heritable and variable between individuals, and the most important one in the nervous system, SULT1A3, are located on chromosome 16 at the position of 16p11.2 to 16p12.1.\n",
    "source": "wikipedia",
    "title": "Phenol sulfur transferase deficiency",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_1403587",
    "text": "The philosophy of chemistry considers the methodology and underlying assumptions of the science of chemistry.  It is explored by philosophers, chemists, and philosopher-chemist teams.  For much of its history, philosophy of science has been dominated by the philosophy of physics, but the philosophical questions that arise from chemistry have received increasing attention since the latter part of the 20th century.\n\nFoundations of chemistry\nMajor philosophical questions arise as soon as one attempts to define chemistry and what it studies.  Atoms and molecules are often assumed to be the fundamental units of chemical theory, but traditional descriptions of molecular structure and chemical bonding fail to account for the properties of many substances, including metals and metal complexes and aromaticity.\nAdditionally, chemists frequently use non-existent chemical entities like resonance structures to explain the structure and reactions of different substances; these explanatory tools use the language and graphical representations of molecules to describe the behavior of chemicals and chemical reactions that in reality do not behave as straightforward molecules.\nSome chemists and philosophers of chemistry prefer to think of substances, rather than microstructures, as the fundamental units of study in chemistry.  There is not always a one-to-one correspondence between the two methods of classifying substances.  For example, many rocks exist as mineral complexes composed of multiple ions that do not occur in fixed proportions or spatial relationships to one another.\nA related philosophical problem is whether chemistry is the study of substances or reactions.  Atoms, even in a solid, are in perpetual motion and under the right conditions many chemicals react spontaneously to form new products.  A variety of environmental variables contribute to a substance's properties, including temperature and pressure, proximity to other molecules and the presence of a magnetic field.  As Schummer puts it, \"Substance philosophers define a chemical reaction by the change of certain substances, whereas process philosophers define a substance by its characteristic chemical reactions.\"\nPhilosophers of chemistry discuss issues of symmetry and chirality in nature. Organic (i.e., carbon-based) molecules are those most often chiral. Amino acids, nucleic acids and sugars, all of which are found exclusively as a single enantiomer in organisms, are the basic chemical units of life. Chemists, biochemists, and biologists alike debate the origins of this homochirality. Philosophers debate facts regarding the origin of this phenomenon, namely whether it emerged contingently, amid a lifeless racemic environment or if other processes were at play. Some speculate that answers can only be found in comparison to extraterrestrial life, if it is ever found. Other philosophers question whether there exists a bias toward assumptions of nature as symmetrical, thereby causing resistance to an",
    "source": "wikipedia",
    "title": "Philosophy of chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_64063915",
    "text": "A phosphorimidazolide is a chemical compound in which a phosphoryl mono-ester is covalently bound to a nitrogen atom in an imidazole ring. They are a type of phosphoramidate. These phosphorus (V) compounds are encountered as reagents used for making new phosphoanhydride bonds with phosphate mono-esters, and as reactive intermediates in phosphoryl transfer reactions in some enzyme-catalyzed transformations. They are also being studied as critical chemical intermediates for the polymerization of nucleotides in pre-biotic settings. They are sometimes referred to as phosphorimidazolidates, imidazole-activated phosphoryl groups, and P-imidazolides.\n\nRole in oligonucleotide formation\nPhosphorimidazolides have been investigated for their mechanistic role in abiogenesis (the natural process by which life arose from non-living matter). Specifically, they have been proposed as the active electrophilic species which may have mediated the formation of inter-nucleotide phosphodiester bonds, thereby enabling template-directed oligonucleotide replication before the advent of enzymes. Phosphorimidazolides were originally proposed as mediators of this process by Leslie Orgel in 1968. Early studies showed that divalent metal cations such as Mg2+, Zn2+, and Pb2+ and a complementary template were required for the formation of short oligonucleotides, although nucleotides exhibited 5'-2' connectivity instead of 5'-3' connectivity of present-day life forms. It was also shown that Montmorillonite clay could provide a surface for phosphorimidazolide-mediated oligonucleotide formation with lengths of 20-50 bases.\nThe research group of Jack W. Szostak has continued to investigate the role of phosphorimidazolides in pre-biotic nucleotide polymerization. The group has investigated a number of imidazole derivatives in the search for chemical moieties which provide longer oligonucleotides necessary for propagating genetic information. Significantly, they discovered that phosphorimidazolides promote template-directed oligonucleotide formation via imidazolium-bridged dinucleotide intermediates.\nJohn D. Sutherland and colleagues have proposed that phosphorimidazolides may have formed in the chemical environment of early Earth via the activation of ribonucleotide phosphates by methyl isocyanaide and acetaldehyde followed by substitution with imidazole.\n",
    "source": "wikipedia",
    "title": "Phosphorimidazolide",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_363430",
    "text": "Photochemistry is the branch of chemistry concerned with the chemical effects of light. Generally, this term is used to describe a chemical reaction caused by absorption of ultraviolet (wavelength from 100 to 400 nm), visible (400–750 nm), or infrared radiation (750–2500 nm).\nIn nature, photochemistry is of immense importance as it is the basis of photosynthesis, vision, and the formation of vitamin D with sunlight. It is also responsible for the appearance of DNA mutations leading to skin cancers.\nPhotochemical reactions proceed differently than temperature-driven reactions. Photochemical paths access high-energy intermediates that cannot be generated thermally, thereby overcoming large activation barriers in a short period of time, and allowing reactions otherwise inaccessible by thermal processes.  Photochemistry can also be destructive, as illustrated by the photodegradation of plastics.\n\nConcepts\nPhotoexcitation is the first step in a photochemical process: the reactant is elevated to a state of higher energy, an excited state.\nPhotochemical reactions\n\nSee also\nPhotonic molecule\nPhotoelectrochemical cell\nPhotochemical logic gate\nPhotosynthesis\nLight-dependent reactions\nList of photochemists\nSingle-photon source\nPhotogeochemistry\nPhotoelectric effect\nPhotolysis\nBlueprint\nReferences\n\nFurther reading\nBowen, E. J., Chemical Aspects of Light. Oxford: The Clarendon Press, 1942. 2nd edition, 1946.\nPhotochemistry\n",
    "source": "wikipedia",
    "title": "Photochemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_51934343",
    "text": "Photopharmacology is an emerging multidisciplinary field that combines photochemistry and pharmacology. Built upon the ability of light to change the pharmacokinetics and pharmacodynamics of bioactive molecules, it aims at regulating the activity of drugs in vivo by using light. The light-based modulation is achieved by incorporating molecular photoswitches such as azobenzene and diarylethenes or photocages such as o-nitrobenzyl, coumarin, and BODIPY compounds into the pharmacophore. This selective activation of the biomolecules helps prevent or minimize off-target activity and systemic side effects. Moreover, light being the regulatory element offers additional advantages such as the ability to be delivered with high spatiotemporal precision, low to negligible toxicity, and the ability to be controlled both qualitatively and quantitatively by tuning its wavelength and intensity.\n\nHistory\nThough photopharmacology is a relatively new field, the concept of using light in therapeutic applications came into practice a few decades ago. Photodynamic therapy (PDT) is a well-established clinically practiced protocol in which photosensitizers are used to produce singlet oxygen for destroying diseased or damaged cells or tissues. Optogenetics is another method that relies on light for dynamically controlling biological functions especially brain and neural. Though this approach has proven useful as a research tool, its clinical implementation is limited by the requirement for genetic manipulation. Mainly, these two techniques laid the foundation for photopharmacology. Today, it is a rapidly evolving field with diverse applications in both basic research and clinical medicine which has the potential to overcome some of the challenges limiting the range of applications of the other light-guided therapies.\nFigure 1. Schematic representation of the mechanism of (a) photopharmacology (b) photodynamic therapy, and (c) optogenetics.\nThe discovery of natural photoreceptors such as rhodopsins in the eye inspired the biomedical and pharmacology research community to engineer light-sensitive proteins for therapeutic applications. The development of synthetic photoswitchable molecules is the most significant milestone in the history of light-delivery systems. Scientists are continuing with their efforts to explore new photoswitches and delivery strategies with enhanced performance to target different biological molecules such as ion channels, nucleic acid, and enzyme receptors. Photopharmacology research progressed from in vitro to in vivo studies in a significantly short period of time yielding promising results in both forms. Clinical trials are underway to assess the safety and efficacy of these photopharmacological therapies further and validate their potential as an innovative drug delivery approach.\n",
    "source": "wikipedia",
    "title": "Photopharmacology",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_1246630",
    "text": "Phytochemistry is the study of phytochemicals, which are chemicals derived from plants. Phytochemists strive to describe the structures of the large number of secondary metabolites found in plants, the functions of these compounds in human and plant biology, and the biosynthesis of these compounds. Plants synthesize phytochemicals for many reasons, including to protect themselves against insect attacks and plant diseases. The compounds found in plants are of many kinds, but most can be grouped into four major biosynthetic classes: alkaloids, phenylpropanoids, polyketides, and terpenoids.\nPhytochemistry can be considered a subfield of botany or chemistry. Activities can be led in botanical gardens or in the wild with the aid of ethnobotany. Phytochemical studies directed toward human (i.e. drug discovery) use may fall under the discipline of pharmacognosy, whereas phytochemical studies focused on the ecological functions and evolution of phytochemicals likely fall under the discipline of chemical ecology. Phytochemistry also has relevance to the field of plant physiology.\n\nTechniques\nTechniques commonly used in the field of phytochemistry are extraction, isolation, and structural elucidation (MS,1D and 2D NMR) of natural products, as well as various chromatography techniques (MPLC, HPLC, and LC-MS).\nPhytochemicals\nMany plants produce chemical compounds for defence against herbivores. The major classes of pharmacologically active phytochemicals are described below, with examples of medicinal plants that contain them. Human settlements are often surrounded by weeds containing phytochemicals, such as nettle, dandelion and chickweed.\nMany phytochemicals, including curcumin, epigallocatechin gallate, genistein, and resveratrol are pan-assay interference compounds and are not useful in drug discovery.\nGenetics\nContrary to bacteria and fungi, most plant metabolic pathways are not grouped into biosynthetic gene clusters, but instead are scattered as individual genes. Some exceptions have been discovered: steroidal glycoalkaloids in Solanum, polyketides in Pooideae, benzoxazinoids in Zea mays, triterpenes in Avena sativa, Cucurbitaceae, Arabidopsis, and momilactone diterpenes in Oryza sativa.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Phytochemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_43939378",
    "text": "Pressure-induced hydration (PIH), also known as “super-hydration”, is a special case of pressure-induced insertion whereby water molecules are injected into the pores of microporous materials. In PIH, a microporous material is placed under pressure in the presence of water in the pressure-transmitting fluid of a diamond anvil cell.\nEarly physical characterization and initial diffraction experiments in zeolites were followed by the first unequivocal structural characterization of PIH in the small-pore zeolite natrolite (Na16Al16Si24O80·16H2O), which in its fully super-hydrated form, Na16Al16Si24O80·32H2O, doubles the amount of water it contains in its pores.\nPIH has now been demonstrated in natrolites containing Li, K, Rb and Ag as monovalent cations as well as in large-pore zeolites, pyrochlores, clays and graphite oxide.\nUsing the noble gases Ar, Kr, and Xe as well as CO2 as pressure-transmitting fluids, researchers have prepared and structurally characterized the products of reversible, pressure-induced insertion of Ar Kr, and CO2 as well as the irreversible insertion of Xe and water.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Pressure-induced hydration",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_77310246",
    "text": "Probico also known as PBCo, is a comprehensive formulation consisting of proanthocyanidin, biotin, and coenzyme Q10. Probico addresses three primary causes of hair loss: amino acid deficiency, inadequate hair synthesis, and follicle inflammation, excluding the hormonal imbalance and blood circulation issues which require prescription medications.\n\nChemical description\nThe key characteristics of the components of Probico are as follows:\n",
    "source": "wikipedia",
    "title": "Probico",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_23568658",
    "text": "Radioanalytical chemistry focuses on the analysis of sample for their radionuclide content.  Various methods are employed to purify and identify the radioelement  of interest through chemical methods and sample measurement techniques.\n\nHistory\nThe field of radioanalytical chemistry was originally developed by Marie Curie with contributions by Ernest Rutherford and Frederick Soddy.  They developed chemical separation and radiation measurement techniques on terrestrial radioactive substances.  During the twenty years that followed 1897 the concepts of radionuclides was born.  Since Curie's time, applications of radioanalytical chemistry have proliferated.  Modern advances in nuclear and radiochemistry research have allowed practitioners to apply chemistry and nuclear procedures to elucidate nuclear properties and reactions, used radioactive substances as tracers, and measure radionuclides in many different types of samples.\nThe importance of radioanalytical chemistry spans many fields including chemistry, physics, medicine, pharmacology, biology, ecology, hydrology, geology, forensics, atmospheric sciences, health protection, archeology, and engineering.  Applications include: forming and characterizing new elements, determining the age of materials, and creating radioactive reagents for specific tracer use in tissues and organs.  The ongoing goal of radioanalytical researchers is to develop more radionuclides and lower concentrations in people and the environment.\nRadiation decay modes\n\nRadiation detection principles\n\nChemical separation techniques\nDue to radioactive nucleotides have similar properties to their stable, inactive, counterparts similar analytical chemistry separation techniques can be used.  These separation methods include precipitation, Ion Exchange, Liquid Liquid extraction, Solid Phase extraction, Distillation, and Electrodeposition.\nRadioanalytical chemistry principles\n\nTypical radionuclides of interest\n\nQuality assurance\nAs this is an analytical chemistry technique quality control is an important factor to maintain.  A laboratory must produce trustworthy results.  This can be accomplished by a laboratories continual effort to maintain instrument calibration, measurement reproducibility, and applicability of analytical methods.  In all laboratories there must be a quality assurance plan.  This plan describes the quality system and procedures in place to obtain consistent results.  Such results must be authentic, appropriately documented, and technically defensible.\"  Such elements of quality assurance include organization, personnel training, laboratory operating procedures, procurement documents, chain of custody records, standard certificates, analytical records, standard procedures, QC sample analysis program and results, instrument testing and maintenance records, results of performance demonstration projects, results of data assessment, audit reports, and record retention policies.  \nThe cost of quality assurance is continu",
    "source": "wikipedia",
    "title": "Radioanalytical chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_49301338",
    "text": "Rayleigh fractionation describes the evolution of a system with multiple phases in which one phase is continuously removed from the system through fractional distillation. It is used in particular to describe isotopic enrichment or depletion as material moves between reservoirs in an equilibrium process. Rayleigh fractionation holds particular importance in hydrology and meteorology as a model for the isotopic differentiation of meteoric water due to condensation.\n\nThe Rayleigh equation\nThe original Rayleigh equation was derived by Lord Rayleigh for the case of fractional distillation of mixed liquids. \nThis is an exponential relation that describes the partitioning of isotopes between two reservoirs as one reservoir decreases in size. The equations can be used to describe an isotope fractionation process if: (1) material is continuously removed from a mixed system containing molecules of two or more isotopic species (e.g., water with 18O and 16O, or sulfate with 34S and 32S), (2) the fractionation accompanying the removal process at any instance is described by the fractionation factor a, and (3) a does not change during the process. Under these conditions, the evolution of the isotopic composition in the residual (reactant) material is described by:\n\n  \n    \n      \n        \n          \n            R\n            \n              R\n              \n                0\n              \n            \n          \n        \n        =\n        \n          \n            (\n            \n              \n                X\n                \n                  X\n                  \n                    0\n                  \n                \n              \n            \n            )\n          \n          \n            a\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle {\\frac {R}{R^{0}}}=\\left({\\frac {X}{X^{0}}}\\right)^{a-1}}\n  \n\nwhere R = ratio of the isotopes (e.g., 18O/16O) in the reactant, R0 = initial ratio, X = the concentration or amount of the more abundant (lighter) isotope (e.g.,16O), and X0 = initial concentration. Because the concentration of X >> Xh (heavier isotope concentration), X is approximately equal to the amount of original material in the phase. Hence, if \n  \n    \n      \n        f\n        =\n        X\n        \n          /\n        \n        \n          X\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle f=X/X^{0}}\n  \n = fraction of material remaining, then:\n\n  \n    \n      \n        R\n        =\n        \n          R\n          \n            0\n          \n        \n        \n          f\n          \n            a\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle R=R^{0}f^{a-1}}\n  \n\nFor large changes in concentration, such as they occur during e.g. distillation of heavy water, these formulae need to be integrated over the distillation trajectory. For small changes such as occur during transport of water vapour through the atmosphere, the differentiated equation will usually be sufficient.\n",
    "source": "wikipedia",
    "title": "Rayleigh fractionation",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_45413683",
    "text": "The scale of a chemical process refers to the rough ranges in mass or volume of a chemical reaction or process that define the appropriate category of chemical apparatus and equipment required to accomplish it, and the concepts, priorities, and economies that operate at each. While the specific terms used—and limits of mass or volume that apply to them—can vary between specific industries, the concepts are used broadly across industry and the fundamental scientific fields that support them. Use of the term \"scale\" is unrelated to the concept of weighing; rather it is related to cognate terms in mathematics (e.g., geometric scaling, the linear transformation that enlarges or shrinks objects, and scale parameters in probability theory), and in applied areas (e.g., in the scaling of images in architecture, engineering, cartography, etc.).\nPractically speaking, the scale of chemical operations also relates to the training required to carry them out, and can be broken out roughly as follows:\n\nprocedures performed at the laboratory scale, which involve the sorts of procedures used in academic teaching and research laboratories in the training of chemists and in discovery chemistry venues in industry,\noperations at the pilot plant scale, e.g., carried out by process chemists, which, though at the lowest extreme of manufacturing operations, are on the order of 200- to 1000-fold larger than laboratory scale, and used to generate information on the behavior of each chemical step in the process that might be useful to design the actual chemical production facility;\nintermediate bench scale sets of procedures, 10- to 200-fold larger than the discovery laboratory, sometimes inserted between the preceding two;\noperations at demonstration scale and full-scale production, whose sizes are determined by the nature of the chemical product, available chemical technologies, the market for the product, and manufacturing requirements, where the aim of the first of these is literally to demonstrate operational stability of developed manufacturing procedures over extended periods (by operating the suite of manufacturing equipment at the feed rates anticipated for commercial production).\nFor instance, the production of the streptomycin-class of antibiotics, which combined biotechnologic and chemical operations, involved use of a 130,000 liter fermenter, an operational scale approximately one million-fold larger than the microbial shake flasks used in the early laboratory scale studies.\nAs noted, nomenclature can vary between manufacturing sectors; some industries use the scale terms pilot plant and demonstration plant  interchangeably.\nApart from defining the category of chemical apparatus and equipment required at each scale, the concepts, priorities and economies that obtain, and the skill-sets needed by the practicing scientists at each, defining scale allows for theoretical work prior to actual plant operations (e.g., defining relevant process parameters used in the n",
    "source": "wikipedia",
    "title": "Scale (chemistry)",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_69512983",
    "text": "The School of Molecular Sciences is an academic unit of The College of Liberal Arts and Sciences at Arizona State University (ASU). The School of Molecular Sciences (SMS) is responsible for the study and teaching of the academic disciplines of chemistry and biochemistry at ASU.\n\nHistory\nChemistry instruction at ASU can be traced back to the early 1890s. At that time, the educational institution, a Normal School for the Territory of Arizona, “acquired...a supply of chemicals” for instructional purposes. Chemistry classes were held in Old Main during the late 1800s and into the early 1900s, taught by Frederick M. Irish.\nIn 1927, President Arthur John Matthews hired George Bateman, the first faculty to hold a PhD who was not also a principal or president of the school. Bateman taught chemistry classes, among other things, for forty years. He oversaw the development of the physical sciences at ASU, including new science facilities and degrees.\nIn 1946, new majors leading to degrees were added, including Physical and Biological Science. In 1947 the State of AZ designated $525,000 for a new science building.\nIn 1953 the first college, the College of Arts and Sciences was established with 14 departments. In 1954 Arizona State College was restructured into 4 colleges, which went into effect in the 1955–56 academic year: the College of Liberal Arts, the College of Education, the College of Applied Arts and Sciences, and the College of Business and Public Administration.\nIn 1957, the Department of Chemistry first appeared in the Arizona State College Bulletin (Vol. LXXII No. 2, April 1957), listed under the Division of Physical Sciences. Early chemists, such as LeRoy Eyring helped build ASU's strong science reputation; Roland K. Robins conducted cancer research as early as 1957.\nIn 1958, Arizona State College was renamed Arizona State University. Chemistry was the first department to be approved to offer a doctoral degree.\nIn 1960, George Boyd, the university's first coordinator of research, helped secure a portion of Harvey H. Nininger’s meteorites for ASU, making it the largest university-based meteorite collection in the world.\nIn 1961, Geochemist Carleton B. Moore became the first director of the Center for Meteorite Studies, which at the time was housed in the Department of Chemistry.\nIn 1963, Peter R. Buseck, who pioneered high-resolution transmission electron microscopy (TEM) research on meteorites and terrestrial minerals.\nIn 1963, ASU awarded its first doctoral degrees to four students, one of whom, Jesse W. Jones, was the first Chemistry PhD of ASU and the first African American to earn a PhD at ASU. Jones went on to teach chemistry at Baylor University for over 30 years.\nIn 1965 Robert Pettit was hired and began developing marine-organism research that led to the creation of anti-cancer drugs and, in 1973, what became the Cancer Research Institute. Pettit taught at ASU until his retirement in 2021.\nIn 1967, George Bateman, after enjoying a produ",
    "source": "wikipedia",
    "title": "School of Molecular Sciences",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_75786010",
    "text": "The shape of the atomic nucleus depends on the variety of factors related to the size and shape of its nucleon (proton or neutron) constituents and the nuclear force holding them together. The spatial extent of the prolate spheroid  nucleon (and larger nuclides) is determined by root mean squared (RMS) charge radius of the proton, as determined mainly by electron and muon scattering experiments, as well as spectroscopic experiments. An important factor in the internal structure of the nucleus is the nucleon-nucleon potential, which ultimately governs the distance between individual nucleons, and the radial charge density of each nuclide. The charge density of some light nuclide indicates a lesser density of nucleonic matter in the center which may have implications for a nucleonic nuclear structure. A surprising non-spherical expectation for the shape of the nucleus originated in 1939 in the spectroscopic analysis of the quadrupole moments  while the prolate spheroid shape of the nucleon arises from analysis of the intrinsic quadruple moment. The simple spherical approximation of nuclear size and shape provides at best a textbook introduction to nuclear size and shape. The unusual cosmic abundance of alpha nuclides has inspired geometric arrangements of alpha particles as a solution to nuclear shapes, although the atomic nucleus generally assumes a prolate spheroid shape. Nuclides can also be discus-shaped (oblate deformation), triaxial (shape of an ellipsoid with its three principal axes having different lengths) or pear-shaped.\n\nOrigins of nuclear shape\nThe atomic nucleus is composed of protons and neutrons (collectively called nucleons). In the Standard Model of particle physics, nucleons are in the group called hadrons, the smallest known particles in the universe to have measurable size and shape. Each is in turn composed of three quarks. The spatial extent and shape of nucleons (and nuclides assembled from them) ultimately involves quark interactions within and between nucleons. The quark itself does not have measurable size at the experimental limit set by the electron (≈ 10−18 m in diameter). The size, or RMS charge radius, of the proton (the smallest nuclide) has a 2018 CODATA recommended value of 0.8414 (19) fm (10−15 m), although values may vary by a few percent according to the experimental method employed (see proton radius puzzle). Nuclide size ranges up to ≈ 6 fm. The largest stable nuclide, lead-208, has an RMS charge radius of 5.5012 fm, and the largest unstable nuclide americium-243 has an experimental RMS charge radius of 5.9048 fm.  The main source of nuclear radius values derives from elastic scattering experiments (electron and muon), but nuclear radii data also come from experiments on spectroscopic isotope shifts (x-ray and optical), β decay by mirror nuclei, α decay, and neutron scattering. Although the radius values delimit the spatial extent of the nucleus, spectroscopic and scattering experiments dating back to 1935 in",
    "source": "wikipedia",
    "title": "Shape of the atomic nucleus",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_80065940",
    "text": "Single-cell nanoencapsulation (SCNE) is an interdisciplinary research field at the intersection of chemistry, biology, nanoscience, and materials science. Single-cell nanoencapsulation involves the development and application of nanometer-scaled shells for the isolation, protection, and functionalization of individual living cells. Single-cell nanoencapsulation enables the fundamental studies of cell–material interactions at the single-cell level, and supports research and development across a range of applied fields, including cell therapy, renewable energy, regenerative medicine, probiotics, and agricultural innovation. By controlling the cellular microenvironment at the nanoscale, single-cell nanoencapsulation allows for fine-tuned investigation of individual cell responses and the design of engineered cellular systems with tailored properties.\nSingle-cell nanoencapsulation is also a chemical strategy that creates \"cell-in-shell\" structures by forming artificial nanoshells (typically <100 nm in thickness) on individual cells. The cell-in-shell structures are referred to by various names depending on the context or application, including artificial spores, cyborg cells, Supracells, micrometric Iron Men, and micrometric Transformers.\nSingle-cell nanoencapsulation is considered complementary or, in some contexts superior to, cell microencapsulation techniques. Single-cell nanoencapsulation enables precise modulation and control of cellular behavior at the single-cell level by encapsulating individual cells within artificial nanoshells composed of organic, inorganic, or hybrid materials.\nThe term \"SCNE\" is also used as a verb in scientific literature, with phrases such as \"SCNEd cells\" referring to the cells that have gone through the process of single-cell nanoencapsulation.\nNanoshell properties for artificial spores have been proposed:\n\ndurability: The nanoshell should be mechanically and (bio)chemically robust, capable of withstanding external stresses such as osmotic pressure and dehydration while preserving its structure. The durability could also enable control over cell growth and division by resisting internal biological forces.\npermselectivity: The porosity of the artificial shell should be chemically tunable to allow the selective exchange of small molecules—such as gases and nutrients—while blocking harmful agents like lytic enzymes and macrophages, thereby supporting cell viability.\ndegradability: The shell should be designed to degrade on demand in a stimulus-responsive manner. Controlled chemical breakdown enables the restoration or activation of the nanoencapsulated cell's biological functions after chemical germination.\nfunctionalizability: The nanoshell should allow for chemical modification either during or after formation without compromising cell viability, enabling functional augmentation as well as specific recognition and interaction with the external environment.\n\nShell materials\n\n",
    "source": "wikipedia",
    "title": "Single-cell nanoencapsulation",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_80483995",
    "text": "Sinner's circle is a concept used in the field of cleaning and detergents. It was introduced by the German chemist Herbert Sinner in 1959, who was working for the detergent manufacturer Henkel at the time. This model, represented as a circle with four quadrants, describes the four main factors that influence cleaning effectiveness.\nThe four elements of the Sinner circle are:\n\nChemistry: This refers to the solvents or detergents used for cleaning. Chemical agents play a crucial role in helping to dissolve dirt and facilitate its removal.\nTemperature: Heat can speed up chemical reactions and help dissolve grease and other residues more effectively.\nMechanical: This involves mechanical agitation or rubbing used to dislodge dirt.\nTime: The length of time the cleaning agents are in contact with the surface to be cleaned.\nAccording to Sinner, cleaning efficiency can be improved by optimizing these four factors. For example, if one cannot use a very strong detergent for safety or cost reasons, one can compensate by increasing the temperature, extending the cleaning time, or using more mechanical agitation.\nLater research has helped to more clearly describe the interactions between Sinner's four factors in real cleaning scenarios.\nA fifth element, water, is sometimes added as an inner circle.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Sinner's circle",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_26786438",
    "text": "Soft chemistry (also known as chimie douce) is a type of chemistry that uses reactions at ambient temperature in open reaction vessels with reactions similar to those occurring in biological systems.\n\nAims\nThe aim of the soft chemistry is to synthesize materials, drawing capacity of living beings - more or less basic - such as diatoms capable of producing glass from silicates dissolved. It is a new branch of materials science that differs from conventional solid-state chemistry and its application to the intense energy to explore the chemical inventiveness of the living world. This specialty emerged in the 1980s around the label of \"chimie douce\", which was first published by the French chemist, Jacques Livage in Le Monde, 26 October 1977. French hits, the term soft chemistry is employed as such in the early twenty-first century  in scientific publications, English and others. His mode of synthesis is similar generally for reactions involved in the polymerizations based on organic and the establishment of solutions reactive energy intake without essential polycondensation. The fundamental interest of this kind of polymerization mineral obtained at room temperature is to preserve organic molecules or microorganisms that wishes to fit. The products obtained by means of the so-called soft chemistry sol-gel can be stored in several types:\n\nmineral structures of various qualities (smoothness, uniformity, etc.)\nmixed structures combining inorganic and organic molecules on mineral structures\nwrapper complex molecules and even microorganisms maintaining or optimizing their beneficial characteristics.\nThe early results have included the creation of glasses and ceramic with new properties. These different structures are more or less composite mobilized a wide range of applications ranging from health to the needs of the conquest of space. Beyond its mode of synthesis, a compound with the label soft chemistry combines the advantages of the mineral (resistance, transparency, repetition patterns, etc.) and now exploring the potential of the biochemistry and organic chemistry (interface with the organic world, reactivity, synthesis capability, etc.). According to its practitioners, \"soft chemistry\" is only in its early success and opens up vast prospects.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Soft chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_64345812",
    "text": "A solid-state electrolyte (SSE) is a solid ionic conductor and electron-insulating material and it is the characteristic component of the solid-state battery. It is useful for applications in electrical energy storage in substitution of the liquid electrolytes found in particular in the lithium-ion battery. Their main advantages are their absolute safety, no issues of leakages of toxic organic solvents, low flammability, non-volatility, mechanical and thermal stability, easy processability, low self-discharge, higher achievable power density and cyclability. \nThis makes possible, for example, the use of a lithium metal anode in a practical device, without the intrinsic limitations of a liquid electrolyte thanks to the property of lithium dendrite suppression in the presence of a solid-state electrolyte membrane. The use of a high-capacity and low reduction potential anode, like lithium with a specific capacity of 3860 mAh/g and a reduction potential of -3.04 V vs standard hydrogen electrode, in substitution of the traditional low capacity graphite, which exhibits a theoretical capacity of 372 mAh/g in its fully lithiated state of LiC6, is the first step in the realization of a lighter, thinner and cheaper rechargeable battery. This allows for gravimetric and volumetric energy densities high enough to achieve 500 miles per single charge in an electric vehicle. Despite these promising advantages, there are still many limitations that are hindering the transition of SSEs from academic research to large-scale production, mainly the poor ionic conductivity compared to that of liquid counterparts. However, many car OEMs (Toyota, BMW, Honda, Hyundai) expect to integrate these systems into viable devices and to commercialize solid-state battery-based electric vehicles by 2025.\n\nHistory\nThe first inorganic solid-state electrolytes were discovered by Michael Faraday in the nineteenth century, these being silver sulfide (Ag2S) and lead(II) fluoride (PbF2). The first polymeric material able to conduct ions at the solid-state was PEO, discovered in the 1970s by V. Wright. The importance of the discovery was recognized in the early 1980s.\nHowever, unresolved fundamental issues remain in order to fully understand the behavior of all-solid batteries, especially in the area of electrochemical interfaces. In recent years the needs of safety and performance improvements with respect to the state-of-the-art Li-ion chemistry are making solid-state batteries very appealing and are now considered an encouraging technology to satisfy the need for long range battery electric vehicles of the near future.\nIn March 2020, the Samsung Advanced Institute of Technology (SAIT) published research on an all-solid-state battery (ASSB) using an argyrodite-based solid-state electrolyte with a demonstrated energy density of 900 Wh L−1 and a stable cyclability of more than 1000 cycles, reaching for the first time a value close to the 1000 Wh L−1. This was achieved by using warm isostat",
    "source": "wikipedia",
    "title": "Solid-state electrolyte",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_3173180",
    "text": "In chemistry, the study of sonochemistry is concerned with understanding the effect of ultrasound in forming acoustic cavitation in liquids, resulting in the initiation or enhancement of the chemical activity in the solution. Therefore, the chemical effects of ultrasound do not come from a direct interaction of the ultrasonic sound wave with the molecules in the solution.\n\nHistory\nThe influence of sonic waves travelling through liquids was first reported by Robert Williams Wood (1868–1955) and Alfred Lee Loomis (1887–1975) in 1927. The experiment was about the frequency of the energy that it took for sonic waves to \"penetrate\" the barrier of water. He came to the conclusion that sound does travel faster in water, but because of the water's density compared to Earth's atmosphere it was incredibly hard to get the sonic waves to couple their energy into the water. Due to the sudden density change, much of the energy is lost, similar to shining a flashlight towards a piece of glass; some of the light is transmitted into the glass, but much of it is lost to reflection outwards. Similarly with an air-water interface, almost all of the sound is reflected off the water, instead of being transmitted into it. After much research they decided that the best way to disperse sound into the water was to create bubbles at the same time as the sound. Another issue was the ratio of the amount of time it took for the lower frequency waves to penetrate the bubbles walls and access the water around the bubble, compared to the time from that point to the point on the other end of the body of water. But despite the revolutionary ideas of this article it was left mostly unnoticed.   Sonochemistry experienced a renaissance in the 1980s with the advent of inexpensive and reliable generators of high-intensity ultrasound, most based around piezoelectric elements.\nPhysical principles\nSound waves propagating through a liquid at ultrasonic frequencies have wavelengths many times longer than the molecular dimensions or the bond length between atoms in the molecule. Therefore, the sound wave cannot directly affect the vibrational energy of the bond, and can therefore not directly increase the internal energy of a molecule. Instead, sonochemistry arises from acoustic cavitation: the formation, growth, and implosive collapse of bubbles in a liquid. The collapse of these bubbles is an almost adiabatic process, thereby resulting in the massive build-up of energy inside the bubble, resulting in extremely high temperatures and pressures in a microscopic region of the sonicated liquid. The high temperatures and pressures result in the chemical excitation of any matter within or very near the bubble as it rapidly implodes. A broad variety of outcomes can result from acoustic cavitation including sonoluminescence, increased chemical activity in the solution due to the formation of primary and secondary radical reactions, and increased chemical activity through the formation of new, relat",
    "source": "wikipedia",
    "title": "Sonochemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_72418328",
    "text": "Stable and persistent phosphorus radicals are phosphorus-centred radicals that are isolable and can exist for at least short periods of time. Radicals consisting of main group elements are often very reactive and undergo uncontrollable reactions, notably dimerization and polymerization. The common strategies for stabilising these phosphorus radicals usually include the delocalisation of the unpaired electron over a pi system or nearby electronegative atoms, and kinetic stabilisation with bulky ligands. Stable and persistent phosphorus radicals can be classified into three categories: neutral, cationic, and anionic radicals. Each of these classes involve various sub-classes, with neutral phosphorus radicals being the most extensively studied. Phosphorus exists as one isotope 31P (I = 1/2) with large hyperfine couplings relative to other spin active nuclei, making phosphorus radicals particularly attractive for spin-labelling experiments.\n\nNeutral phosphorus radicals\nNeutral phosphorus radicals include a large range of conformations with varying spin densities at the phosphorus. Generally, they can categorised as mono- and bi/di-radicals (also referred to as bisradicals and biradicaloids) for species containing one or two radical phosphorus centres respectively.\nPhosphorus radical cations\n\nPhosphorus radical anions\n\nFurther reading\n\n",
    "source": "wikipedia",
    "title": "Stable phosphorus radicals",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_28756",
    "text": "Stereochemistry, a subdiscipline of chemistry, studies the spatial arrangement of atoms that form the structure of molecules and their manipulation. The study of stereochemistry focuses on the relationships between stereoisomers, which are defined as having the same molecular formula and sequence of bonded atoms (constitution) but differing in the geometric positioning of the atoms in space. For this reason, it is also known as 3D chemistry—the prefix \"stereo-\" means \"three-dimensionality\" because many of the types of stereochemistry are based on 3D geometric relationships. Stereochemistry applies to all kinds of compounds and ions,  organic and inorganic species alike.  Stereochemistry affects biological, physical, and supramolecular chemistry. \nStereochemistry also studies the reactivity of the molecules in question (dynamic stereochemistry).\nCahn–Ingold–Prelog priority rules are part of a system for describing a molecule's stereochemistry. They rank the atoms around a stereochemical region of a molecule in a standard way, allowing unambiguous descriptions of their relative positions in the molecule.\n\nVisual representations\nRather than using a high-quality 3D rendering a molecule, there are several simplified standard ways of representing the 3D positioning of atoms around a stereocenter. One common convention uses a bond drawn as a solid wedge to indicate that a bond that is projecting towards the viewer, a dashed or hashed bond to indicate that a bond is receding away from the viewer, and plain lines to represent bonds that are in the plane of the molecule itself.\nA Fischer projection represents the four directions around a tetrahedral atom by drawing the bonds horizontally or vertically, with vertical meaning the bond recedes away from the viewer and horizontal meaning the bond projects towards the viewer.\nThalidomide example\nStereochemistry has important applications in the field of medicine, particularly pharmaceuticals. An often cited example of the importance of stereochemistry relates to the thalidomide disaster. Thalidomide is a pharmaceutical drug, first prepared in 1957 in Germany,  prescribed for treating morning sickness in pregnant women. The drug was discovered to be teratogenic, causing serious genetic damage to early embryonic growth and development, leading to limb deformation in babies. Several proposed mechanisms of teratogenicity involve different biological functions for the (R)- and (S)-thalidomide enantiomers. In the human body, however, thalidomide undergoes racemization: even if only one of the two enantiomers is administered as a drug, the other enantiomer is produced as a result of metabolism. Accordingly, it is incorrect to state that one stereoisomer is safe while the other is teratogenic. Thalidomide is currently used for the treatment of other diseases, notably cancer and leprosy. Strict regulations and controls have been implemented to avoid its use by pregnant women and prevent developmental deformities. This d",
    "source": "wikipedia",
    "title": "Stereochemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_52821408",
    "text": "Structural chemistry is a part of chemistry and deals with the connectivity and shape of individual chemical species, which includes molecules, ions, and polymers. Structure is central to understanding the behavior of chemical species, so the topic is foundational.  Structure is integrated into the major branches of chemistry, i.e. inorganic chemistry and organic chemistry.  One indicator of the centrality of structural chemistry, many of the Nobel Prize in Chemistry have been awarded for structural insights or techniques important for determining structure.  \nIn chemistry, \"structure\" takes distinct meanings depending on the length scale and time scale.  With respect to length scales, on a local level, structure refers to interatomic distances and angles.  On a larger length, symmetry and periodicity are important descriptors.\n\nHistory\nTheories of chemical structure were first developed by August Kekulé, Archibald Scott Couper, and Aleksandr Butlerov, among others, from about 1858. Kekulé proposed the earliest ideas about valency by suggesting the elements had a prefered number of chemical bonds. Couper developed the first chemical structure diagrams, ways of representing structure on paper. Butlerov was the first to use 'structure' in chemistry and to recognize that chemical compounds are not a random cluster of atoms and functional groups, but rather had a definite order defined by the valency of the elements composing the molecule.\nIn 1883 Alexander Crum Brown deduced the crystal structure of NaCl and built a model of it using knitting needles and wool balls. He also proposed a structure for ethanoic acid that matches modern models well before experimental structural analysis techniques were developed.\nAt the beginning of the 20th century only visible light spectroscopy gave direct structural information, but that changed rapidly. By 1915 William Henry Bragg and son William Lawrence Bragg received the Nobel prize for developing early forms of X-ray crystal structure analysis and in 1922, Francis Aston was awarded the prize for creating the first mass spectrometer. The discovery of quantum mechanics in the 1920 lead to a succession of fundamental physical studies that could be applied to determining chemical structure, including the Raman effect, Mossbauer spectroscopy, and nuclear magnetic resonance (NMR). Beginning in the 1970s, electronic detectors and computer-aided analysis techniques, dramatically increased structural data and reduced the time to create and analyze it. Especially notable for structural chemistry was the development of  direct methods of X-ray crystallography by Herbert A. Hauptman and Jerome Karle, leading to the 1985 Nobel prize in Chemistry.\n",
    "source": "wikipedia",
    "title": "Structural chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_69976681",
    "text": "Superelectrophilic anions are a class of molecular ions that exhibit highly electrophilic reaction behavior despite their overall negative charge. Thus, they are even able to bind the unreactive noble gases or molecular nitrogen at room temperature. The only representatives known so far are the fragment ions of the type (B12X11)− derived from the closo-dodecaborate dianions (B12X12)2−. X represents a substituent connected to a boron atom (cf. Fig. 1). For this reason, the following article deals exclusively with superelectrophilic anions of this type.\n\nOverview\nAnions are negatively charged ions and therefore usually exhibit nucleophilic reaction behavior. However, it has been shown that there are anions which behave in a strongly electrophilic manner despite their negative charge. This means that they form bonds with reaction partners in chemical reactions by accepting electron density from them. Their affinity for electrons is so great that they are even able to bind very unreactive small molecules such as nitrogen (N2) or noble gas atoms at room temperature. For this reason, they are called \"superelectrophilic anions\". Furthermore, these superelectrophilic anions allow the direct reaction with small alkanes such as methane. Reactions with other anions are also possible and lead to the formation of highly charged ions.\nThe only superelectrophilic anions known so far are the fragment ions with the molecular formula (B12X11)−, which can be generated from the closo-dodecaborate dianions (B12X12)2−. The X represents a substituent connected to a boron atom, e.g. Cl or Br.\nDue to their high reactivity, these fragment ions can so far only be generated in the evacuated gas phase of a mass spectrometer.  Therefore, reactions of this class of compounds have been studied mainly in the gas phase. In the condensed phase, reaction products of the superelectrophilic anions were synthesized in small amounts using the ion soft-landing method.\nPotential applications of this research include the preparation of exotic compounds (e.g., noble gas compounds) of interest for fundamental chemical research. If syntheses with superelectrophilic anions would become possible on a larger scale, they might be used for applications, for example the development of cancer drugs for the Boron Neutron Capture Therapy (BNCT).\n",
    "source": "wikipedia",
    "title": "Superelectrophilic anion",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_2945180",
    "text": "Superplasticizers (SPs), also known as high-range water reducers (HRWRs), are additives used for making high-strength concrete or to place self-compacting concrete. Plasticizers are chemical compounds enabling the production of concrete with approximately 15% less water content. Superplasticizers allow reduction in water content by 30% or more. These additives are employed at the level of a few weight percent. Plasticizers and superplasticizers also retard the setting and hardening of concrete.\nAccording to their dispersing functionality and action mode, one distinguishes two classes of superplasticizers: \n\nIonic interactions (electrostatic repulsion): lignosulfonates (first generation of ancient water reducers), sulfonated synthetic polymers (naphthalene, or melamine, formaldehyde condensates) (second generation), and;\nSteric effects: Polycarboxylates-ether (PCE) synthetic polymers bearing lateral chains (third generation).\nSuperplasticizers are used when well-dispersed cement particle suspensions are required to improve the flow characteristics (rheology) of concrete. Their addition allows to decrease the water-to-cement ratio of concrete or mortar without negatively affecting the workability of the mixture. It enables the production of self-consolidating concrete and high-performance concrete. The water–cement ratio is the main factor determining the concrete strength and its durability. Superplasticizers greatly improve the fluidity and the rheology of fresh concrete. The concrete strength increases when the water-to-cement ratio decreases because avoiding to add water in excess only for maintaining a better workability of fresh concrete results in a lower porosity of the hardened concrete, and so to a better resistance to compression.\nThe addition of SP in the truck during transit is a fairly modern development within the industry. Admixtures added in transit through automated slump management system, allow to maintain fresh concrete slump until discharge without reducing concrete quality.\n\nWorking mechanism\nTraditional plasticizers are lignosulfonates as their sodium salts.  Superplasticizers are synthetic polymers. Compounds used as superplasticizers include (1) sulfonated naphthalene formaldehyde condensate, sulfonated melamine formaldehyde condensate, acetone formaldehyde condensate and (2) polycarboxylates ethers. Cross-linked melamine- or naphthalene-sulfonates, referred to as PMS (polymelamine sulfonate) and PNS (polynaphthalene sulfonate), respectively, are illustrative. They are prepared by cross-linking of the sulfonated monomers using formaldehyde or by sulfonating the corresponding crosslinked polymer.\n\nThe polymers used as plasticizers exhibit surfactant properties. They are often ionomers bearing negatively charged groups (sulfonates, carboxylates, or phosphonates...). They function as dispersants to minimize particles segregation in fresh concrete (separation of the cement slurry and water from the coarse and fine aggregates s",
    "source": "wikipedia",
    "title": "Superplasticizer",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_1048518",
    "text": "Supramolecular chemistry is the branch of chemistry concerning chemical systems composed of discrete numbers of molecules. The strength of the forces responsible for spatial organization of the system ranges from weak intermolecular forces, electrostatic charge, or hydrogen bonding to strong covalent bonding, provided that the electronic coupling strength remains small relative to the energy parameters of the component. While traditional chemistry concentrates on the covalent bond, supramolecular chemistry examines the weaker and reversible non-covalent interactions between molecules. These forces include hydrogen bonding, metal coordination, hydrophobic forces, van der Waals forces, pi–pi interactions and electrostatic effects.\nImportant concepts advanced by supramolecular chemistry include molecular self-assembly, molecular folding, molecular recognition, host–guest chemistry, mechanically-interlocked molecular architectures, and dynamic covalent chemistry. The study of non-covalent interactions is crucial to understanding many biological processes that rely on these forces for structure and function. Biological systems are often the inspiration for supramolecular research.\n\nHistory\nThe existence of intermolecular forces was first postulated by Johannes Diderik van der Waals in 1873. However, Nobel laureate Hermann Emil Fischer developed supramolecular chemistry's philosophical roots. In 1894, Fischer suggested that enzyme–substrate interactions take the form of a \"lock and key\", the fundamental principles of molecular recognition and host–guest chemistry. In the early twentieth century non-covalent bonds were understood in gradually more detail, with the hydrogen bond being described by Latimer and Rodebush in 1920.\nWith the deeper understanding of the non-covalent interactions, for example, the clear elucidation of DNA structure, chemists started to emphasize the importance of non-covalent interactions. In 1967, Charles J. Pedersen discovered crown ethers, which are ring-like structures capable of chelating certain metal ions. Then, in 1969, Jean-Marie Lehn discovered a class of molecules similar to crown ethers, called cryptands. After that, Donald J. Cram synthesized many variations to crown ethers, on top of separate molecules capable of selective interaction with certain chemicals. The three scientists were awarded the Nobel Prize in Chemistry in 1987 for \"development and use of molecules with structure-specific interactions of high selectivity\". In 2016, Bernard L. Feringa, Sir J. Fraser Stoddart, and Jean-Pierre Sauvage were awarded the Nobel Prize in Chemistry, \"for the design and synthesis of molecular machines\".\n\nThe term supermolecule (or supramolecule) was introduced by Karl Lothar Wolf et al. (Übermoleküle) in 1937 to describe hydrogen-bonded acetic acid dimers.  The term supermolecule is also used in biochemistry to describe complexes of biomolecules, such as peptides and oligonucleotides composed of multiple strands.\nEventually,",
    "source": "wikipedia",
    "title": "Supramolecular chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_79925532",
    "text": "Supramolecular coordination complexes (SCCs) are discrete self-assembled constructs formed through highly directional and stoichiometric metal-ligand coordination bonds. They are also referred to as coordination-driven self-assemblies and belong to the class of supramolecular structures called metal-organic complexes (MOC). Transition metal ions serve as Lewis-acceptor units with preferred coordination geometries, and labile or rigid ligands serve as Lewis-donor molecules that spontaneously assemble with specific directionality, leading to different types of well-defined geometries. The different coordination-driven discrete topological architecture of SCCs is categorized as two-dimensional (2D) metallacycles and three-dimensional (3D) metallacages. SCCs allow design flexibility with precision through careful selection of the structure of metal and ligand components, along with the coordination angle to obtain a range of sizes, shapes, and topologies with different physicochemical properties. Among metallacycles triangles, rectangles, hexagons, trigonal prisms, hexagonal prisms, rhomboids, and cubes, design geometries have been reported. Whereas in 3D systems, trigonal pyramids, trigonal prisms, truncated and snub cubes, truncated tetrahedra, cuboctahedra, double squares, adamantanoids, dodecahedra are among the variety of cage geometries reported. Several design strategies or approaches have been identified and studied for the synthesis of metallacycles and metallacages, and are summarized in several reviews on SCCs.\n\nDistinctive features and uses\nThe distinctive feature of the SCCs is imparted by the predictable nature of metal-ligand coordination spheres and moderate bond strength, allowing dynamic flexibility or reversibility of weak non-covalent bonds and relative stability or rigidity of stronger covalent bonds, which dictate the coordination kinetics of the self-assembly process. The metal-ligand bonds have energies of (15-50 kcal/mol) compared to organic covalent bonds (approx. 60-120 kcal/mol) and the weak interactions (ca. 0.5 10 kcal/mol). The feature of kinetic reversibility due to substitutional lability of metal-ligand bonds and reactive intermediates endows the coordination-driven self-assembled architectures ability to \"self-correct\" to the most thermodynamically favorable product. In simpler words, transition metals have their preferred geometry (of acceptor sites), so if a rigid or flexible donor ligand coordinates in an improper orientation, the thermodynamically favored structure or closed geometry is not formed (kinetic intermediates are formed). Then the metal-ligand bonds can dissociate and reassociate \"correctly\" to transform into the target thermodynamic product. To ensure SCCs are free from kinetic impurities, the synthetic conditions for self-assembly must allow easy reconstruction of reactive intermediates (avoid precipitation, insolubility, or sluggishness in rectifying) en route to the energetically minimum product.\n",
    "source": "wikipedia",
    "title": "Supramolecular coordination complex",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_55573246",
    "text": "Systems chemistry is the science of studying networks of interacting molecules, to create new functions from a set (or library) of molecules with different hierarchical levels and emergent properties.\nSystems chemistry is also related to the origin of life (abiogenesis).\n\nRelations to systems biology\nSystems chemistry is a relatively young sub-discipline of chemistry, where the focus does not lie on the individual chemical components but rather on the overall network of interacting molecules and on their emergent properties. Hence, it combines the classical knowledge of chemistry (structure, reactions and interactions of molecules) together with a systems approach inspired by systems biology and systems science.\nExamples\nDynamic combinatorial chemistry has been used as a method to develop ligands for biomolecules and receptors for small molecules.\nLigands that can recognize biomolecules are being identified by preparing libraries of potential ligands in the presence of a target biomacromolecule. This is relevant for application as biosensors for fast monitoring of imbalances and illnesses and therapeutic agents.\nIndividual components of certain chemical system will self-assemble to form receptors which are complementary to target molecule. In principle, the preferred library members will be selected and amplified based on the strongest interactions between the template and products.\nMolecular networks and equilibrium\nA fundamental difference exists between chemistry as it is performed in most laboratories and chemistry as it occurs in life. Laboratory processes are mostly designed such that the (closed) system goes thermodynamically downhill; i.e. the product state is of lower Gibbs free energy, yielding stable molecules that can be isolated and stored. Yet the chemistry of life operates in a very different way: most molecules from which living systems are constituted are turned over continuously and are not necessarily thermodynamically stable. Nevertheless, living systems can be stable, but in a homeostatic sense. Such homeostatic (open) systems are far-from-equilibrium and are dissipative: they need energy to maintain themselves. In dissipative controlled systems the continuous supply of energy allows a continuous transition between different supramolecular states, where systems with unexpected properties may be discovered. One of the grand challenges of Systems Chemistry is to unveil complex reactions networks, where molecules continuously consume energy to perform specific functions.\n",
    "source": "wikipedia",
    "title": "Systems chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_31491",
    "text": "Theoretical chemistry is the branch of chemistry which develops theoretical generalizations that are part of the theoretical arsenal of modern chemistry: for example, the concepts of chemical bonding, chemical reaction, valence, the surface of potential energy, molecular orbitals, orbital interactions, and molecule activation.\n\nOverview\nTheoretical chemistry unites principles and concepts common to all branches of chemistry. Within the framework of theoretical chemistry, there is a systematization of chemical laws, principles and rules, their refinement and detailing, the construction of a hierarchy. The central place in theoretical chemistry is occupied by the doctrine of the interconnection of the structure and properties of molecular systems. It uses mathematical and  physical methods to explain the structures and dynamics of chemical systems and to correlate, understand, and predict their thermodynamic and kinetic properties. In the most general sense, it is explanation of chemical phenomena by methods of theoretical physics. In contrast to theoretical physics, in connection with the high complexity of chemical systems, theoretical chemistry, in addition to approximate mathematical methods, often uses semi-empirical and empirical methods.\nIn recent years, it has consisted primarily of quantum chemistry, i.e., the application of quantum mechanics to problems in chemistry. Other major components include molecular dynamics, statistical thermodynamics and theories of electrolyte solutions, reaction networks, polymerization, catalysis, molecular magnetism and spectroscopy.\nModern theoretical chemistry may be roughly divided into the study of chemical structure and the study of chemical dynamics. The former includes studies of: electronic structure, potential energy surfaces, and force fields; vibrational-rotational motion; equilibrium properties of condensed-phase systems and macro-molecules. Chemical dynamics includes: bimolecular kinetics and the collision theory of reactions and energy transfer; unimolecular rate theory and metastable states; condensed-phase and macromolecular aspects of dynamics.\n",
    "source": "wikipedia",
    "title": "Theoretical chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_48458437",
    "text": "This is a timeline of the development of plastics, comprising key discoveries and developments in the production of plastics.\n\nPre 19th Century\n\n19th Century\n\n20th Century\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Timeline of plastic development",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_77207015",
    "text": "The TOP Assay (Total Oxidizable Precursor Assay) is a laboratory method developed in 2012 that oxidatively converts (unknown) precursor compounds of perfluorocarboxylic acids (PFCAs) into the latter. This makes quantification possible. Potassium peroxodisulfate is used. This sum parameter can be used to determine the concentration of precursor compounds present by comparing the sample before and after the application of the TOP Assay.\n\nApplication\nThis method is used, for example, in the analysis of fire-fighting foams (aqueous film forming foam), textiles or water samples. Blood serum can also be analyzed in this way.\nNeutral, anionic and cationic precursor compounds may be distinguished using an extension of the method published in 2025.\nIn addition to fluorotelomer compounds, hydrogen-substituted perfluorosulfonic acids (Hn-PFSAs), for example, can also be oxidized using the TOP Assay. Saturated and unsaturated perfluorosulfonic acids as well as perfluoroalkyl ether sulfonic acids, on the other hand, are stable.\nFurther reading\nAteia, Mohamed; Chiang, Dora; Cashman, Michaela; Acheson, Carolyn (2023-04-11). \"Total Oxidizable Precursor (TOP) Assay─Best Practices, Capabilities and Limitations for PFAS Site Investigation and Remediation\". Environmental Science & Technology Letters. 10 (4): 292–301. Bibcode:2023EnSTL..10..292A. doi:10.1021/acs.estlett.3c00061. ISSN 2328-8930. PMC 10259459. PMID 37313434.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "TOP Assay",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_60875",
    "text": "Triboluminescence is a phenomenon in which light is generated when a material is mechanically pulled apart, ripped, scratched, crushed, or rubbed (see tribology). The phenomenon is not fully understood but appears in most cases to be caused by the separation and reunification of static electric charges, see also triboelectric effect. The term comes from the Greek τρίβειν (\"to rub\"; see tribology) and the Latin lumen (light).  Triboluminescence can be observed when breaking sugar crystals and peeling adhesive tapes.\nTriboluminescence is often a synonym for fractoluminescence (a term mainly used when referring only to light emitted from fractured crystals). Triboluminescence differs from piezoluminescence in that a piezoluminescent material emits light when deformed, as opposed to broken. These are examples of mechanoluminescence, which is luminescence resulting from any mechanical action on a solid.\n\nHistory\n\nMechanism of action\nThere remain a few ambiguities about the effect. The current theory of triboluminescence—based upon crystallographic, spectroscopic, and other experimental evidence—is that upon fracture of asymmetrical materials, charge is separated. When the charges recombine, the electrical discharge ionizes the surrounding air, causing a flash of light. Research further suggests that crystals that display triboluminescence often lack symmetry and are poor conductors.  However, there are substances which break this rule, and which do not possess asymmetry, yet display triboluminescence, such as hexakis(antipyrine)terbium iodide.  It is thought that these materials contain impurities, which make the substance locally asymmetric. Further information on some of the possible processes involved can be found in the page on the triboelectric effect.\nThe biological phenomenon of triboluminescence is thought to be controlled by recombination of free radicals during mechanical activation.\nExamples\n\nFractoluminescence\nFractoluminescence is often used as a synonym for triboluminescence.  It is the emission of light from the fracture (rather than rubbing) of a crystal, but fracturing often occurs with rubbing.  Depending upon the atomic and molecular composition of the crystal, when the crystal fractures, a charge separation can occur, making one side of the fractured crystal positively charged and the other side negatively charged. Like in triboluminescence, if the charge separation results in a large enough electric potential, a discharge across the gap and through the bath gas between the interfaces can occur. The potential at which this occurs depends upon the dielectric properties of the bath gas.\n",
    "source": "wikipedia",
    "title": "Triboluminescence",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_5653017",
    "text": "Wet chemistry is a form of analytical chemistry that uses classical methods such as observation to analyze materials. The term wet chemistry is used as most analytical work is done in the liquid phase. Wet chemistry is also known as bench chemistry, since many tests are performed at lab benches.\n\nMaterials\nWet chemistry commonly uses laboratory glassware such as beakers and graduated cylinders to prevent materials from being contaminated or interfered with by unintended sources. Gasoline, Bunsen burners, and crucibles may also be used to evaporate and isolate substances in their dry forms. Wet chemistry is not performed with any advanced instruments since most automatically scan substances. Although, simple instruments such as scales are used to measure the weight of a substance before and after a change occurs. Many high school and college laboratories teach students basic wet chemistry methods.\nHistory\nBefore the age of theoretical and computational chemistry, wet chemistry was the predominant form of scientific discovery in the chemical field. This is why it is sometimes referred to as classic chemistry or classical chemistry. Scientists would continuously develop techniques to improve the accuracy of wet chemistry. Later on, instruments were developed to conduct research impossible for wet chemistry. Over time, this became a separate branch of analytical chemistry called instrumental analysis. Because of the high volume of wet chemistry that must be done in today's society and new quality control requirements, many wet chemistry methods have been automated and computerized for streamlined analysis. The manual performance of wet chemistry mostly occurs in schools.\nMethods\n\nUses\nWet chemistry techniques can be used for qualitative chemical measurements, such as changes in color (colorimetry), but often involves more quantitative chemical measurements, using methods such as gravimetry and titrimetry. Some uses for wet chemistry include tests for:\n\npH (acidity, alkalinity)\nconcentration\nconductivity (specific conductance)\ncloud point (nonionic surfactants)\nhardness\nmelting point\nsolids or dissolved solids\nsalinity\nspecific gravity\ndensity\nturbidity\nviscosity\nmoisture (Karl Fischer titration)\nWet chemistry is also used in environmental chemistry settings to determine the current state of the environment. It is used to test:\n\nBiochemical Oxygen Demand (BOD)\nChemical Oxygen Demand (COD)\neutrophication\ncoating identification\nIt can also involve the elemental analysis of samples, e.g., water sources, for chemicals such as:\n\nAmmonia nitrogen\nChloride\nChromium\nCyanide\nDissolved oxygen\nFluoride\nNitrogen\nNitrate\nPhenols\nPhosphate\nPhosphorus\nSilica\nSulfates\nSulfides\n",
    "source": "wikipedia",
    "title": "Wet chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_5323",
    "text": "Computer science is the study of computation, information, and automation. Included broadly in the sciences, computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software). An expert in the field is known as a computer scientist. \nAlgorithms and data structures are central to computer science.\nThe theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human–computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.\nThe fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science.\n\nHistory\nThe earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.\nWilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the ",
    "source": "wikipedia",
    "title": "Computer science",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_57143357",
    "text": "This glossary of computer science is a list of definitions of terms and concepts used in computer science, its sub-disciplines, and related fields, including terms relevant to software, data science, and computer programming.\n\nA\nabstract data type (ADT)\nA mathematical model for data types in which a data type is defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. This contrasts with data structures, which are concrete representations of data from the point of view of an implementer rather than a user.\n\nabstract method\nOne with only a signature and no implementation body. It is often used to specify that a subclass must provide an implementation of the method. Abstract methods are used to specify interfaces in some computer languages.\n\nabstraction\n1.  In software engineering and computer science, the process of removing physical, spatial, or temporal details or attributes in the study of objects or systems in order to more closely attend to other details of interest; it is also very similar in nature to the process of generalization.\n2.  The result of this process: an abstract concept-object created by keeping common features or attributes to various concrete objects or systems of study.\n\nagent architecture\nA blueprint for software agents and intelligent control systems depicting the arrangement of components. The architectures implemented by intelligent agents are referred to as cognitive architectures.\n\nagent-based model (ABM)\nA class of computational models for simulating the actions and interactions of autonomous agents (both individual or collective entities such as organizations or groups) with a view to assessing their effects on the system as a whole. It combines elements of game theory, complex systems, emergence, computational sociology, multi-agent systems, and evolutionary programming. Monte Carlo methods are used to introduce randomness.\n\naggregate function\nIn database management, a function in which the values of multiple rows are grouped together to form a single value of more significant meaning or measurement, such as a sum, count, or max.\n\nagile software development\nAn approach to software development under which requirements and solutions evolve through the collaborative effort of self-organizing and cross-functional teams and their customer(s)/end user(s). It advocates adaptive planning, evolutionary development, early delivery, and continual improvement, and it encourages rapid and flexible response to change.\n\nalgorithm\nAn unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing, and automated reasoning tasks. They are ubiquitous in computing technologies.\n\nalgorithm design\nA method or mathematical process for problem-solving and for engineering algorithms. The design of algorithms is part of many solution theories of o",
    "source": "wikipedia",
    "title": "Glossary of computer science",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_169633",
    "text": "Computer science (also called computing science) is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. One well known subject classification system for computer science is the ACM Computing Classification System devised by the Association for Computing Machinery.\nComputer science can be described as all of the following:\n\nAcademic discipline\nScience\nApplied science\n\nSubfields\n\nHistory\nHistory of computer science\nList of pioneers in computer science\nHistory of Artificial Intelligence\nHistory of Operating Systems\nProfessions\nComputer Scientist\nProgrammer (Software developer)\nTeacher/Professor\nSoftware engineer\nSoftware architect\nSoftware tester\nHardware engineer\nData analyst\nInteraction designer\nNetwork administrator\nData scientist\nData and data structures\nData structure\nData type\nAssociative array and Hash table\nArray\nList\nTree\nString\nMatrix (computer science)\nDatabase\nProgramming paradigms\nImperative programming/Procedural programming\nFunctional programming\nLogic programming\nDeclarative Programming\nEvent-Driven Programming\nObject oriented programming\nClass\nInheritance\nObject\nSee also\nAbstraction\nBig O notation\nClosure\nCompiler\nCognitive science\nExternal links\n\nList of Computer Scientists\nGlossary of Computer Science\n",
    "source": "wikipedia",
    "title": "Outline of computer science",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_61776737",
    "text": "In computing, a device or software program is said to be agnostic or data agnostic if the method or format of data transmission is irrelevant to the device or program's function. This means that the device or program can receive data in multiple formats or from multiple sources, and still process that data effectively.\n\nDefinition\nMany devices or programs need data to be presented in a specific format to process the data. For example, Apple Inc devices generally require applications to be downloaded from their App Store. This is a non data-agnostic method, as it uses a specified file type, downloaded from a specific location, and does not function unless those requirements are met.\nNon data-agnostic devices and programs can present problems. For example, if your file contains the right type of data (such as text), but in the wrong format, you may have to create a new file and enter the text manually in the proper format in order to use that program. Various file conversion programs exist because people need to convert their files to a different format in order to use them effectively.\nImplementation\nData agnostic devices and programs work to solve these problems in a variety of ways. Devices can treat files in the same way whether they are downloaded over the internet or transferred over a USB or other cable.\nDevices and programs can become more data-agnostic by using a generic storage format to create, read, update and delete files. Formats like XML and JSON can store information in a data agnostic manner. For example, XML is data agnostic in that it can save any type of information. However, if you use Data Transform Definitions (DTD) or XML Schema Definitions (XSD) to define what data should be placed where, it becomes non-data agnostic; it produces an error if the wrong type of data is placed in a field.\nOnce you have your data saved in a generic storage format, this source can act as an entity synchronization layer. The generic storage format can interface with a variety of different programs, with the data extraction method formatting the data in a way that the specific program can understand. This allows two programs that require different data formats to access the same data. Multiple devices and programs can create, read, update and delete (CRUD) the same information from the same storage location without formatting errors.\nWhen multiple programs are accessing the same records, they may have different defined fields for the same type of concept. Where the fields are differently labelled but contain the same data, the program pulling the information can ensure the correct data is used. If one program contains fields and information that another does not, those fields can be saved to the record and pulled for that program, but ignored by other programs. As the entity synchronization layer is data agnostic, additional fields can be added without worrying about recoding the whole database, and concepts created in other programs (that do not ",
    "source": "wikipedia",
    "title": "Agnostic (data)",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_212335",
    "text": "Any kind of logic, function, expression, or theory based on the work of George Boole is considered Boolean.  \nRelated to this, \"Boolean\" may refer to:\n\nBoolean data type, a form of data with only two possible values (usually \"true\" and \"false\")\nBoolean algebra, a logical calculus of truth values or set membership\nBoolean algebra (structure), a set with operations resembling logical ones\nBoolean domain, a set consisting of exactly two elements whose interpretations include false and true\nBoolean circuit, a mathematical model for digital logical circuits.\nBoolean expression, an expression in a programming language that produces a Boolean value when evaluated\nBoolean function, a function that determines Boolean values or operators\nBoolean model (probability theory), a model in stochastic geometry\nBoolean network, a certain network consisting of a set of Boolean variables whose state is determined by other variables in the network\nBoolean processor, a 1-bit variable computing unit\nBoolean ring, a mathematical ring for which x2 = x for every element x\nBoolean satisfiability problem,  the problem of determining if there exists an interpretation that satisfies a given Boolean formula\nBoolean prime ideal theorem, a theorem which states that ideals in a Boolean algebra can be extended to prime ideals\n\nSee also\nBinary (disambiguation)\n",
    "source": "wikipedia",
    "title": "Boolean",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_79297557",
    "text": "Catalytic computing is a technique in computer science, relevant to complexity theory, that uses full memory, as well as empty memory space, to perform computations. Full memory is memory that begins in an arbitrary state and must be returned to that state at the end of the computation, for example important data. It can sometimes be used to reduce the memory needs of certain algorithms, for example the tree evaluation problem. It was defined by Buhrman, Cleve, Koucký, Loff, and Speelman in 2014 and was named after catalysts in chemistry, based on the metaphorically viewing the full memory as a \"catalyst\", a non-consumed factor critical for the computational \"reaction\" to succeed.\nThe complexity class CSPACE(s(n)) is the class of sets computable by catalytic Turing machines whose work tape is bounded by s(n) tape cells and whose auxiliary full memory space is bounded by \n  \n    \n      \n        \n          2\n          \n            s\n            (\n            n\n            )\n          \n        \n      \n    \n    {\\displaystyle 2^{s(n)}}\n  \n tape cells. It has been shown that CSPACE(log(n)), or catalytic logspace, is contained within ZPP and, importantly, contains TC1.\n\nResults\nIn 2020 J. Cook and Mertz used catalytic computing to prove to attack the tree evaluation problem (TreeEval) a type of pebble game introduced by Cook, McKenzie, Wehr, Braverman and Santhanam as an example where any algorithm for solving the problem would require too much memory to belong in the L complexity class, proving that in fact the conjectured minimum can be lowered and in 2023 they lowered the bound even further to space \n  \n    \n      \n        O\n        (\n        log\n        ⁡\n        n\n        log\n        ⁡\n        log\n        ⁡\n        n\n        )\n      \n    \n    {\\displaystyle O(\\log n\\log \\log n)}\n  \n, almost ruling out the problem as an approach to the question if L=P.\nIn a 2025 preprint Williams showed that the work of J. Cook and Mertz could be used to prove that every deterministic multitape Turing machine of time complexity \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n can be simulated in space \n  \n    \n      \n        O\n        (\n        \n          \n            t\n            log\n            ⁡\n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {t\\log t}})}\n  \n improving the previous bound of \n  \n    \n      \n        O\n        (\n        t\n        \n          /\n        \n        log\n        ⁡\n        t\n        )\n      \n    \n    {\\displaystyle O(t/\\log t)}\n  \n by Hopcroft, Paul, and Valiant and strengthening the case in the negative for the question if PSPACE=P.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Catalytic computing",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_78245236",
    "text": "Computational gastronomy is an interdisciplinary field combining computational science with culinary studies. It applies data-driven techniques to analyze various aspects of food, including recipes, flavors, nutrition, and sustainability. The field utilizes advancements in data analytics, machine learning, and computational models to systematically study food and optimize culinary practices. Applications of computational gastronomy include recipe optimization, flavor profiling, nutritional analysis, and personalized dietary recommendations.\n\nOverview\nThe field of computational gastronomy aims to enhance understanding and innovation in culinary science through computational tools. By analyzing the relationships between food components, health, and flavor, researchers seek to create innovative culinary experiences and improve food preparation techniques. Despite its potential, the field faces challenges such as the lack of high-quality, well-structured datasets, particularly for traditional recipes, and the inherent subjectivity of sensory experiences like taste.\nTechniques and Applications\n\nChallenges and Future Directions\nComputational gastronomy faces challenges related to data quality, cultural diversity in recipes, and the subjective nature of taste. Researchers emphasize collaboration among chefs, scientists, and technologists to address these issues.\nNotable researchers\nGanesh Bagler\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Computational gastronomy",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_25852537",
    "text": "Computer science in sport is an interdisciplinary discipline that has its goal in combining the theoretical as well as practical aspects and methods of the areas of informatics and sport science. The main emphasis of the interdisciplinarity is placed on the application and use of computer-based, but also mathematical techniques in sport science, aiming in this way at the support and advancement of theory and practice in sports. The reason computer science has become an important partner for sport science is mainly connected with \"the fact that the use of data and media, the design of models, the analysis of systems etc. increasingly requires the support of suitable tools and concepts which are developed and available in computer science\".\n\nHistorical background\nGoing back in history, computers in sports were used for the first time in the 1960s, when the main purpose was to accumulate sports information. Databases were created and expanded in order to launch documentation and dissemination of publications like articles or books that contain any kind of knowledge related to sports science. Until the mid-1970s also the first organization in this area called IASI (International Association for Sports Information) was formally established. Congresses and meetings were organized more often with the aim of standardization and rationalization of sports documentation. Since at that time this area was obviously less computer-oriented, specialists talk about sports information rather than sports informatics when mentioning the beginning of this field of science.\nBased on the progress of computer science and the invention of more powerful computer hardware in the 1970s, also the real history of computer science in sport began. This was as well the first time when this term was officially used and the initiation of a very important evolution in sports science.\nIn the early stages of this area statistics on biomechanical data, like different kinds of forces or rates, played a major role. Scientists started to analyze sports games by collecting and looking at such values and features in order to interpret them. Later on, with the continuous improvement of computer hardware — in particular microprocessor speed – many new scientific and computing paradigms were introduced, which were also integrated in computer science in sport. Specific examples are modeling as well as simulation, but also pattern recognition, and design.\nAs another result of this development, the term 'computer science in sport' has been added in the encyclopedia of sports science in 2004.\n",
    "source": "wikipedia",
    "title": "Computer science in sport",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_76805178",
    "text": "Filter and Refine Principle (FRP) is a general computational strategy in computer science.\nFRP is used broadly across various disciplines, particularly in information retrieval, database management, and pattern recognition, which efficiently processes large sets of objects through a two-stage approach: filtering and refinement.\nThe filtering stage quickly eliminates less promising or irrelevant objects from a large set using efficient, less resource-intensive algorithms. This stage is designed to reduce the volume of data that needs to be processed in the more resource-demanding refinement stage.\nFollowing filtering, the refinement stage applies more complex and computationally expensive techniques to the remaining objects to achieve higher accuracy via finer-grained processing. This stage is essential for obtaining the desired quality and precision in the results.\nFRP is a general method for completing a computationally intensive task as quickly as possible (Filter and Refine Strategy),  which is important in scenarios where managing the inherent trade-offs between speed and accuracy is crucial. Its implementations span various fields and applications, from database indexing/query processing, and information retrieval to machine learning and big data analytics. Its implementation helps in optimizing systems to better manage the inherent trade-offs between speed and accuracy.\n\nOverview\nFRP follows a two-step processing strategy:\n\nFilter: an efficient filter function \n  \n    \n      \n        \n          f\n          \n            f\n            i\n            l\n            t\n            e\n            r\n          \n        \n      \n    \n    {\\displaystyle f_{filter}}\n  \n is applied to each object \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n in the dataset \n  \n    \n      \n        \n          \n            D\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}}\n  \n. The filtered subset \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          ′\n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}'}\n  \n is defined as \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          ′\n        \n        =\n        {\n        x\n        \n          |\n        \n        \n          f\n          \n            f\n            i\n            l\n            t\n            e\n            r\n          \n        \n        (\n        x\n        )\n        ≥\n        v\n        }\n      \n    \n    {\\displaystyle {\\mathcal {D}}'=\\{x|f_{filter}(x)\\geq v\\}}\n  \n for value-based tasks, where \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n is a threshold value, or \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          ′\n        \n        =\n        {\n        x\n        \n          |\n        \n        \n          f\n          \n            f\n            i\n            l\n            t\n            e\n            r\n          \n        \n        (\n        x",
    "source": "wikipedia",
    "title": "Filter and refine",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_81725547",
    "text": "A learnable function is a mathematical function that can be learned from data, typically through a machine learning algorithm, to minimize errors and perform a specific task. In the context of statistical learning theory, this refers to a  function class where an algorithm can identify a specific function that best approximates the underlying pattern or distribution within the data.\nIn the domain of database systems, specifically within the emerging field of AI-powered database systems, learnable function is a general concept, representing a paradigm shift where traditional, hard-coded system heuristics are replaced by these parameterized functions. This enables the DBMS to learn, adapt, and govern based on the data it manages. Instead of relying on static rules designed by human experts, database components utilize learnable functions to dynamically adapt to specific data distributions and workloads.\n\nDefinition\nFundamentally, a learnable function can be formalized as a mapping \n  \n    \n      \n        f\n        :\n        \n          \n            X\n          \n        \n        →\n        \n          \n            Y\n          \n        \n      \n    \n    {\\displaystyle f:{\\mathcal {X}}\\to {\\mathcal {Y}}}\n  \n, parameterized by a set of weights \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n. The goal of the learning process is to find the optimal parameters \n  \n    \n      \n        \n          θ\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle \\theta ^{*}}\n  \n that minimize a specific loss function \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n over a dataset \n  \n    \n      \n        \n          \n            D\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}}\n  \n:\n\n  \n    \n      \n        \n          θ\n          \n            ∗\n          \n        \n        =\n        arg\n        ⁡\n        \n          min\n          \n            θ\n          \n        \n        \n          ∑\n          \n            (\n            x\n            ,\n            y\n            )\n            ∈\n            \n              \n                D\n              \n            \n          \n        \n        L\n        (\n        f\n        (\n        x\n        \n          |\n        \n        θ\n        )\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle \\theta ^{*}=\\arg \\min _{\\theta }\\sum _{(x,y)\\in {\\mathcal {D}}}L(f(x|\\theta ),y)}\n  \n\nIn this framework:\n\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n represents the input features (covariates).\n\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n represents the target output or label.\n\n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n is the hypothesis or model (e.g., a linear regression model, a decision tree, or a Neural network).\nThe \"learnability\" of the function ensures that as the size of the dataset \n  \n    \n      \n        \n          \n            D\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}}\n  \n increases",
    "source": "wikipedia",
    "title": "Learnable function",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_81180325",
    "text": "The Linked Data Modeling Language, or LinkML, is a data modeling framework based on YAML that aims to \"bring semantic web standards to the masses, simplifying the production of FAIR, ontology-ready data.\" It can be used to specify data schemas that can be used for validating data via formats like JSON-LD and ShEx.\n\nDescription\nThe Linked data Modeling Language is an object-oriented data modeling framework aiming to simplify the production of FAIR data. It is intended to be used for schematizing a variety of kinds of data, ranging from simple flat checklist-style standards to complex interrelated normalized data utilizing polymorphism/inheritance.\nNotes\nThis page contains text in CC-BY 4.0 from Sierra Moxon and colleagues, from the publication The Linked Data Modeling Language (LinkML): A General-Purpose Data Modeling Framework Grounded in Machine-Readable Semantics, in the proceedings of the International Conference on Biomedical Ontologies in 2021.\nReferences\n\nExternal links\nOfficial website\nOfficial documentation\nLinkML registry\n",
    "source": "wikipedia",
    "title": "LinkML",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_46814283",
    "text": "Abstractions are fundamental building blocks of computer science, enabling complex systems and ideas to be simplified into more manageable and relatable concepts.\n\nGeneral Programming Abstractions\nGeneral programming abstractions are foundational concepts that underlie virtually all of the programming tasks that software developers engage in. By providing a layer of separation from the specifics of the underlying hardware and system details, these abstractions allow for the creation of complex logic in a more approachable and manageable form. They emerge as a consensus on best practices for expressing and solving programming problems in efficient and logically sound ways. From the simplicity of a variable to the structured flow of control structures, these abstractions are the building blocks that constitute high-level programming languages and give rise to detailed software implementations.\nData Structures\nIn the context of data structures, the term \"abstraction\" refers to the way in which a data structure represents and organizes data. Each data structure provides a particular way of organizing data in memory so that it can be accessed and modified according to specific rules. The data structure itself is an abstraction because it hides the details of how the data is stored in memory and provides a set of operations or interfaces for working with the data (e.g., push and pop for a stack, insert and delete for a binary search tree).\nFunctional Programming Abstractions\nIn the world of functional programming, abstraction is not just a tool but a core principle that influences the entire programming model. The abstractions used in functional programming are designed to enhance expressiveness, provide greater modularity, and enable transformative operations that are both concise and predictable. By treating computation as the evaluation of mathematical functions, functional programming moves away from the mutable state and side effects that are typical in imperative programming, presenting a declarative approach to problem-solving.\n",
    "source": "wikipedia",
    "title": "List of abstractions (computer science)",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_60644",
    "text": "A peripheral  device, or simply peripheral, is an auxiliary hardware device that a computer uses to transfer information externally. A peripheral is a hardware component that is accessible to and controlled by a computer but is not a core component of the computer. It can communicate with a computer through wired or wireless connections. Many modern electronic devices, such as Internet-enabled digital watches, video game consoles, smartphones, and tablet computers, have interfaces for use as a peripheral.\nMouses and keyboards became the standard for computer peripheral input devices in the 1970s, while memory storage devices continued to be developed in new ways. Output devices, such as monitors, began as cathode rays, before switching to LCD monitors in the 1980s.\n\nTypes\nA peripheral can be categorized based on the direction in which information flows relative to the computer, and is usually categorised one of three ways: Input, output and storage.\nHistory\nOne of the earliest known computer peripherals to be made was the punched card, which was first introduced into computing in the late 1880s by Herman Hollerith, an American engineer.  As a result, the punched card tabulator was invented, which was able to read the punch cards. In addition, it was the first computer peripheral to be mass-produced. In 1901, the introduction of the punched card also led to the creation of the Hollerith Type 001 Mechanical Card Punch, would become the basis of computer keyboards. The keypunch allowed operators to punch the digits 0-9, without the user having prior knowledge of the corresponding card codes.\nPunched tape was later used instead of punched cards as a computer peripheral, because of its lower cost, and higher storage capacity. Programs were written to punched tape using existing teleprinters, then were transferred to a reader so that a computer could load the program. The first documented computer to use punched tape as storage was the Zuse Z1, released in 1938 by German inventor Konrad Zuse. which was able to read source code on punched tape.\nIn the early 1950s, the UNISERVO I, created for the UNIVAC I computer, became the first commercially available magnetic tape drive. Magnetic tape drives have both the ability to read and write to magnetic tape. Magnetic tape is often used to backup or archive digital data for long periods of time, due to its higher cost efficiency compared to other storage mediums, and because it is not possible cannot both read and write to magnetic tape at the same time.\nIn 1956, the IBM 305 RAMAC was the first the first commercial computer to ship with a hard disk, the IBM Model 350. The IBM Model 350 was the first hard disk, and it had the ability to be randomly read and written to at any time. The hard drive was able to store 5 MB of storage. To achieve this, it used fifty magnetic disks, that were double sided and had a diameter of 24 inches each,  spinning at 1200 RPM.\nIn the early 1960s, the RS-232 standard was developed ",
    "source": "wikipedia",
    "title": "Peripheral",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_1806683",
    "text": "Prefetching is a technique used in computing to improve performance by retrieving data or instructions before they are needed. By predicting what a program will request in the future, the system can load information in advance to reduced wait times .\nPrefetching is used in various areas of computing, including CPU architectures and operating systems. It can be implemented in both hardware and software, and it relies on detecting access patterns that suggest what data is likely to be needed soon.\n\nOverview\nPrefetching works by predicting which memory addresses or resources will be accessing and load them into faster access storage, like caches.\nPrefetching may be used:\n\nHardware-level, such as CPU memory controllers\nSoftware-level, strategies in compilers, operating systems, logic in web browsers or file systems\nHardware\nProcessors (CPU's) often include prefetching that attempts to reduce cache misses by loading data into cache before it is requested by the running program. This is for programs that access memory in predictable patterns, such as loops that iterate over arrays.\nHardware prefetching can be done without software involvement and can be found in most modern CPU's. For example, Intel CPU's feature a variety of prefetch that work across multiple cache levels.\n\nStride prefetching detects constant-stride memory access patterns (fixed distance between consecutive memory accesses)\nStream prefetching identifies long sequences of contiguous memory accesses (sequential access to a block of memory)\nCorrelation prefetching learns patterns between cache misses and triggers prefetches based on those patterns\nSoftware\nPrefetch instructions can be written into the code by the programmer or by the compiler. Prefetch instructions specify the memory addresses to be prefetched and the desired prefetch distance.\nIn software, there are instructions that can be written with:\n\nprefetch on x86 architecture\n__builtin_prefetch in the GCC compiler\n_mm_prefetch in the Intel Intrinsics Guide\n",
    "source": "wikipedia",
    "title": "Prefetching",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_44409131",
    "text": "Technology transfer in computer science refers to the transfer of technology developed in computer science or applied computing research, from universities and governments to the private sector. These technologies may be abstract, such as algorithms and data structures, or concrete, such as open source software packages.\n\nExamples\nNotable examples of technology transfer in computer science include:\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Technology transfer in computer science",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_81470199",
    "text": "Thermodynamic computing refers to a new type of computing that has reached latter stages of development as of 2025. This type of computing has been pioneered and developed by the computing company Extropic.\n\nOverview\n\nSee also\nThermodynamics\nAlgorithms\nBiological computing\nReferences\n\nExternal reading\n\n",
    "source": "wikipedia",
    "title": "Thermodynamic computing",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_53650506",
    "text": "Transition refers to a computer science paradigm in the context of communication systems which describes the change of communication mechanisms, i.e., functions of a communication system, in particular, service and protocol components. In a transition, communication mechanisms within a system are replaced by functionally comparable mechanisms with the aim to ensure the highest possible quality, e.g., as captured by the quality of service.\n\nTransitions enable communication systems to adapt to changing conditions during runtime. This change in conditions can, for example, be a rapid increase in the load on a certain service that may be caused, e.g., by large gatherings of people with mobile devices. A transition often impacts multiple mechanisms at different communication layers of a layered architecture.\nMechanisms are given as conceptual elements of a networked communication system and are linked to specific functional units, for example, as a service or protocol component. In some cases, a mechanism can also comprise an entire protocol. For example on the transmission layer, LTE can be regarded as such a mechanism. Following this definition, there exist numerous communication mechanisms that are partly equivalent in their basic functionality, such as Wi-Fi, Bluetooth and Zigbee for local wireless networks and UMTS and LTE for broadband wireless connections. For example, LTE and Wi-Fi have equivalent basic functionality, but they are technologically significantly different in their design and operation. Mechanisms affected by transitions are often components of a protocol or service. For example, in case of video streaming/transmission, the use of different video data encoding can be carried out depending on the available data transmission rate. These changes are controlled and implemented by transitions; A research example is a context-aware video adaptation service to support mobile video applications. Through analyzing the current processes in a communication system, it is possible to determine which transitions need to be executed at which communication layer in order to meet the quality requirements. In order for communication systems to adapt to the respective framework conditions, architectural approaches of self-organizing, adaptive systems can be used, such as the MAPE cycle  (Monitor-Analyze-Plan-Execute). This central concept of Autonomic Computing can be used to determine the state of the communication system, to analyze the monitoring data and to plan and execute the necessary transition(s). A central goal is that users do not consciously perceive a transition while running applications and that the functionality of the used services is perceived as smooth and fluid.\n\nRecent research\nThe study of new and fundamental design methods, models and techniques that enable automated, coordinated and cross-layer transitions between functionally similar mechanisms within a communication system is the main goal of a collaborative research cente",
    "source": "wikipedia",
    "title": "Transition (computer science)",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_81998986",
    "text": "Sara Uckelman is the professor of logic at Durham University, co-director of Durham Centre for Ancient and Medieval Philosophy, and deputy director of liberal arts. Her research in formal logic focuses on formal modelling and interactive logic, bringing together techniques from logic and computer science to explore and understand reasoning in historical contexts. She is the vice-president of the British Logic Colloquium, and the editor in chief at Dictionary of Medieval Names from European Sources.\nShe is also a short story writer, published in Manawaker Studio Flash Fiction Podcast, Pilcrow & Dagger, Story Seed Vault, and The Martian Wave. Uckelman is the founder of SFF Reviews, a website that provides short reviews of short science-fiction & fantasy stories, to which she also contributes reviews.\n\nSelected publications\n\"The logic of categorematic and syncategorematic infinity\", Synthese, 2015\n\"\"Sit Verum Obligationes\" and Counterfactual Reasoning\", Vivarium (journal), 53, 2015\n\"What Problem Did Ladd-Franklin (Think She) Solve(d)?\", Notre Dame Journal of Formal Logic 62(3): 527-552 (August 2021)\n\"John Eliot's Logick Primer: A Bilingual English-Massachusett Logic Textbook\", History and Philosophy of Logic 45 (3):278-301 (2023)\n\"What Logical Consequence Could, Could Not, Should, and Should Not Be\", Aristotelian Society Supplementary Volume 98 (1):255-275 (2024)\nReferences\n\nExternal links\nSara L. Uckelman publications indexed by Google Scholar\n",
    "source": "wikipedia",
    "title": "Sara L. Uckelman",
    "topic": "Computer_science"
  },
  {
    "id": "wiki_18963870",
    "text": "Literature is any collection of written work. The term is also used more narrowly for writings considered an art form, especially novels, plays, and poems. It includes both print and digital writing. In recent centuries, the definition has expanded to include oral literature, much of which has been transcribed. Literature is a method of recording, preserving, and transmitting knowledge and entertainment. It can also have a social, psychological, spiritual, or political role.\nLiterary criticism is one of the oldest academic disciplines, and is concerned with the literary merit or intellectual significance of specific texts. The study of books and other texts as artifacts or traditions is instead encompassed by textual criticism or the history of the book. \"Literature\", as an art form, is sometimes used synonymously with literary fiction, fiction written with the goal of artistic merit, but can also include works in various non-fiction genres, such as biography, diaries, memoirs, letters, and essays. Within this broader definition, literature includes non-fictional books, articles, or other written information on a particular subject.\nDevelopments in print technology have allowed an ever-growing distribution and proliferation of written works, while the digital era has blurred the lines between online electronic literature and other forms of modern media.\n\nDefinitions\nDefinitions of literature have varied over time. In Western Europe, prior to the 18th century, literature denoted all books and writing. It can be seen as returning to older, more inclusive notions, so that cultural studies, for instance, include, in addition to canonical works, popular and minority genres. The word is also used in reference to non-written works: to \"oral literature\" and \"the literature of preliterate culture\".\nEtymologically, the term derives from Latin literatura/litteratura, \"learning, writing, grammar,\" originally \"writing formed with letters,\" from litera/littera, \"letter.\" In spite of this, the term has also been applied to spoken or sung texts. Literature is often referred to synecdochically as \"writing,\" especially creative writing, and poetically as \"the craft of writing\" (or simply \"the craft\"). Syd Field described his discipline, screenwriting, as \"a craft that occasionally rises to the level of art.\"\nA value judgment definition of literature considers it as consisting solely of high quality writing that forms part of the belles-lettres (\"fine writing\") tradition. An example of this is in the 1910–1911 Encyclopædia Britannica, which classified literature as \"the best expression of the best thought reduced to writing\".\n",
    "source": "wikipedia",
    "title": "Literature",
    "topic": "Literature"
  },
  {
    "id": "wiki_18345",
    "text": "The following outline is provided as an overview of and topical guide to literature:\nLiterature – prose, written or oral, including fiction and non-fiction, drama, and poetry.\n\nSee also the Outline of poetry.\n\nWhat type of thing is literature?\nLiterature can be described as all of the following:\n\nCommunication – activity of conveying information. Communication requires a sender, a message, and an intended recipient, although the receiver need not be present or aware of the sender's intent to communicate at the time of communication; thus communication can occur across vast distances in time and space.\nWritten communication (writing) – representation of language in a textual medium through the use of a set of signs or symbols (known as a writing system).\nSubdivision of culture – shared attitudes, values, goals, and practices that characterizes an institution, organization, or group.\nOne of the arts – imaginative, creative, or nonscientific branch of knowledge, especially as studied academically.\nEssence of literature\nComposition –\nWorld literature –\nCreative writing –\nForms of literature\n\nLiterature by region and country\n\nHistory of literature\nHistory of literature\n\nHistory of the book\nHistory of theater\nHistory of science fiction\nHistory of ideas\nIntellectual history\nGeneral literature concepts\nBook\nWestern canon –\nTeaching of writing:\nComposition –\nRhetoric –\nPoetry –\nProsody –\nMeter –\nScansion –\nConstrained writing –\nPoetics –\nVillanelle –\nSonnet –\nSestina –\nGhazal –\nBallad –\nBlank verse –\nFree verse –\nEpic poetry –\nProse –\nFiction –\nNon-fiction –\nBiography –\nProse genres –\nEssay –\nFlash prose –\nHypertext fiction –\nJournalism –\nNovel –\nNovella –\nShort story –\nTheater –\nHistory of theater –\nRhetoric –\nMetaphor –\nMetonymy –\nSymbol –\nAllegory –\nBasic procedural knowledge\nPoetry analysis –\neffective reasoning in argument writing\nNarratology\nFrame tale –\nAnecdote –\nIn Medias Res –\nPoint of view –\nLiterary criticism – an application of literary theory\nMarxist literary criticism –\nSemiotic literary interpretation –\nPsychoanalytic literary interpretation –\nFeminist literary interpretation –\nNew historicism –\nQueer literary interpretation –\n",
    "source": "wikipedia",
    "title": "Outline of literature",
    "topic": "Literature"
  },
  {
    "id": "wiki_297267",
    "text": "Allusion, or alluding, is a figure of speech that makes a reference to someone or something by name (a person, object, location, etc.) without explaining how it relates to the given context, so that the audience must realize the connection in their own minds. When a connection is directly and explicitly explained (as opposed to indirectly implied), it is instead often simply termed a reference. In the arts, a literary allusion puts the alluded text in a new context under which it assumes new meanings and denotations. Literary allusion is closely related to parody and pastiche, which are also \"text-linking\" literary devices.\nIn a wider, more informal context, an allusion is a passing or casually short statement indicating broader meaning. It is an incidental mention of something, either directly or by implication, such as \"In the stock market, he met his Waterloo.\"\n\nScope of the term\nIn the most traditional sense, allusion is a literary term, though the word has also come to encompass indirect references to any source, including allusions in film or the visual arts. In literature, allusions are used to link concepts that the reader already has knowledge of, with concepts discussed in the story. It is not possible to predetermine the nature of all the new meanings and inter-textual patterns that an allusion will generate. In the field of film criticism, a filmmaker's intentionally unspoken visual reference to another film is also called an homage. It may even be sensed that real events have allusive overtones, when a previous event is inescapably recalled by a current one. \"Allusion is bound up with a vital and perennial topic in literary theory, the place of authorial intention in interpretation\", William Irwin observed, in asking \"What is an allusion?\"\nWithout the hearer or reader comprehending the author's intention, an allusion becomes merely a decorative device. Allusion is an economical device, a figure of speech that uses a relatively short space to draw upon the ready stock of ideas, cultural memes or emotion already associated with a topic. Thus, an allusion is understandable only to those with prior knowledge of the covert reference in question, a mark of their cultural literacy.\n",
    "source": "wikipedia",
    "title": "Allusion",
    "topic": "Literature"
  },
  {
    "id": "wiki_77691928",
    "text": "Anticipatory plagiarism is a concept first introduced by the Oulipo group of poets. The concept involves the study of historical literature to uncover works which either use, or refer to, constraint- or rule-based writing methods as defined by members of the Oulipo group. The Oulipo poets called these past writers 'anticipatory plagiarists'.\nThe paradoxical concept of anticipatory plagiarism has more recently been proposed as an analytical tool with reference to Russian studies.\n\nList of anticipatory plagiarists (according to the Oulipo group)\nJonathan Swift and his literary invention called The Engine, in his 1726 novel Gulliver's Travels\nJoe Brainard's I Remember\nFrançois Rabelais' Gargantua and Pantagruel\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Anticipatory plagiarism",
    "topic": "Literature"
  },
  {
    "id": "wiki_79946569",
    "text": "Bole Botake (1947–2016) was an anglophone Cameroonian playwright, poet, and critic known for using theatre for political intervention and education.\n\nEarly life and education\nBole Butake was born in 1947 in Nkor, North West region of Cameroon. He pursued his education at Sacred Heart College Mankon and later at the University of Yaoundé, where he earned a BA in Modern English Letters(1972), a Maîtrise(1973) and a Master's degree in English Literature from the University of Leeds, England(1974). He then completed a PhD at Yaoundé in 1983, after which he taught at Yaoundé until 2012 when he retired.\nCareer\n\nWorks\n1984: The Rape of Michelle, Yaounde, CEPER (published and performed)\n1986: Lake God, Yaounde, BET & Co (Pub) Ltd. (published and performed).\n1989: The Survivors, Yaounde, Editions SOPECAM (performed and published).\n1990: And Palm-wine Will Flow, Yaounde, Editions SOPECAM (performed and published)\n1993: Shoes and Four Men in Arms translated into German as VIER MANN IN UNIFORM UND EIN BERG SCHUHE and broadcast on Westdeutscher Rundfunk Köln. Toured Germany with The Flame Players in June 1996 (four performances).\n1993: (with Gilbert Doho) Zintgraff and the Battle of Mankon, a semi-historical play (performed).\n1995: Dance of the Vampires (performed; published in 1999).\n1999: Lake God and other plays. Yaounde, Editions CLE\n2003: Zintgraff and the Battle of Manko. Bamenda, Patron Publishing House.\n2005: Family Saga. Yaounde, Editions CLE.\n2005: Betrothal Without Libation. Yaounde, Editions CLE.\n2010: Cameroon Anthology of Poetry. Yaounde, Africana Publications.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Bole Butake",
    "topic": "Literature"
  },
  {
    "id": "wiki_13607556",
    "text": "Comedy of menace is the body of plays written by David Campton, Nigel Dennis, N. F. Simpson, and Harold Pinter.  The term was coined by drama critic Irving Wardle, who borrowed it from the subtitle of Campton's play The Lunatic View: A Comedy of Menace, in reviewing Pinter's and Campton's plays in Encore in 1958.  (Campton's subtitle Comedy of Menace is a jocular play-on-words derived from comedy of manners—menace being manners pronounced with somewhat of an English accent.)\n\nBackground\nCiting Wardle's original publications in Encore magazine (1958), Susan Hollis Merritt points out that in \"Comedy of Menace\" Wardle \"first applies this label to Pinter's work … describ[ing] Pinter as one of 'several playwrights who have been tentatively lumped together as the \"non-naturalists\" or \"abstractionists\" ' (28)\". His article \"Comedy of Menace,\" Merritt continues,\n\ncenters on The Birthday Party because it is the only play of Pinter's that Wardle had seen [and reviewed] at the time, yet he speculates on the basis of \"descriptions of [Pinter's] other plays, 'The Room' and 'The Dumb Waiter', [that Pinter] is a writer dogged by one image—the womb\" (33).  Mentioning the acknowledged \"literary influences\" on Pinter's work—\"Beckett, Kafka and American gangster films\"—Wardle argues that \" 'The Birthday Party' exemplifies the type of comic menace which gave rise to this article.\" (225)\nIn \"Comedy of Menace\", as Merritt observes, on the basis of his experience of The Birthday Party and others' accounts of the other two plays, Wardle proposes that \"Comedy enables the committed agents and victims of destruction to come on and off duty; to joke about the situation while oiling a revolver; to display absurd or endearing features behind their masks of implacable resolution; to meet … in paper hats for a game of blind man's buff\"; he suggests how \"menace\" in Pinter's plays \"stands for something more substantial: destiny,\" and that destiny, \"handled in this way—not as an austere exercise in classicism, but as an incurable disease which one forgets about most of the time and whose lethal reminders may take the form of a joke—is an apt dramatic motif for an age of conditioned behaviour in which orthodox man is a willing collaborator in his own destruction\".\n\"Just two years later\" (1960), however, Wardle retracted \"Comedy of Menace\" in his review of The Caretaker, stating: \"On the strength of 'The Birthday Party' and the pair of one-acters, I rashly applied the phrase 'comedy of menace' to Pinter's writing.  I now take it back\".\nAfter Wardle's retraction of comedy of menace as he had applied it to Pinter's writing, Pinter himself also occasionally disavowed it and questioned its relevance to his work (as he  also did with his own offhand but apt statement that his plays are about \"the weasel under the cocktail cabinet\").  For example, in December 1971, in his interview with Pinter about Old Times, Mel Gussow recalled that \"After The Homecoming [Pinter] said that [he] 'couldn'",
    "source": "wikipedia",
    "title": "Comedy of menace",
    "topic": "Literature"
  },
  {
    "id": "wiki_19284595",
    "text": "Domestic realism normally refers to the genre of 19th-century fictional works about the daily lives of ordinary Victorian women. This body of writing is also known as \"sentimental fiction\" or \"woman's fiction\". The genre is mainly reflected in the novel though short-stories and non-fiction works such as Harriet Beecher Stowe's \"Our Country Neighbors\" and The New Housekeeper's Manual written by Stowe and her sister Catharine Beecher are works of domestic realism.\n\nGeneric conventions\nThe style's particular characteristics are:\n\n\"Plot focuses on a heroine who embodies one of two types of exemplar: the angel and the practical woman (Reynolds) who sometimes exist in the same work. Baym says that this heroine is contrasted with the passive woman (incompetent, cowardly, ignorant; often the heroine's mother is this type) and the \"belle,\" who is deprived of a proper education.\nThe heroine struggles for self-mastery, learning the pain of conquering her own passions (Tompkins, Sensational Designs, 172).\nThe heroine learns to balance society's demands for self-denial with her own desire for autonomy, a struggle often addressed in terms of religion.\nShe suffers at the hands of abusers of power before establishing a network of surrogate kin.\nThe plots \"repeatedly identify immersion in feeling as one of the great temptations and dangers for a developing woman. They show that feeling must be controlled. . . \" (Baym 25). Frances Cogan notes that the heroines thus undergo a full education within which to realize feminine obligations (The All-American Girl).\nThe tales generally end with marriage, usually one of two possible kinds:\nReforming the bad or \"wild\" male, as in Augusta Evans's St. Elmo (1867)\nMarrying the solid male who already meets her qualifications. Examples: Maria Cummins, The Lamplighter (1854) and Susan Warner, The Wide, Wide World (1850)\nThe novels may use a \"language of tears\" that evokes sympathy from the readers.\nRichard Brodhead (Cultures of Letters) sees class as an important issue, as the ideal family or heroine is poised between a lower-class family exemplifying poverty and domestic disorganization and upper-class characters exemplifying an idle, frivolous existence (94).\"\n",
    "source": "wikipedia",
    "title": "Domestic realism",
    "topic": "Literature"
  },
  {
    "id": "wiki_1851712",
    "text": "A film adaptation transfers the details or story of an existing source text, such as a novel, into a feature film. This transfer can involve adapting most details of the source text closely, including characters or plot points, or the original source can serve as loose inspiration, with the implementation of only a few details. Although often considered a type of derivative work, film adaptation has been conceptualized recently by academic scholars such as Robert Stam as a dialogic process.\nWhile the most common form of film adaptation is the use of a novel as the basis, other works adapted into films include non-fiction (including journalism), autobiographical works, comic books, scriptures, plays, historical sources and even other films. Adaptation from such diverse resources has been a ubiquitous practice of filmmaking since the earliest days of cinema in nineteenth-century Europe. In contrast to when making a remake, movie directors usually take more creative liberties when creating a film adaptation, changing the context of factors such as audience or genre.\n\nFidelity\n\nElision and interpolation\nIn 1924, Erich von Stroheim attempted a literal adaptation of Frank Norris's novel McTeague with his film Greed. The resulting film was 9½ hours long, and was cut to four hours at studio insistence. It was then cut again (without Stroheim's input) to around two hours. The result was a film that was largely incoherent. Since that time, few directors have attempted to put everything in a novel into a film. Therefore, elision is all but essential.\nIn some cases, film adaptations also interpolate scenes or invent characters. This is especially true when a novel is part of a literary saga. Incidents or quotations from later or earlier novels will be inserted into a single film. Additionally and far more controversially, filmmakers will invent new characters or create stories that were not present in the source material at all. Given the anticipated audience for a film, the screenwriter, director or movie studio may wish to increase character time or to invent new characters. For example, William J. Kennedy's Pulitzer Prize-winning novel Ironweed included a short appearance by a prostitute named Helen. Because the film studio anticipated a female audience for the film and had Meryl Streep for the role, Helen became a significant part of the film. However, characters are also sometimes invented to provide the narrative voice.\n",
    "source": "wikipedia",
    "title": "Film adaptation",
    "topic": "Literature"
  },
  {
    "id": "wiki_15944015",
    "text": "Futurism is a modernist avant-garde movement in literature and part of the Futurism art movement that originated in Italy in the early 20th century. It made its official literature debut with the publication of Filippo Tommaso Marinetti's Manifesto of Futurism (1909). Futurist poetry is characterised by unexpected combinations of images and by its hyper-concision (in both economy of speech and actual length). Futurist theatre also played an important role within the movement and is distinguished by scenes that are only a few sentences long, an emphasis on nonsensical humour, and attempts to examine and subvert traditions of theatre via parody and other techniques. Longer forms of literature, such as the novel, have little place in the Futurist aesthetic of speed and compression, although there are exceptions like Marinetti's Mafarka the Futurist (1909) and Aldo Palazzeschi's Man of Smoke (1911).\nFuturist literature primarily focuses on seven aspects: intuition, analogy, irony, abolition of syntax, metrical reform, onomatopoeia, and essential/synthetic lyricism. The ideals of the futurists expanded to their sculptures and painting styles as well; they were not fond of the cubism movement in France or the renaissance era progression (in their point of view, emasculation) and would often preach going back to old fashioned values in their manifestos and articles as well as their artwork. Although the movement was founded with manifestos written by men there were responses to Marinetti in particular from women whom considered themselves traditional feminists and did not see the previous renaissance movement as a shift towards emasculation, but relied too much on the traditional titles of \"men\" and \"women\" that pigeon holed society into believing they couldn't be empathetic and that a woman couldn't be vigorous.\n\nMethodology\n\nFuturism in the theatre\nTraditional theatre often served as a target for Futurists because of its deep roots in classical societies. In its stead, the Futurists exalted the variety theatre, vaudeville, and music hall because, they argued, it \"had no tradition; it was a recent discovery\". Vaudevillian acts aligned themselves well to the notions of \"stupefaction\" as there was the desire to surprise and excite the audience. Furthermore, the heavy use of machinery attracted the Futurists, as well as Vaudevillian acts' tendency to \"destroy\" the masterpieces of the past through parody and other forms of depreciation.\nBy adding other Futurist ideals mentioned above, they firmly rooted their beliefs into theatre. They wanted to blur the line between art and life in order to reach below the surface to reality. In practice, this manifested itself in various ways:\n\n\"Collaboration between the public and the actors was to be developed to the point of indistinction of roles—such cooperating confusion was to be partly impromptu…e.g. chairs were to be covered with glue so that ladies' gowns would stick to them; and tickets sold in such a way as t",
    "source": "wikipedia",
    "title": "Futurism (literature)",
    "topic": "Literature"
  },
  {
    "id": "wiki_21424551",
    "text": "The National Literary Awards defines independent or \"indie\" literature as \"books published outside mainstream publishing.\"[1] Such books are rarely recognized and hard to pin down, but some examples include \"Damastor\" by Dimitri Iatrou,  \"Returning Home\" by Marcus Blake and \"Hope...Joy (and a Few Little Thoughts) for Pregnant Teens\" by Rachel Brigoni.\n\nSee also\nIndie publisher\n",
    "source": "wikipedia",
    "title": "Indie literature",
    "topic": "Literature"
  },
  {
    "id": "wiki_79350823",
    "text": "Intelligence literature, sometimes referred to as espionage nonfiction, is a genre of non-fiction or historical literature, written in any language, that focuses on the intelligence field, and its most popular subfield known as espionage. This field of literature includes biographies and autobiographies of intelligence officers, historical research and analysis of intelligence operations and missions, studies of undercover work, policy and legal studies surrounding the fields of intelligence law, intelligence history, and national security law, academic and professional journals, essays, textbooks, and more. Other works of intelligence literature include official histories, official reports, tradecraft and technical manuals, declassified documents and archival materials, and oral histories and interviews.\n\nThe origins of intelligence literature\nIntelligence literature is rooted in the original works of myth, folklore, oral tradition, political science and philosophy, as spycraft has always been a tool of statecraft, warfare, and diplomacy, and for the majority of its history, intelligence literature was wrapped-up into these genres. In ancient cultures, there often was no distinction between fiction and nonfiction. It took hundreds to thousands of years for it to become its own, unique, genre of literature.\nMemoir and autobiography\n\nAcademic literature\n\nIntelligence literature curation and intelligence collections\nThe largest collection of intelligence history remains the Intelligence History Collection (IHC), which is housed in the CIA Library, containing over 23,000 volumes. This collection was mostly gathered by Walter Pforzheimer, who was often referred to by his honorific title in the intelligence world as \"The Dean of Intelligence Literature.\"\nIn 1954, Pforzheimer was assigned by Allen Dulles to develop the Historical Intelligence Collection, a role he held until his retirement in 1974. Pforzheimer's mandate was to develop a collection encompassing all aspects of intelligence operations and doctrine, providing a valuable resource for Agency personnel. Under Pforzheimer's leadership, the HIC expanded rapidly. In its inaugural year, the collection grew to include 3,570 volumes, with Pforzheimer personally acquiring 1,308 books during a European trip across ten countries, all for under $2,500. He defined the collection's scope to cover a wide range of topics, including military and national intelligence, espionage, counterespionage, unconventional warfare, cryptography, and various elements of intelligence tradecraft. This comprehensive approach ensured that the HIC became an unparalleled resource within the intelligence community.\nWhen he retired, the HIC had grown to 22,000 volumes, the largest professional intelligence collection in the world. His efforts not only provided CIA officers with a rich repository of historical intelligence materials but also laid the foundation for ongoing scholarly research in the field. The Collection is now a",
    "source": "wikipedia",
    "title": "Intelligence literature",
    "topic": "Literature"
  },
  {
    "id": "wiki_73826791",
    "text": "Interlingue literature broadly encompasses the body of fiction and nonfiction work created or translated into Interlingue, a constructed language created by Edgar de Wahl. Although largely composed of original short stories and translations published in the central magazine of the language, Cosmoglotta, full length novels and poetry anthologies also exist, in particular those by authors such as Vicente Costalago, Jan Amos Kajš, and Jaroslav Podobský.\n\nHistory\n\nNotable authors\n\nSee also\nEsperanto literature\nReferences\n\n",
    "source": "wikipedia",
    "title": "Interlingue literature",
    "topic": "Literature"
  },
  {
    "id": "wiki_646598",
    "text": "Literary language is the register of a language used when writing in a formal, academic, or particularly polite tone; when speaking or writing in such a tone, it can also be known as formal language. It may be the standardized variety of a language. It can sometimes differ noticeably from the various spoken lects, but the difference between literary and non-literary forms is greater in some languages than in others. If there is a strong divergence between a written form and the spoken vernacular, the language is said to exhibit diglossia.\nThe understanding of the term differs from one linguistic tradition to another and is dependent on the terminological conventions adopted.\n\nLiterary English\nFor much of its history, there has been a distinction in the English language between an elevated literary language (written) and a colloquial or vernacular language (spoken, but sometimes also represented in writing). After the Norman conquest of England, for instance, Latin and French displaced English as the official and literary languages, and standardized literary English did not emerge until the end of the Middle Ages. At this time and into the Renaissance, the practice of aureation (the introduction of terms from classical languages, often through poetry) was an important part of the reclamation of status for the English language, and many historically aureate terms are now part of general common usage. Modern English no longer has quite the same distinction between literary and colloquial registers.\nEnglish has been used as a literary language in countries that were formerly part of the British Empire, for instance in India up to the present day, Malaysia in the early 20th century and Nigeria, where English remains the official language.\nWritten in Early Modern English, the King James Bible and works by William Shakespeare from the 17th century are defined as prototype mediums of literary English and are taught in advanced English classes. Furthermore, many literary words that are used today are found in abundance in the works of Shakespeare and as well as in King James Bible, hence the literary importance of early modern English in contemporary English literature and English studies.\n",
    "source": "wikipedia",
    "title": "Literary language",
    "topic": "Literature"
  },
  {
    "id": "wiki_30792823",
    "text": "Liberature is literature in which the material form is considered an important part of the whole and essential to understanding the work.\n\nDescription of liberature\nLiberature refers to a new kind of literature, a trans-genre, in which the text and the material form of a book constitute an inseparable whole. The term itself is derived from the word 'literature', but draws from the Latin liber, meaning \"a book\" and \"the free one\", as well as libra meaning \"measurement\" or \"writing as a measurement of words.\" In a work of liberature, text does not serve as the sole source of meaning; the shape and the construction of the book, its format, the number of pages, its typographical layout, the size and type of the font applied, pictures and photographs integrated with the text, and type of paper or other material used in the process of creation of the book are all taken into consideration. The reader confronts a work of liberature as a total package, which often assumes a non-traditional shape, the quality of which, in practice, sometime involves a radical separation from the traditional design of the book. Its textual message dictates the physical shape that the work finally assumes. All of this lends a level of intent and control to the creator of liberature that surpasses that of other genres.\nThe demands posed by liberature shape a new kind of reader. In traditional texts, the reader empirically processes the text of a work in order to attain a desired level of understanding; one follows the steps of a model reader, whose purpose is to bring oneself closer to one's virtual model. Certain works diverge from this concept of linear, textual reading such as those by Umberto Eco or Roland Barthes. Such works introduce uncertainty into these structured expectations and programs of behavior. Neither the works of Eco nor Barthes constitute liberature, but they exemplify a difference in the quality of the programming of the reader's experience.\nThe printed works of liberature, although diverse in the application, share the following variables of the traversal function:\n\nOnly in several instances the work may breach the above-mentioned scheme of characteristic variables of liberature. In all other cases, readers are responsible for configuring their reading of the text.\nTexts of liberature that generate multiple statements are not limited to literary works, and definitely not to electronic texts – the category embraces computer games and other forms of both ergodic and non‐ergodic literature. The former requires a non‐trivial effort to traverse the text. In a classical, non-ergodic literary work, such as The Odyssey, the reader is required only to turn the pages and to interpret the text.\nIn permutative works – those in which the order of text is inconsequential - instead of static function defining the dynamics, something else occurs.  An intratextonic dynamic, one that guides the reader throughout the text, occurs. The text is also not determinate, meaning ",
    "source": "wikipedia",
    "title": "Liberature",
    "topic": "Literature"
  },
  {
    "id": "wiki_46629670",
    "text": "Dunce is a mild insult in English meaning \"a person who is slow at learning or stupid\". The etymology given by Richard Stanyhurst is that the word is derived from the name of the Scottish scholastic theologian and philosopher John Duns Scotus.\n\nDunce cap\nA dunce cap, also variously known as a dunce hat, dunce's cap or dunce's hat, is a pointed hat, formerly used as an article of discipline in schools in Europe and the United States—especially in the 19th and early 20th centuries—for children who were disruptive or were considered slow in learning. In the 19th century, it was seen by some as degrading: in 1831, children's book author Sidney Babcock wrote of the dunce cap as debasing and harsh, and in 1899, historian Alice Morse Earle compared it to other forms of school discipline she saw as degrading and outdated. It became unpopular in the early 20th century. However, some North American schools still permitted caps as late as the 1950s. In modern pedagogy, punishments like dunce caps have fallen out of favor: By 1927 an editorial in the Educational Research Bulletin stated: \"The rod and the cap were not eminently successful ... we have our doubts about exclusion being the solution to the problem. ... High scholarship is not produced by students who have their curiosity stifled by their teachers. Curiosity must be stimulated if scholarship is desired, and sympathy is essential to this stimulation.\"\nThe Oxford English Dictionary (3rd edition) cites mid-16th century examples of the term dunce used to describe a follower of Duns Scotus, a person engaged in ridiculous pedantry, or a person regarded as a \"fool\" or \"dimwit\". A visual depiction of the hat was first shown in the 1727 edition of The New England Primer, and the term dunce's cap is recorded as early as 1791. The first use of the term in literature was in 1840, in Charles Dickens' The Old Curiosity Shop. Scotus apparently believed that the hat would funnel knowledge into the brain, and in the centuries before his followers became unpopular, was a social signal of an intelligent person.\nThe dunce cap has also been connected with donkeys to portray the student as asinine. An engraving featured in an early 1900s textbook depicts a child sitting on a wooden donkey in an \"eighteenth-century\" classroom, wearing a dunce cap with donkey ears.\nA similar cap made of paper and called a capirote was prescribed for sinners and penitents during the Spanish Inquisition.\nThe dunce cap was also used to humiliate intellectuals and officials during the Cultural Revolution in China during the Maoist era.\n",
    "source": "wikipedia",
    "title": "Dunce",
    "topic": "Literature"
  },
  {
    "id": "wiki_2588267",
    "text": "Outdoor literature is a literature genre about or involving the outdoors. Outdoor literature encompasses several different subgenres including exploration literature, adventure literature and nature writing. Another subgenre is the guide book, an early example of which was Thomas West's guide to the Lake District published in 1778. The genres can include activities such as exploration, survival, sailing, hiking, mountaineering, whitewater boating, geocaching or kayaking, or writing about nature and the environment. Travel literature is similar to outdoor literature but differs in that it does not always deal with the out-of-doors, but there is a considerable overlap between these genres, in particular with regard to long journeys.\n\nHistory\nHenry David Thoreau's Walden (1854) is an early and influential work. Although not entirely an outdoor work (he lived in a cabin close to civilization) he expressed the ideas of why people go out into the wilderness to camp, backpack and hike: to get away from the rush of modern society and simplify life. This was a new perspective for the time and thus Walden has had a lasting influence on most outdoor authors.\n\nThoreau's careful observations and devastating conclusions have rippled into time, becoming stronger as the weaknesses Thoreau noted have become more pronounced […] Events that seem to be completely unrelated to his stay at Walden Pond have been influenced by it, including the national park system, the British labour movement, the creation of India, the civil rights movement, the hippie revolution, the environmental movement, and the wilderness movement. Today, Thoreau's words are quoted with feeling by liberals, socialists, anarchists, libertarians, and conservatives alike.\n\nRobert Louis Stevenson's Travels with a Donkey in the Cévennes (1879), about his travels in Cévennes (France), is among the first popular books to present hiking and camping as recreational activities, and tells of commissioning one of the first sleeping bags.\nIn the world of sailing Frank Cowper's Sailing Tours (1892–1896) and Joshua Slocum's Sailing Alone Around the World (1900) are classics of outdoor literature. \nIn April 1895, Joshua Slocum set sail from Boston, Massachusetts and in  Sailing Alone Around the World, he described his departure:\n\nI had resolved on a voyage around the world, and as the wind on the morning of April 24, 1895 was fair, at noon I weighed anchor, set sail, and filled away from Boston, where the Spray had been moored snugly all winter. […] A thrilling pulse beat high in me. My step was light on deck in the crisp air. I felt there could be no turning back, and that I was engaging in an adventure the meaning of which I thoroughly understood.\nMore than three years later, on June 27, 1898, he returned to Newport, Rhode Island, having circumnavigated the world, a distance of more than 46,000 miles (74,000 km).\nThe National Outdoor Book Award was established in 1997 as a US-based non-profit program which eac",
    "source": "wikipedia",
    "title": "Outdoor literature",
    "topic": "Literature"
  },
  {
    "id": "wiki_70991052",
    "text": "Phoenician–Punic literature is literature written in Phoenician, the language of the ancient civilization of Phoenicia, or in the Punic language that developed from Phoenician and was used in Ancient Carthage. Its nature and extent are extremely uncertain because there is very little direct evidence. Phoenician and Punic text survives only in inscriptions, of which very few can be interpreted as literary; on coins; and via Greek and Latin sources, such as the fragments of Sanchuniathon's History and Mago's agricultural treatise, the Greek translation of the voyage of Hanno the Navigator, and a few lines in the Poenulus by Plautus.\nThis limited evidence has led some scholars to argue that there was not a substantial literary tradition in Phoenician or Punic. However, Greek and Roman writings suggest that both Phoenicia and Carthage had extensive libraries and produced literature, including the Phoenician sources used by Greek and Latin writers like Philo of Byblos and Menander of Ephesus.\n\nHistory and sources\nThe Jewish historian Flavius Josephus alludes to the Phoenician or Tyrian chronicles that he allegedly consulted to write his historical works. Herodotus also mentioned the existence of books from Byblos and a History of Tyre preserved in the temple of Hercules-Melqart in Tyre. In addition, it is possible to find some remnants of the influence exerted by certain writings of Ugarit in some biblical books, such as the Genesis or the Book of Ruth, that had traces of poetic compositions of religious or political themes – with a markedly propagandistic or philosophical undertone.\nRufius Festus Avienius also alludes to old Punic records from which he would have drawn his reports on the voyage of Himilco. Greco-Roman sources mention a number of Punic books saved from the looting and burning of Carthage by the legions of Scipio Africanus in the spring of 146 BC. In his work Natural History, Pliny indicates that after the fall of Carthage, many of these books were handed over to the Numidian rulers and the Roman Senate ordered the translation into Latin of one text, Mago's agricultural treatise, establishing a commission under the leadership of Decimus Junius Piso.\n\nAccording to the Byzantine Encyclopedia called Suda, there was a historian of antiquity known as Charon of Carthage that wrote a collection of books: Lives of Illustrious Men, Lives of Illustrious Women, and Tyrants.\nAugustine of Hippo (who lived between the 3rd and 4th centuries AD) considered Punic as one of the main \"sapiential\" languages, along with Hebrew, Canaanite, Latin and Greek. On Punic literature, he wrote:Quae lingua si improbatur abs te, nega Punicis Libris, ut a viris doctissimus proditur, multa sapienter esse mandata memoriae (English: If you reject this language, you are denying what many scholars have acknowledged: many things have been wisely preserved from oblivion thanks to books written in Punic.)To Augustine, this literature was not only ancient but also contemporary",
    "source": "wikipedia",
    "title": "Phoenician–Punic literature",
    "topic": "Literature"
  },
  {
    "id": "wiki_7410249",
    "text": "Poetics is the study or theory of poetry, specifically the study or theory of device, structure, form, type, and effect with regards to poetry, though usage of the term can also refer to literature broadly. Poetics is distinguished from hermeneutics by its focus on the synthesis of non-semantic elements in a text rather than its semantic interpretation. Most literary criticism combines poetics and hermeneutics in a single analysis; however, one or the other may predominate given the text and the aims of the one doing the reading.\n\nHistory of Poetics\n\nSee also\n\nNotes and references\n\nFurther reading\nOlson, Charles (1950). Projective Verse. New York, NY: Poetry New York.\nCiardi, John (1959). How Does a Poem Mean?. Cambridge, MA: The Riverside Press.\nDrew, Elizabeth (1933). Discovering Poetry. New York: W.W. Norton & Company.\nHarmon, William (2003). Classic Writings on Poetry. New York: Columbia University Press.\nHashmi, Alamgir (2011). \"Eponymous Écriture and the Poetics of Reading a Transnational Epic\". Dublin Quarterly, 15.\nHobsbaum, Philip (1996). Metre, Rhythm, and Verse Form. New York: Routledge. ISBN 0-415-12267-8.\nKinzie, Mary (1999). A Poet's Guide to Poetry. Chicago: University of Chicago Press. ISBN 0-226-43739-6.\nNorman, Charles (1962). Poets on Poetry. New York: Collier Books. Original texts from 8 English poets before the 20th Century and from 8 20th Century Americans.\nOliver, Mary (1994). A Poetry Handbook. New York: Harcourt Brace & Co. ISBN 0-15-672400-6.\nOliver, Mary (1998). Rules for the Dance. Boston: Houghton Mifflin. ISBN 0-395-85086-X.\nPinsky, Robert (1999). The Sounds of Poetry. New York: Farrar, Straus and Giroux. ISBN 0-374-52617-6.\nQuinn, Arthur (1993). Figures of Speech. Hillsdale: Lawrence Erlbaum Associates. ISBN 1-880393-02-6.\nIturat, Isidro (2010). Poetics. Brazil: Indrisos.com.\nWorks cited\nT. V. F. Brogan (1993). Preminger, Alex; Brogan, T. V. F. (eds.). The New Princeton encyclopedia of poetry and poetics. Princeton, N.J: Princeton University Press. ISBN 0691021236.\n",
    "source": "wikipedia",
    "title": "Poetics",
    "topic": "Literature"
  },
  {
    "id": "wiki_20959122",
    "text": "Popular history, also called pop history, is a broad genre of historiography that takes a popular approach, aims at a wide readership, and usually emphasizes narrative, personality and vivid detail over scholarly analysis. The term is used in contradistinction to professional academic or scholarly history writing which is usually more specialized and technical and thus less accessible to the general reader.\n\nConceptualizations\nIt is proposed that popular history is a \"moral science\" in the sense that recreates the past not only for its own sake but also to underscore how history could facilitate an ethically responsible present. Some view it as history produced by authors who are better interlocutors capable of translating the language of scientificity to ordinary everyday language.\nSome scholars partly attributed the development of popular history to the increase of writers-turned-historians such as Benson Lossing, David Pae, and Mary Botham Howitt, who wrote historical events \"in good style\" and, thus, more appealing to the public.\nPopular historians\nSome popular historians are without academic affiliation while others are academics, or former academics, who have (according to one writer) \"become somehow abstracted from the academic arena, becoming cultural commentators.\" Many worked as journalists, perhaps after taking an initial degree in history. Popular historians may become nationally renowned or best-selling authors and may or may not serve the interests of particular political viewpoints in their roles as historians that write for a wide-ranging readership. Many authors of supposed official histories and authorized biographies would qualify as popular historians serving the interests of particular institutions or public figures.\nPopular historians aim to appear on the \"general lists\" of general publishers, rather than the university presses that have dominated academic publishing in recent years. Increasingly, popular historians have taken to television where they are able, often accompanying a series of documentaries with a tie-in book.\n",
    "source": "wikipedia",
    "title": "Popular history",
    "topic": "Literature"
  },
  {
    "id": "wiki_17569583",
    "text": "The sociology of literature is a subfield of the sociology of culture. It studies the social production of literature and its social implications. A notable example is Pierre Bourdieu's 1992  Les Règles de L'Art: Genèse et Structure du Champ Littéraire, translated by Susan Emanuel as Rules of Art: Genesis and Structure of the Literary Field (1996).\n\nClassical sociology\nNone of the 'founding fathers' of sociology produced a detailed study of literature, but they did develop ideas that were subsequently applied to literature by others. Karl Marx's theory of ideology has been directed at literature by Pierre Macherey, Terry Eagleton and Fredric Jameson. Max Weber's theory of modernity as cultural rationalisation, which he applied to music, was later applied to all the arts, literature included, by Frankfurt School writers such as Theodor Adorno and Jürgen Habermas. Emile Durkheim's view of sociology as the study of externally defined social facts was redirected towards literature by Robert Escarpit. Bourdieu's work is clearly indebted to Marx, Weber and Durkheim.\nLukács and the theory of the novel\nAn important first step in the sociology of literature was taken by Georg Lukács's The Theory of the Novel, first published in German in 1916, in the Zeitschrift fur Aesthetik und Allgemeine Kunstwissenschaft. In 1920 it was republished in book form and this version strongly influenced the Frankfurt School. A second edition, published in 1962, was similarly influential on French structuralism. The Theory of the Novel argued that, whilst the classical epic poem had given form to a totality of life pregiven in reality by the social integration of classical civilisation, the modern novel had become 'the epic of an age in which the extensive totality of life is no longer directly given'. The novel form is therefore organised around the problematic hero in pursuit of problematic values within a problematic world.\nLukács's second distinctive contribution to the sociology of literature was The Historical Novel, written in German but first published in Russian in 1937, which appeared in English translation in 1962. Here, Lukács argued that the early 19th century historical novel's central achievement was to represent realistically the differences between pre-capitalist past and capitalist present. This was not a matter of individual talent, but of collective historical experience, because the French Revolution and the revolutionary and Napoleonic wars had made history for the first time a mass experience. He went on to argue that the success of the 1848 revolutions led to the decline of the historical novel into 'decorative monumentalization' and the 'making private of history'. The key figures in the historical novel were thus those of the early 19th century, especially Sir Walter Scott.\nLukács was an important influence on Lucien Goldmann's Towards a Sociology of the Novel, Alan Swingewood's discussion of the sociology of the novel in Part 3 of Laurenson and Swi",
    "source": "wikipedia",
    "title": "Sociology of literature",
    "topic": "Literature"
  },
  {
    "id": "wiki_302445",
    "text": "Stylistics, a branch of applied linguistics, is the study and interpretation of texts of all types, but particularly literary texts, and spoken language with regard to their linguistic and tonal style, where style is the particular variety of language used by different individuals in different situations and settings. For example, the vernacular, or everyday language, may be used among casual friends, whereas more formal language, with respect to grammar, pronunciation or accent, and lexicon or choice of words, is often used in a cover letter and résumé and while speaking during a job interview.\nAs a discipline, stylistics links literary criticism to linguistics. It does not function as an autonomous domain on its own, and it can be applied to an understanding of literature and journalism as well as linguistics. Sources of study in stylistics may range from canonical works of writing to popular texts, and from advertising copy to news, non-fiction, and popular culture, as well as to political and religious discourse. Indeed, as recent work in critical stylistics, multimodal stylistics and mediated stylistics has made clear, non-literary texts may be of just as much interest to stylisticians as literary ones. Literariness, in other words, is here conceived as 'a point on a cline rather than as an absolute'.\nStylistics as a conceptual discipline may attempt to establish principles capable of explaining particular choices made by individuals and social groups in their use of language, such as in the literary production and reception of genre, the study of folk art, in the study of spoken dialects and registers, and can be applied to areas such as discourse analysis as well as literary criticism.\nPlain language has different features.\nCommon stylistic features are using dialogue, regional accents and individual idioms (or idiolects). Stylistically, also sentence length prevalence and language register use.\n\nEarly twentieth century\nThe analysis of literary style goes back to the study of classical rhetoric, though modern stylistics has its roots in Russian Formalism and the related Prague School of the early twentieth century.\nIn 1909, Charles Bally proposed stylistics as a distinct academic discipline to complement Saussurean linguistics. For Bally, Saussure's linguistics by itself couldn't fully describe the language of personal expression. Bally's programme fits well with the aims of the Prague School.\nTaking forward the ideas of the Russian Formalists, the Prague School built on the concept of foregrounding, where it is assumed that poetic language is considered to stand apart from non-literary background language, by means of  deviation (from the norms of everyday language) or parallelism. According to the Prague School, however, this background language isn't constant, and the relationship between poetic and everyday language is therefore always shifting.\n",
    "source": "wikipedia",
    "title": "Stylistics",
    "topic": "Literature"
  },
  {
    "id": "wiki_80723378",
    "text": "Tears of Blood (Ratwała jaswa) is a song in the Homeric tradition, originally composed and performed orally for extended family Roma audiences at the close of and in the wake of World War II. Papusza wrote it down in 1952, as a witness account of the Kali Traš (Romani Holocaust), as experienced by her extended family in wartime Volhynia. The singer-cum-poet sought to reproduce all the particularities of the oral performance of the song in the resultant form of a narrative poem that counts 1,100 linese (verses).\n\nPublication history\nIn 1956, in communist Poland, Papusza's friend Jerzy Ficowski published a quarter of Tears of Blood, both in the Romani original and his Polish translation, in a volume with Papusza's 'Gypsy songs.' It was possible to publish the volume, thanks to the political liberalization (thaw) across the Soviet bloc, including Poland. Yet, despite this short-lived ideological relaxation, the events related in Tears of Blood were highly sensitive, requiring the cutting out of three-quarters of the text before the censors permitted the volume's publication.\nSubsequently, Papusza's poems were published in such a censored form, exclusively in Polish with the omission of the Romani originals. The fall of communism in 1989 and the liquidation of censorship in democratic Poland did not change anything in this approach.\nManuscript\nPapusza's poetry is translated into other languages mainly from Ficowski's Polish translations-cum-adaptations or from the heavily censored Romani originals published in 1956.\n\nGiven the importance of Papusza's oeuvre for Romani literature and her status as originator of this literature, scholars and Roma activists postulated the faithful transcription and publication of the poet's Romani-language manuscripts, including that of Tears of Blood. But until 2020 the location of Papusza's literary estate was unknown to interested scholars. Nowadays, Papusza's papers are archivized and made available to researchers in the Zbigniew Herbert Regional and Municipal Public Library (Wojewódzka i Miejska Biblioteka Publiczna im. Zbigniewa Herberta) in Gorzów Wielkopolski in north-western Poland.\n",
    "source": "wikipedia",
    "title": "Tears of Blood (Ratwała jaswa)",
    "topic": "Literature"
  },
  {
    "id": "wiki_32923",
    "text": "The Western canon is the embodiment of high-culture literature, music, philosophy, and works of art that are highly cherished across the Western world, such works having achieved the status of classics.\nRecent discussions upon the matter emphasise cultural diversity within the canon. The canons of music and visual arts have been broadened to encompass often overlooked periods, whilst recent media like cinema grapple with a precarious position. Criticism arises, with some viewing changes as prioritising activism over aesthetic values, often associated with critical theory, as well as postmodernism. Another critique highlights a narrow interpretation of the West, dominated by British and American culture, at least under contemporary circumstances, prompting demands for a more diversified canon amongst the hemisphere.\nThere is no official list of works that a recognized panel of experts or scholars agreed upon that is \"the Western Canon,\" nor has there ever been such. A corpus of great works is an idea that has been discussed, negotiated, and criticized for the past century.\n\nLiterary canon\n\nCanon of philosophers\nMany philosophers today agree that Greek philosophy has influenced much of Western culture since its inception. Alfred North Whitehead once noted: \"The safest general characterization of the European philosophical tradition is that it consists of a series of footnotes to Plato.\" Clear, unbroken lines of influence lead from ancient Greek and Hellenistic philosophers to Early Islamic philosophy, the European Renaissance, and the Age of Enlightenment.\nPlato was a philosopher in Classical Greece and the founder of the Academy in Athens. He is widely considered the most pivotal figure in the development of philosophy, especially the Western tradition.\nAristotle was an ancient Greek philosopher. His writings cover many subjects – including physics, biology, zoology, metaphysics, logic, ethics, aesthetics, rhetoric, linguistics, politics and government—and constitute the first comprehensive system of Western philosophy. Aristotle's views on physical science had a profound influence on medieval scholarship. Their influence extended from Late Antiquity into the Renaissance, and his views were not replaced systematically until the Enlightenment and theories such as classical mechanics. In metaphysics, Aristotelianism profoundly influenced Judeo-Islamic philosophical and theological thought during the Middle Ages and continues to influence Christian theology, especially the Neoplatonism of the Early Church and the scholastic tradition of the Roman Catholic Church. Aristotle was well known among medieval Muslim intellectuals and revered as \"The First Teacher\" (Arabic: المعلم الأول). His ethics, though always influential, gained renewed interest with the modern advent of virtue ethics.\nBoethius' On the Consolation of Philosophy (Latin: De consolatione philosophiae) is often acclaimed as a central work from Late Antiquity, at the cusp of the early mediev",
    "source": "wikipedia",
    "title": "Western canon",
    "topic": "Literature"
  },
  {
    "id": "wiki_14400",
    "text": "The history of science covers the development of science from ancient times to the present. It encompasses all three major branches of science: natural, social, and formal. Protoscience, early sciences, and natural philosophies such as alchemy and astrology that existed during the Bronze Age, Iron Age, classical antiquity and the Middle Ages, declined during the early modern period after the establishment of formal disciplines of science in the Age of Enlightenment.\nThe earliest roots of scientific thinking and practice can be traced to Ancient Egypt and Mesopotamia during the 3rd and 2nd millennia BCE. These civilizations' contributions to mathematics, astronomy, and medicine influenced later Greek natural philosophy of classical antiquity, wherein formal attempts were made to provide explanations of events in the physical world based on natural causes. After the fall of the Western Roman Empire, knowledge of Greek conceptions of the world deteriorated in Latin-speaking Western Europe during the early centuries (400 to 1000 CE) of the Middle Ages, but continued to thrive in the Greek-speaking Byzantine Empire. Aided by translations of Greek texts, the Hellenistic worldview was preserved and absorbed into the Arabic-speaking Muslim world during the Islamic Golden Age. The recovery and assimilation of Greek works and Islamic inquiries into Western Europe from the 10th to 13th century revived the learning of natural philosophy in the West. Traditions of early science were also developed in ancient India and separately in ancient China, the Chinese model having influenced Vietnam, Korea and Japan before Western exploration. Among the Pre-Columbian peoples of Mesoamerica, the Zapotec civilization established their first known traditions of astronomy and mathematics for producing calendars, followed by other civilizations such as the Maya.\nNatural philosophy was transformed by the Scientific Revolution that transpired during the 16th and 17th centuries in Europe, as new ideas and discoveries departed from previous Greek conceptions and traditions. The New Science that emerged was more mechanistic in its worldview, more integrated with mathematics, and more reliable and open as its knowledge was based on a newly defined scientific method. More \"revolutions\" in subsequent centuries soon followed. The chemical revolution of the 18th century, for instance, introduced new quantitative methods and measurements for chemistry. In the 19th century, new perspectives regarding the conservation of energy, age of Earth, and evolution came into focus. And in the 20th century, new discoveries in genetics and physics laid the foundations for new sub disciplines such as molecular biology and particle physics. Moreover, industrial and military concerns as well as the increasing complexity of new research endeavors ushered in the era of \"big science,\" particularly after World War II.\n\nApproaches to history of science\nThe nature of the history of science - including both",
    "source": "wikipedia",
    "title": "History of science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1802769",
    "text": "The history and philosophy of science (HPS) is an academic discipline that encompasses the philosophy of science and the history of science. Although many scholars in the field are trained primarily as either historians or as philosophers, there are degree-granting departments of HPS at several prominent universities. Though philosophy of science and history of science are their own disciplines, history and philosophy of science is a discipline in its own right.\nPhilosophy of science is a branch of philosophy concerned with the foundations, methods, and implications of science. The central questions of this study concern what qualifies as science, the reliability of scientific theories, and the ultimate purpose of science. This discipline overlaps with metaphysics/ontology and epistemology, for example, when it explores the relationship between science and truth. Philosophy of science focuses on metaphysical, epistemic and semantic aspects of science. Ethical issues such as bioethics and scientific misconduct are often considered ethics or science studies rather than philosophy of science.\nThere is no consensus among philosophers about many of the central problems concerned with the philosophy of science, including whether science can reveal the truth about unobservable things and whether scientific reasoning can be justified at all. In addition to these general questions about science as a whole, philosophers of science consider problems that apply to particular sciences (such as astronomy, biology, chemistry, Earth science, or physics). Some philosophers of science also use contemporary results in science to reach conclusions about philosophy itself.\n\nHistory\nOne origin of the unified discipline is the historical approach to the discipline of the philosophy of science. This hybrid approach is reflected in the career of Thomas Kuhn. His first permanent appointment, at the University of California, Berkeley, was to a position advertised by the philosophy department, but he also taught courses from the history department. When he was promoted to full professor in the history department only, Kuhn was offended at the philosophers' rejection because \"I sure as hell wanted to be there, and it was my philosophy students who were working with me, not on philosophy but on history, were nevertheless my more important students\". This attitude is also reflected in his historicist approach, as outlined in Kuhn's seminal Structure of Scientific Revolutions (1962, 2nd ed. 1970), wherein philosophical questions about scientific theories and, especially, theory change are understood in historical terms, employing concepts such as paradigm shift.\nHowever, Kuhn was also critical of attempts fully to unify the methods of history and philosophy of science: \"Subversion is not, I think, too strong a term for the likely result of an attempt to make the two fields into one. They differ in a number of their central constitutive characteristics, of which the most general",
    "source": "wikipedia",
    "title": "History and philosophy of science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_14285",
    "text": "The history of science and technology (HST) is a field of history that examines the development of the understanding of the natural world (science) and humans' ability to manipulate it (technology) at different points in time. This academic discipline also examines the cultural, economic, and political context and impacts of scientific practices; it likewise may study the consequences of new technologies on existing scientific fields.\n\nAcademic study of history of science\nHistory of science is an academic discipline with an international community of specialists. Main professional organizations for this field include the History of Science Society, the British Society for the History of Science, and the European Society for the History of Science.\nMuch of the study of the history of science has been devoted to answering questions about what science is, how it functions, and whether it exhibits large-scale patterns and trends.\nUniversities with history of science and technology programs\n\nProminent historians of the field\nSee also the list of George Sarton medalists.\nJournals and periodicals\nAnnals of Science\nThe British Journal for the History of Science\nCentaurus\nDynamis\nHistory and Technology (magazine)\nHistory of Science and Technology (journal)\nHistory of Technology (book series)\nHistorical Studies in the Physical and Biological Sciences (HSPS)\nHistorical Studies in the Natural Sciences (HSNS)\nHoST - Journal of History of Science and Technology\nICON\nIEEE Annals of the History of Computing\nIsis\nJournal of the History of Biology\nJournal of the History of Medicine and Allied Sciences\nNotes and Records of the Royal Society\nOsiris\nScience & Technology Studies\nScience in Context\nScience, Technology, & Human Values\nSocial History of Medicine\nSocial Studies of Science\nTechnology and Culture\nTransactions of the Newcomen Society\nHistoria Mathematica\nBulletin of the Scientific Instrument Society\nSee also\nHistory of science\nHistory of technology\nAncient Egyptian technology\nHistory of science and technology in China\nHistory of science and technology in Japan\nHistory of science and technology in France\nHistory of science and technology in the Indian subcontinent\nMesopotamian science\nProductivity improving technologies (historical)\nScience and technology in Argentina\nScience and technology in Canada\nScience and technology in Iran\nScience and technology in the United States\nScience in the medieval Islamic world\nScience tourism\nTechnological and industrial history of the United States\nTimeline of science and engineering in the Islamic world\n",
    "source": "wikipedia",
    "title": "History of science and technology",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_56498399",
    "text": "The Academic Family Tree, which began as Neurotree, is an online database for academic genealogy, containing numerous \"family trees\" of academic disciplines. Neurotree was established in 2005 as a family tree of neuroscientists. Later that year Academic Family Tree incorporated Neurotree and family trees of other scholarly disciplines.\nUnlike a conventional genealogy or family tree, in which connections among individuals are from kinship (e.g., parents to children), connections in Academic Family Tree are from mentoring relationships, usually among people working in academic settings (e.g., doctoral supervisors to students).\nAcademic Family Tree has been used as sources of information for the history and prospects of academic fields such as psychology, meteorology, organizational communication, and neuroscience. It has been used to address infometrics, to research issues of scientific methodology, and to examine mentor characteristics that predict mentee academic success.\n\nFunctioning and scope\nThe founders of the initial trees, including Neurotree, populated them from published sources, such as ProQuest. Later, they set up discipline-specific family trees of Academic Family Tree to be volunteer-run; accuracy is maintained by a group of volunteer editors. Hierarchical connections between mentors (\"parents\") and mentees (\"children\") are defined as any meaningful mentoring relationship (research assistant, graduate student, postdoctoral fellow, or research scientist). Continuous records extend well into the Middle Ages and earlier.\nAs of 29 September 2023, Academic Family Tree contained 871,361 people with 882,278 connections among them.\nAcademic Family Tree encompasses a broad range of discipline-specific trees. As of 29 September 2023, there were 73 trees spanning science (e.g., human genetics, microbiology, and psychology), mathematics and philosophy, engineering, the humanities (e.g., economics, law, theology, and music), and business (e.g., organizational communication and advertising).\nAll trees within Academic Family Tree are closely linked. A search for a person in one tree gives hits from all trees in Academic Family Tree.\nThe data in Academic Family Tree are owned by the nonprofit academictree.org, but they are shared under the Creative Commons License (CC-BY 3.0). This means a person may use the data in any tree for any purpose as long as the source is cited.\n",
    "source": "wikipedia",
    "title": "Academic Family Tree",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_4083089",
    "text": "An academic genealogy (or scientific genealogy) organizes a family tree of scientists and scholars according to mentoring relationships, often in the form of dissertation supervision relationships, and not according to genetic relationships as in conventional genealogy. Since the term academic genealogy has now developed this specific meaning, its additional use to describe a more academic approach to conventional genealogy would be ambiguous, so the description scholarly genealogy is now generally used in the latter context.\n\nOverview\nThe academic lineage or academic ancestry of someone is a chain of professors who have served as academic mentors or thesis advisors of each other, ending with the person in question. Many genealogical terms are often recast in terms of academic lineages, so one may speak of academic descendants, children,  siblings, etc. One method of developing an academic genealogy is to organize individuals by prioritizing their degree of relationship to a mentor/advisor as follows: (1). doctoral students, (2). post-doctoral researchers, (3). master's students and (4). current students, including undergraduate researchers.\nThrough the 19th century, particularly for graduates in sciences such as chemistry, it was common to have completed a degree in medicine or pharmacy before continuing with post-graduate or post-doctoral studies. Until the early 20th century, attaining professorial status or mentoring graduate students did not necessarily require a doctorate or graduate degree. For instance, the University of Cambridge did not require a formal doctoral thesis until 1919, and academic genealogies that include earlier Cambridge students tend to substitute an equivalent mentor. Academic genealogies are particularly easy to research in the case of Spain's doctoral degrees, because until 1954 only Complutense University had the power to grant doctorates. This means that all holders of a doctorates in Spain can trace back their academic lineage to a doctoral supervisor who was a member of Complutense's Faculty.\nWebsites such as the Mathematics Genealogy Project or the Chemical Genealogy document academic lineages for specific subject areas, while some other sites, such as Neurotree and Academic Family Tree aim to provide a complete academic genealogy across all fields of academia.\n",
    "source": "wikipedia",
    "title": "Academic genealogy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1949268",
    "text": "According to ancient and medieval science, aether (, alternative spellings include æther, aither, and ether), also known as the fifth element or quintessence, is the material that fills the region of the universe beyond the terrestrial sphere. The concept of aether was used in several theories to explain several natural phenomena, such as the propagation of light and gravity. In the late 19th century, physicists postulated that aether permeated space, providing a medium through which light could travel in a vacuum, but evidence for the presence of such a medium was not found in the Michelson–Morley experiment, and this result has been interpreted to mean that no luminiferous aether exists.\n\nMythological origins\nThe word αἰθήρ (aithḗr) in Homeric Greek means \"pure, fresh air\" or \"clear sky\". In Greek mythology, it was thought to be the pure essence that the gods breathed, filling the space where they lived, analogous to the air breathed by mortals. It is also personified as a deity, Aether, the son of Erebus and Nyx in traditional Greek mythology. Aether is related to αἴθω \"to incinerate\", and intransitive \"to burn, to shine\" (related is the name Aithiopes (Ethiopians; see Aethiopia), meaning \"people with a burnt (black) visage\").\nFifth element\nIn Plato's Timaeus (58d) speaking about air, Plato mentions that \"there is the most translucent kind which is called by the name of aether (αἰθήρ)\" but otherwise he adopted the classical system of four elements. Aristotle, who had been Plato's student at the Academy, agreed on this point with his former mentor, emphasizing additionally that fire has sometimes been mistaken for aether. However, in his Book On the Heavens he introduced a new \"first\" element to the system of the classical elements of Ionian philosophy. He noted that the four terrestrial classical elements were subject to change and naturally moved linearly. The first element however, located in the celestial regions and heavenly bodies, moved circularly and had none of the qualities the terrestrial classical elements had. It was neither hot nor cold, neither wet nor dry. With this addition the system of elements was extended to five and later commentators started referring to the new first one as the fifth and also called it aether, a word that Aristotle had used in On the Heavens and the Meteorology.\nAether differed from the four terrestrial elements; it was incapable of motion of quality or motion of quantity. Aether was only capable of local motion. Aether naturally moved in circles, and had no contrary, or unnatural, motion. Aristotle also stated that celestial spheres made of aether held the stars and planets. The idea of aethereal spheres moving with natural circular motion led to Aristotle's explanation of the observed orbits of stars and planets in perfectly circular motion.\nMedieval scholastic philosophers granted aether changes of density, in which the bodies of the planets were considered to be more dense than the medium which fille",
    "source": "wikipedia",
    "title": "Aether (classical element)",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_573",
    "text": "Alchemy (from the Arabic word al-kīmīā, الكیمیاء) is an ancient branch of natural philosophy, a philosophical and protoscientific tradition that was historically practised in China, India, the Muslim world, and Europe. In its Western form, alchemy is first attested in a number of pseudepigraphical texts written in Greco-Roman Egypt during the first few centuries AD. Greek-speaking alchemists often referred to their craft as \"the Art\" (τέχνη) or \"Knowledge\" (ἐπιστήμη), and it was often characterised as mystic (μυστική), sacred (ἱɛρά), or divine (θɛíα).\nAlchemists attempted to purify, mature, and perfect certain materials. Common aims were chrysopoeia, the transmutation of \"base metals\" (e.g., lead) into \"noble metals\" (particularly gold); the creation of an elixir of immortality; and the creation of panaceas able to cure any disease. The perfection of the human body and soul was thought to result from the alchemical magnum opus (\"Great Work\"). The concept of creating the philosophers' stone was variously connected with all of these projects.\nIslamic and European alchemists developed a basic set of laboratory techniques, theories, and terms, some of which are still in use today. They did not abandon the Ancient Greek philosophical idea that everything is composed of four elements, and they tended to guard their work in secrecy, often making use of cyphers and cryptic symbolism. In Europe, the 12th-century translations of medieval Islamic works on science and the rediscovery of Aristotelian philosophy gave birth to a flourishing tradition of Latin alchemy. This late medieval tradition of alchemy would go on to play a significant role in the development of early modern science (particularly chemistry and medicine).\nModern discussions of alchemy are generally split into an examination of its exoteric practical applications and its esoteric spiritual aspects, despite criticisms by scholars such as Eric J. Holmyard and Marie-Louise von Franz that they should be understood as complementary. The former is pursued by historians of the physical sciences, who examine the subject in terms of early chemistry, medicine, and charlatanism, and the philosophical and religious contexts in which these events occurred. The latter interests historians of esotericism, psychologists, and some philosophers and spiritualists. The subject has also made an ongoing impact on literature and the arts.\n\nEtymology\nThe word alchemy comes from Old French alkimie, used in Medieval Latin as alchymia. This name was itself adopted from the Arabic word al-kīmiyā (الكيمياء). The Arabic al-kīmiyā in turn was a borrowing of the Late Greek term khēmeía (χημεία), also spelled khumeia (χυμεία) and khēmía (χημία), with al- being the Arabic definite article 'the'. Together this association can be interpreted as 'the process of transmutation by which to fuse or reunite with the divine or original form'. Several etymologies have been proposed for the Greek term. The first was proposed by Zosimos",
    "source": "wikipedia",
    "title": "Alchemy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_39480604",
    "text": "The history of anthropometry includes its use as an early tool of anthropology, use for identification, use for the purposes of understanding human physical variation in paleoanthropology and in various attempts to correlate physical with racial and psychological traits. At various points in history, certain anthropometrics have been cited by advocates of discrimination and eugenics often as a part of some social movement or through pseudoscientific claims.\n\nCraniometry and paleoanthropology\nIn 1716 Louis-Jean-Marie Daubenton, who wrote many essays on comparative anatomy for the Académie française, published his Memoir on the Different Positions of the Occipital Foramen in Man and Animals (Mémoire sur les différences de la situation du grand trou occipital dans l'homme et dans les animaux). Six years later Pieter Camper (1722–1789), distinguished both as an artist and as an anatomist, published some lectures that laid the foundation of much work. Camper invented the \"facial angle,\" a measure meant to determine intelligence among various species. According to this technique, a \"facial angle\" was formed by drawing two lines: one horizontally from the nostril to the ear; and the other perpendicularly from the advancing part of the upper jawbone to the most prominent part of the forehead. Camper's measurements of facial angle were first made to compare the skulls of men with those of other animals. Camper claimed that antique statues presented an angle of 90°, Europeans of 80°, Central Africans of 70° and the orangutan of 58°.\nSwedish professor of anatomy Anders Retzius (1796–1860) first used the cephalic index in physical anthropology to classify ancient human remains found in Europe. He classed skulls in three main categories; \"dolichocephalic\" (from the Ancient Greek kephalê \"head\", and dolikhos \"long and thin\"), \"brachycephalic\" (short and broad) and \"mesocephalic\" (intermediate length and width). Scientific research was continued by Étienne Geoffroy Saint-Hilaire (1772–1844) and Paul Broca (1824–1880), founder of the Anthropological Society in France in 1859. Paleoanthropologists still rely upon craniofacial anthropometry to identify species in the study of fossilized hominid bones. Specimens of Homo erectus and athletic specimens of Homo sapiens, for example, are virtually identical from the neck down but their skulls can easily be told apart.\n\nSamuel George Morton (1799–1851), whose two major monographs were the Crania Americana (1839), An Inquiry into the Distinctive Characteristics of the Aboriginal Race of America and Crania Aegyptiaca (1844) concluded that the ancient Egyptians were not Negroid but Caucasoid and that Caucasians and Negroes were already distinct three thousand years ago. Since The Bible indicated that Noah's Ark had washed up on Mount Ararat only a thousand years before this Noah's sons could not account for every race on earth. According to Morton's theory of polygenism the races had been separate from the start. Josiah C.",
    "source": "wikipedia",
    "title": "History of anthropometry",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_89732",
    "text": "The Antikythera mechanism ( AN-tik-ih-THEER-ə, US also  AN-ty-kih-) is an ancient Greek hand-powered orrery (model of the Solar System). It is the oldest known example of an analogue computer. It could be used to predict astronomical positions and eclipses decades in advance. It could also be used to track the four-year cycle of athletic games similar to an olympiad, the cycle of the ancient Olympic Games.\nThe artefact was among wreckage retrieved from a shipwreck off the coast of the Greek island Antikythera in 1901. In 1902, during a visit to the National Archaeological Museum in Athens, it was noticed by Greek politician Spyridon Stais as containing a gear, prompting the first study of the fragment by his cousin, Valerios Stais, the museum director. The device, housed in the remains of a wooden-framed case of (uncertain) overall size 34 cm × 18 cm × 9 cm (13.4 in × 7.1 in × 3.5 in), was found as one lump, later separated into three main fragments which are now divided into 82 separate fragments after conservation efforts. Four of these fragments contain gears, while inscriptions are found on many others. The largest gear is about 13 cm (5 in) in diameter and originally had 223 teeth. All these fragments of the mechanism are kept at the National Archaeological Museum, along with reconstructions and replicas, to demonstrate how it may have looked and worked.\nIn 2005, a team from Cardiff University led by Mike Edmunds used computer X-ray tomography and high resolution scanning to image inside fragments of the crust-encased mechanism and read faint inscriptions that once covered the outer casing. These scans suggest that the mechanism had 37 meshing bronze gears enabling it to follow the movements of the Moon and the Sun through the zodiac, to predict eclipses and to model the irregular orbit of the Moon, where the Moon's velocity is higher in its perigee than in its apogee. This motion was studied in the 2nd century BC by astronomer Hipparchus of Rhodes, and he may have been consulted in the machine's construction. There is speculation that a portion of the mechanism is missing and it calculated the positions of the five classical planets. The inscriptions were further deciphered in 2016, revealing numbers connected with the synodic cycles of Venus and Saturn.\nThe instrument is believed to have been designed and constructed by Hellenistic scientists and been variously dated to about 87 BC, between 150 and 100 BC, or 205 BC. It must have been constructed before the shipwreck, which has been dated by multiple lines of evidence to approximately 70–60 BC. In 2022, researchers proposed its initial calibration date, not construction date, could have been 23 December 178 BC. Other experts propose 204 BC as a more likely calibration date. Machines with similar complexity did not appear again until the 14th century in western Europe.\n\nHistory\n\n",
    "source": "wikipedia",
    "title": "Antikythera mechanism",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_20860737",
    "text": "Antiquarian science books are original historical works (e.g., books or technical papers) concerning science, mathematics and sometimes engineering. These books are important primary references for the study of the history of science and technology, they can provide valuable insights into the historical development of the various fields of scientific inquiry (History of science, History of mathematics, etc.)\n\nThe landmark are significant first (or early) editions typically worth hundreds or thousands of dollars (prices may vary widely based on condition, etc.). \nReprints of these books are often available, for example from Great Books of the Western World, Dover Publications or Google Books.\nIncunabula are extremely rare and valuable, but as the Scientific Revolution is only taken to have started around the 1540s, such works of Renaissance literature (including alchemy, Renaissance magic, etc.) are not usually included under the notion of \"scientific\" literature. Printed originals of the beginning Scientific Revolution thus date to the 1540s or later, notably beginning with the original publication of Copernican heliocentrism. Nicolaus Copernicus' De revolutionibus orbium coelestium of 1543 sold for more than US$2 million at auctions.\n\nList of notable books\n\nReferences\n\nExternal links\nHeralds of Science - Smithsonian Libraries\nMilestones of Science Books - Antiquarian booksellers\n",
    "source": "wikipedia",
    "title": "Antiquarian science books",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1018868",
    "text": "Astrology and astronomy were archaically treated together (Latin: astrologia), but gradually distinguished through the Late Middle Ages into the Age of Reason. Developments in 17th century philosophy resulted in astrology and astronomy operating as independent pursuits by the 18th century.\nWhereas the academic discipline of astronomy studies observable phenomena beyond the Earth's atmosphere, astrology uses the apparent positions of celestial objects as the basis for divination.\n\nOverview\nIn pre-modern times, most cultures did not make a clear distinction between the two disciplines, putting them both together as one. In ancient Babylonia, famed for its astrology, there were not separate roles for the astronomer as predictor of celestial phenomena, and the astrologer as their interpreter; both functions were performed by the same person. In ancient Greece, pre-Socratic thinkers such as Anaximander, Xenophanes, Anaximenes, and Heraclides speculated about the nature and substance of the stars and planets. Astronomers such as Eudoxus (contemporary with Plato) observed planetary motions and cycles, and created a geocentric cosmological model that would be accepted by Aristotle. This model generally lasted until Ptolemy, who added epicycles to explain the retrograde motion of Mars. (Around 250 BC, Aristarchus of Samos postulated a proto-heliocentric theory, which would not be reconsidered for nearly two millennia (Copernicus), as Aristotle's geocentric model continued to be favored.) The Platonic school promoted the study of astronomy as a part of philosophy because the motions of the heavens demonstrate an orderly and harmonious cosmos. In the third century BC, Babylonian astrology began to make its presence felt in Greece. Astrology was criticized by Hellenistic philosophers such as the Academic Skeptic Carneades and Middle Stoic Panaetius. However, the notions of the Great Year (when all the planets complete a full cycle and return to their relative positions) and eternal recurrence were Stoic doctrines that made divination and fatalism possible.\nIn the Hellenistic world, the Greek words 'astrologia' and 'astronomia' were often used interchangeably, but they were conceptually not the same. Plato taught about 'astronomia'  and stipulated that planetary phenomena should be described by a geometrical model. The first solution was proposed by Eudoxus. Aristotle favored a physical approach and adopted the word 'astrologia'. Eccentrics and epicycles came to be thought of as useful fictions. For a more general public, the distinguishing principle was not evident and either word was acceptable. For the Babylonian horoscopic practice, the words specifically used were 'apotelesma' and 'katarche', but otherwise it was subsumed under the aristotelian term 'astrologia'.\nIn his compilatory work Etymologiae, Isidore of Seville noted explicitly the difference between the terms astronomy and astrology (Etymologiae, III, xxvii) and the same distinction appeared late",
    "source": "wikipedia",
    "title": "Astrology and astronomy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_21671434",
    "text": "Barlow's law is an incorrect physical law proposed by Peter Barlow in 1825 to describe the ability of wires to conduct electricity. It says that the strength of the effect of electricity passing through a wire varies inversely with the square root of its length and directly with the square root of its cross-sectional area, or, in modern terminology:\n\n  \n    \n      \n        I\n        ∝\n        \n          \n            \n              A\n              L\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle I\\propto {\\sqrt {\\frac {A}{L}}},}\n  \n\nwhere I is electric current, A is the cross-sectional area of the wire, and L is the length of the wire. Barlow formulated his law in terms of the diameter d of a cylindrical wire.  Since A is proportional to the square of d the law becomes \n  \n    \n      \n        I\n        ∝\n        d\n        \n          /\n        \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle I\\propto d/{\\sqrt {L}}}\n  \n for cylindrical wires.\nBarlow undertook his experiments with the aim of determining whether long-distance telegraphy was feasible and believed that he proved that it was not. The publication of Barlow's law delayed research into telegraphy for several years, until 1831 when Joseph Henry and Philip Ten Eyck constructed a circuit 1,060 feet long, which used a large battery to activate an electromagnet. Barlow did not investigate the dependence of the current strength on electric tension (that is, voltage). He endeavoured to keep this constant, but admitted there was some variation. Barlow was not entirely certain that he had found the correct law, writing \"the discrepancies are rather too great to enable us to say, with confidence, that such is the law in question.\"\nIn 1827, Georg Ohm published a different law, in which current varies inversely with the wire's length, not its square root; that is,\n\n  \n    \n      \n        I\n        ∝\n        \n          \n            1\n            \n              c\n              +\n              L\n              \n                /\n              \n              A\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle I\\propto {\\frac {1}{c+L/A}},}\n  \n\nwhere \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n is a constant dependent on the circuit setup. Ohm's law is now considered the correct law, and Barlow's false.\nThe law Barlow proposed was not in error due to poor measurement; in fact, it fits Barlow's careful measurements quite well.  Heinrich Lenz pointed out that Ohm took into account \"all the conducting resistances … of the circuit\", whereas Barlow did not. Ohm explicitly included a term for what we would now call the internal resistance of the battery. Barlow did not have this term and approximated the results with a power law instead. Ohm's law in modern usage is rarely stated with this explicit term, but nevertheless an awareness of it is necessary for a full understanding of the current in a circuit.\n\n\n==",
    "source": "wikipedia",
    "title": "Barlow's law",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_64310512",
    "text": "Bharat Ki Chhap (Identity of India) is a 13-episode Indian TV science documentary series chronicling the history of science and technology in India from pre-historic times until the present. It was directed by filmmaker Chandita Mukherjee and funded by the Department of Science and Technology's National Council for Science and Technology Communication (NCSTC) in 1987. It was telecasted on Doordarshan every Sunday Morning. It was introduced by Professor Yash Pal.\nIt projected in a pragmatic way alternative viewpoints on the subject of science as pioneered in India, in contrast with western scientific endeavours. It drew support from People's Science Movement.\nA companion book was later published by Comet Project titled Bhārat Ki Chhāp: A Companion Book to the Film Series by Chayanika Shah, Suhas Paranjape, Swatija Manorama.\n\nEpisodes\nA total of 13 episodes were released.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Bharat Ki Chhap",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_777174",
    "text": "Big science is a term used by scientists and historians of science to describe a series of changes in science which occurred in industrial nations during and after World War II, as scientific progress increasingly came to rely on large-scale projects usually funded by national governments or groups of governments. Individual or small group efforts, or small science, are still relevant today as theoretical results by individual authors may have a significant impact, but very often the empirical verification requires experiments using constructions, such as the Large Hadron Collider, costing between $5 and $10 billion.\n\nDevelopment\nWhile science and technology have always been important to and driven by warfare, the increase in military funding of science following the second World War was on a scale wholly unprecedented.  James Conant, in a 1941 letter to Chemical Engineering News, said that World War II \"is a physicist's war rather than a chemist's,\" a phrase that was cemented in the vernacular in post-war discussion of the role that those scientists played in the development of new weapons and tools, notably the proximity fuse, radar, and the atomic bomb.  The bulk of these last two activities took place in a new form of research facility: the government-sponsored laboratory, employing thousands of technicians and scientists, managed by universities (in this case, the University of California and the Massachusetts Institute of Technology).\nThe need of a strong scientific research establishment was obvious in the shadow of the first atomic weapons to any country seeking to play a prominent role in world affairs. After the success of the Manhattan Project, governments became the chief patron of science, and the character of the scientific establishment underwent several key changes. This was especially marked in the United States and the Soviet Union during the Cold War, but also to a lesser extent in many other countries.\nDefinitions\n\"Big science\" usually implies one or more of these specific characteristics:\n\nBig budgets: No longer required to rely on philanthropy or industry, scientists were able to use budgets on an unprecedented scale for basic research.\nBig staffs: Similarly, the number of practitioners of science on any one project grew as well, creating difficulty, and often controversy, in the assignment of credit for scientific discoveries (the Nobel Prize system, for example, allows awarding only three individuals in any one topic per year, based on a 19th-century model of the scientific enterprise).\nBig machines: Ernest Lawrence's cyclotron at his Radiation Laboratory in particular ushered in an era of massive machines (requiring massive staffs and budgets) as the tools of basic scientific research. The use of many machines, such as the many sequencers used during the Human Genome Project, might also fall under this definition.\n\nBig laboratories: Because of the increase in cost to do basic science (with the increase of large machines),",
    "source": "wikipedia",
    "title": "Big science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_31468613",
    "text": "The Book of Nature (Lat. liber naturae/liber mundi, Ar. kitāb takwīnī) is a religious and philosophical cosmological metaphor known from Antiquity in various cultures, and prominent in the Latin and Romance literature of the European Middle Ages. The idea of a cosmos formed by letters is already found in the fragments of Heraclitus, where it relates to the Greek concept of logos, in Plato’s Timaeus, and in Lucretius’ De rerum natura.\nThe metaphor of the Book of Nature straddles the divide between religion and science, viewing nature as a readable text open to knowledge and understanding. Early theologians, such as St. Paul, believed the Book of Nature was a source of God's revelation to humankind. He believed that when read alongside sacred scripture, the \"book\" and the study of God's creations would lead to a knowledge of God himself. This type of revelation is often referred to as a general revelation. The concept corresponds to the early Greek philosophical concept of logos, which implies that humans, as part of a coherent universe, are capable of understanding the design of the natural world through reason. The phrase liber naturae was famously used by Galileo when writing about how \"the book of nature [can become] readable and comprehensible\".\n\nHistory\nFrom the earliest times in known civilizations, events in the natural world were expressed through a collection of stories concerning everyday life. In ancient times, it was believed that the visible, mortal world existed alongside an upper world of spirits and gods acting through nature to create a unified and intersecting moral and natural cosmos. Humans, living in a world that was acted upon by free-acting and conspiring gods of nature, attempted to understand their world and the actions of the divine by observing and correctly interpreting natural phenomena, such as the motion and position of stars and planets. Efforts to analyze and understand divine intentions led mortals to believe that intervention and influence over godly acts were possible—either through religious persuasions, such as prayer and gifts, or through magic, which depended on sorcery and the manipulation of nature to bend the will of the gods. Humans believed they could discover divine intentions through observing or manipulating the natural world. Thus, mankind had a reason to learn more about nature.\nAround the sixth century BCE, humanity’s relationship with the deities and nature began to change. Greek philosophers, such as Thales of Miletus, no longer viewed natural phenomena as the result of omnipotent gods. Instead, natural forces resided within nature, an integral part of a created world, and appeared under certain conditions that had little to do with personal deities. The Greeks believed that natural phenomena occurred by \"necessity\" through intersecting chains of \"cause\" and \"effect\". Greek philosophers, however, lacked a new vocabulary to express such abstract concepts as \"necessity\" or \"cause\" and consequently",
    "source": "wikipedia",
    "title": "Book of Nature",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_7802702",
    "text": "The Bridgewater Treatises (1833–36) are a series of eight works that were written by leading scientific figures appointed by the President of the Royal Society in fulfilment of a bequest of £8000, made by Francis Henry Egerton, 8th Earl of Bridgewater, for work on \"the Power, Wisdom, and Goodness of God, as manifested in the Creation.\"\nDespite being voluminous and costly, the series was very widely read and discussed, becoming one of the most important contributions to Victorian literature on the relationship between religion and science.  They made such an impact that Charles Darwin began On the Origin of Species with a quotation from the Bridgewater Treatise of William Whewell.\n\nThe Bridgewater Bequest\nBefore unexpectedly becoming the 8th Earl of Bridgewater in 1823, Francis Henry Egerton spent most of his life as an absentee person. He published works of classical scholarship and issued others praising the historical achievements of his family, including those of his father's cousin, Francis Egerton, 3rd Duke of Bridgewater, the \"father of British inland navigation.\"  In 1781, he was elected a Fellow of the Royal Society; after 1802 he lived mostly in Paris, where he amassed a collection of manuscripts later donated to the British Museum and gained a reputation as an eccentric. He died in February 1829, leaving a will dated 25 February 1825, in which he directed that £8000 was to be used by the President of the Royal Society to appoint a \"person or persons\":...to write, print, and publish, one thousand copies of a work On the Power, Wisdom, and Goodness of God, as manifested in the Creation; illustrating such work by all reasonable arguments, as, for instance, the variety and formation of God's creatures in the animal, vegetable, and mineral kingdoms; the effect of digestion, and thereby of conversion; the construction of the hand of man, and an infinite variety of other arguments: as also by discoveries, ancient and modern, in arts, sciences, and the whole extent of literature.The President of the Royal Society at the time was Davies Gilbert, who sought the assistance of the Archbishop of Canterbury, William Howley, and the Bishop of London, Charles James Blomfield, in selecting authors. Those appointed, with the titles and dates of their treatises, were:\nThe Adaptation of External Nature to the Moral and Intellectual Condition of Man (1833), by Thomas Chalmers, D.D.\nOn The Adaptation of External Nature to the Physical Condition of Man (1833), by John Kidd, M.D.\nAstronomy and General Physics Considered with Reference to Natural Theology (1833), by William Whewell, D.D.\nThe Hand, its Mechanism and Vital Endowments as Evincing Design (1833), by Sir Charles Bell.\nAnimal and Vegetable Physiology Considered with Reference to Natural Theology (1834), by Peter Mark Roget.\nGeology and Mineralogy Considered with Reference to Natural Theology (1836), by William Buckland, D.D.\nOn the History, Habits and Instincts of Animals (1835), by William Kirby.\nChe",
    "source": "wikipedia",
    "title": "Bridgewater Treatises",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_75954090",
    "text": "Charles Babbage's Saturday night soirées were gatherings held by the mathematician and inventor Charles Babbage at his home in Dorset Street, Marylebone, London from 1828 and into the 1840s. The soirées were attended by the cultural elite of the time.\n\nScientific soirées\nBabbage left England when his wife and father died in 1827. On his return in 1828, now in possession of a considerable inheritance, he began to host Saturday evening parties. The science historian James A. Secord describes the parties as \"scientific soirées\". Secord writes that Babbage imported the idea from France, and once established, such soirées \"became one of the chief ways in which scientific discussion could take place on a more sustained basis within polite society.\"\nIn her autobiography, the English writer and sociologist Harriet Martineau wrote: \"All were eager to go to his glorious soirées and I always thought he appeared to great advantage as a host. His patience in explaining his machine in those days was really exemplary.\"\nAccording to biographers Bruce Collier and James H. MacLachlan, \"Babbage was a bon vivant with a love of dining out and socialising. He sparkled as a host and raconteur. His Saturday soirées were glittering events attended by the social and intellectual elite of London.\"\nGuests\nHundreds of prominent people attended the soirées, including Ada Lovelace, Lady Byron, Arthur Wellesley, 1st Duke of Wellington, Charles Darwin and Emma Darwin, Charles Dickens, Michael Faraday, Sophia Elizabeth De Morgan, Mary Somerville, Harriet Martineau, photographic inventor Henry Fox Talbot, the actor William Macready, the composer Felix Mendelssohn, the historian Thomas Babington Macaulay, telegraph inventor Charles Wheatstone, the French philosopher Alexis de Tocqueville, geologist Charles Lyell and his wife Mary Lyell, Mary's sister Frances, the Belgian ambassador Sylvain Van de Weyer, electrical inventor Andrew Crosse and many others. According to C. R. Keeler, up to 200-300 people might attend one evening event.\n",
    "source": "wikipedia",
    "title": "Charles Babbage's Saturday night soirées",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_31706469",
    "text": "This list of chemical elements named after people includes elements named for people both directly and indirectly. Of the 118 elements, 19 are connected with the names of 20 people. 15 elements were named to honor 16 scientists (as curium honours both Marie and Pierre Curie). Four others have indirect connection to the names of non-scientists. Only gadolinium and samarium occur in nature; the rest are man-made.\n\nList\nThese 19 elements are connected to the names of people. Seaborg and Oganessian were the only living persons honored by having elements named after them; Oganessian is the only one still alive. Names were proposed to honor Einstein and Fermi while they were still alive, but they had both died by the time those names became official.\nThe four elements associated with non-scientists were not named in their honor but named for something else bearing their name: samarium for the mineral samarskite from which it was isolated; and americium, berkelium and livermorium after places named for them. The cities of Berkeley, California and Livermore, California are the locations of the University of California Radiation Laboratory and Lawrence Livermore National Laboratory, respectively.\nOther connections\nOther element names connected with people (real or mythological) have been proposed but failed to gain official international recognition. The following such names received past significant use among scientists:\n\ncassiopeium after the constellation Cassiopeia, hence indirectly connected to the mythological Cassiopeia (now lutetium);\ncolumbium after Christopher Columbus (now niobium);\nhahnium after Otto Hahn (now dubnium, also later proposed for what is now hassium);\njoliotium after Irène Joliot-Curie (now nobelium, also later proposed for what is now dubnium);\nkurchatovium after Igor Kurchatov (now rutherfordium);\nNames had also been suggested (but not used) to honour Henri Becquerel (becquerelium) and Paul Langevin (langevinium). George Gamow, Lev Landau, and Vitalii Goldanski (who was alive at the time) were suggested for consideration for honoring with elements during the Transfermium Wars, but were not actually proposed.\n(See the article on element naming controversies and List of chemical elements named after places.)\nAlso, mythological entities have had a significant impact on the naming of elements. Helium, titanium, selenium, palladium, promethium, cerium, europium, tantalum, mercury, thorium, uranium, neptunium and plutonium are all given names connected to mythological characters. With some, that connection is indirect:\n\nhelium: named for the Sun where it was discovered by spectral analysis, being associated with the deity Helios,\niridium: named for the Greek goddess Iris,\ntellurium: named for the Roman goddess of the earth, Tellus Mater,\nniobium: named for Niobe, a character of Greek mythology,\nvanadium: named for Vanadis, another name for Norse goddess Freyja,\nselenium: named for the Moon being associated with the deity Selene,\npalla",
    "source": "wikipedia",
    "title": "List of chemical elements named after people",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1416046",
    "text": "The history of chemistry represents a time span from ancient history to the present. By 1000 BC, civilizations used technologies that would eventually form the basis of the various branches of chemistry. Examples include the discovery of fire, extracting metals from ores, making pottery and glazes, fermenting beer and wine, extracting chemicals from plants for medicine and perfume, rendering fat into soap, making glass,\nand making alloys like bronze.\nThe protoscience of chemistry, and alchemy, was unsuccessful in explaining the nature of matter and its transformations. However, by performing experiments and recording the results, alchemists set the stage for modern chemistry. The history of chemistry is intertwined with the history of thermodynamics, especially through the work of Willard Gibbs.\n\nAncient history\n\nMedieval alchemy\nThe elemental system used in medieval alchemy was developed primarily by Jābir ibn Hayyān and was rooted in the classical elements of Greek tradition. His system consisted of the four Aristotelian elements of air, earth, fire, and water in addition to two philosophical elements: sulphur, characterizing the principle of combustibility, \"the stone which burns\"; and mercury, characterizing the principle of metallic properties. They were seen by early alchemists as idealized expressions of irreducible components of the universe and are of larger consideration within philosophical alchemy.\nThe three metallic principles (sulphur to flammability or combustion, mercury to volatility and stability, and salt to solidity) became the tria prima of the Swiss alchemist Paracelsus. He reasoned that Aristotle's four-element theory appeared in bodies as three principles. Paracelsus saw these principles as fundamental and justified them by recourse to the description of how wood burns in fire. Mercury included the cohesive principle, so that when it left the wood (in smoke) the wood fell apart. Smoke described the volatility (the mercurial principle), the heat-giving flames described flammability (sulphur), and the remnant ash described solidity (salt).\n",
    "source": "wikipedia",
    "title": "History of chemistry",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_42657576",
    "text": "Most scientific and technical innovations prior to the Scientific Revolution were achieved by societies organized by religious traditions. Ancient Christian scholars pioneered individual elements of the scientific method. Historically, Christianity has been and still is a patron of sciences. It has been prolific in the foundation of schools, universities and hospitals, and many Christian clergy have been active in the sciences and have made significant contributions to the development of science.\nHistorians of science such as Pierre Duhem credit medieval Catholic mathematicians and philosophers such as John Buridan, Nicole Oresme and Roger Bacon as the founders of modern science. Duhem concluded that \"the mechanics and physics of which modern times are justifiably proud to proceed, by an uninterrupted series of scarcely perceptible improvements, from doctrines professed in the heart of the medieval schools\". Many of the most distinguished classical scholars in the Byzantine Empire held high office in the Eastern Orthodox Church. Protestantism has had an important influence on science, according to the Merton Thesis, there was a positive correlation between the rise of English Puritanism and German Pietism on the one hand, and early experimental science on the other.\nChristian scholars and scientists have made noted contributions to science and technology fields, as well as medicine, both historically and in modern times. Some scholars state that Christianity contributed to the rise of the Scientific Revolution. Between 1901 and 2001, about 56.5% of Nobel prize laureates in scientific fields were Christians, and 26% were of Jewish descent (including Jewish atheists).\nEvents in Christian Europe, such as the Galileo affair, that were associated with the Scientific Revolution and the Age of Enlightenment led some scholars such as John William Draper to postulate a conflict thesis, holding that religion and science have been in conflict throughout history. While the conflict thesis remains popular in atheistic and antireligious circles, it has lost favor among most contemporary historians of science. Most contemporary historians of science believe the Galileo affair is an exception in the overall relationship between science and Christianity and have also corrected numerous false interpretations of this event.\n\nOverview\nMost sources of knowledge available to the early Christians were connected to pagan worldviews as the early Christians largely lived among pagans. There were various opinions on how Christianity should regard pagan learning, which included its ideas about nature. For instance, among early Christian teachers, from Tertullian (c. 160–220) held a generally negative opinion of Greek philosophy, while Origen (c. 185–254) regarded it much more favourably and required his students to read nearly every work available to them.\nEarlier attempts at reconciliation of Christianity with Newtonian mechanics appear quite different from later attempts ",
    "source": "wikipedia",
    "title": "Christianity and science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1694427",
    "text": "Science in classical antiquity encompasses inquiries into the workings of the world or universe aimed at both practical goals (e.g., establishing a reliable calendar or determining how to cure a variety of illnesses) as well as more abstract investigations belonging to natural philosophy. Classical antiquity is traditionally defined as the period between the 8th century BC (beginning of Archaic Greece) and the 6th century AD (after which there was medieval science). It is typically limited geographically to the Greco-Roman West, Mediterranean basin, and Ancient Near East, thus excluding traditions of science in the ancient world in regions such as China and the Indian subcontinent.\nIdeas regarding nature that were theorized during classical antiquity were not limited to science but included myths as well as religion. Those who are now considered as the first scientists may have thought of themselves as natural philosophers, as practitioners of a skilled profession (e.g., physicians), or as followers of a religious tradition (e.g., temple healers). Some of the more widely known figures active in this period include Hippocrates, Aristotle, Euclid, Archimedes, Hipparchus, Galen, and Ptolemy. Their contributions and commentaries spread throughout the Eastern, Islamic, and Latin worlds and contributed to the birth of modern science. Their works covered many different categories including mathematics, cosmology, medicine, and physics.\n\nClassical Greece\n\nHellenistic age\nThe military campaigns of Alexander the Great spread Greek thought to Egypt, Asia Minor, Persia, up to the Indus River. The resulting migration of many Greek speaking populations across these territories provided the impetus for the foundation of several seats of learning, such as those in Alexandria, Antioch, and Pergamum. \nHellenistic science differed from Greek science in at least two respects: first, it benefited from the cross-fertilization of Greek ideas with those that had developed in other non-Hellenic civilizations; secondly, to some extent, it was supported by royal patrons in the kingdoms founded by Alexander's successors. The city of Alexandria, in particular, became a major center of scientific research in the 3rd century BC. Two institutions established there during the reigns of Ptolemy I Soter (367–282 BC) and Ptolemy II Philadelphus (309–246 BC) were the Library and the Museum. Unlike Plato's Academy and Aristotle's Lyceum, these institutions were officially supported by the Ptolemies, although the extent of patronage could be precarious depending on the policies of the current ruler.\nHellenistic scholars often employed the principles developed in earlier Greek thought in their scientific investigations, such as the application of mathematics to phenomena or the deliberate collection of empirical data. The assessment of Hellenistic science, however, varies widely. At one extreme is the view of English classical scholar Cornford, who believed that \"all the most important",
    "source": "wikipedia",
    "title": "Science in classical antiquity",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_64433437",
    "text": "A conversazione is a \"social gathering [predominantly] held by [a] learned or art society\" for conversation and discussion, especially about the arts, literature, medicine, and science.\n\nIt would not be easy to devise a happier way [than the conversazione] of bringing novelties at once under practical criticism—of making the outliers of science acquainted with the centre, of enabling investigators to compare operations and discuss facts and speculations, and of giving occasion for renewal of intercourse and removal of misunderstandings. …[The] tangible gain to science [from the conversazione is that] inventors and experimentalists … hear [directly] what contemporaries say of their schemes and experiments, and much can be said and done with advantage amid the free talk of a general gathering which could not be permitted in the formal meeting of a scientific society. (Nature, 5 May 1870.)\n\nOrigin\nThe writer Horace Walpole is credited with the first recorded English use of conversazione in a letter written (from Italy) on 11 November 1739 to Richard West (1716–1742) in which he writes, \"After the play we were introduced to the assembly, which they [viz., the Italians] call the conversazione\".\nHistorical usage in Britain\nIn Italy, the term generally refers to a gathering for conversation; and was first used in English to identify the sort of private social gathering more generally known today as an \"At Home\".\nIn England, however, it soon came to be far more widely used to denote the gatherings of a far more intellectual character and was applied in the more specific sense of a scientific, artistic, or literary assembly/soirée, generally held at night.\n\nA conversazione like everything else has undergone conspicuous development in these days.Formerly the word was applicable only to a meeting of cognoscenti, who were themselves proficient in some art or science which might be the immediate subject of learned interest.At the present time the materials for discussion are supplied by the proficients, and the general public are invited to provide the talk or the criticism.Moreover a \"conversation\" of this kind is not limited to a specific subject, but may comprise topics incidental to any branch of science and art whatever. (New Zealand Herald, 17 September 1880.)\nIn its report on the first conversazione ever conducted by the Lambeth Literary Institution (on 22 June 1836), The Gentleman's Magazine noted that,\n\nthe principal object [of the Lambeth Literary Institution's inaugural conversazione] has been—by the collection of articles of virtù, antiquity, science, or art, and by the reading of original papers, conversation, and music,— to unite its members, at stated periods, into one focus of neighbourly community; where all may be on a footing of social equality,—the aristocracy of mind, united with urbanity of manners, alone maintaining its ascendancy here; where the high attainments of the classical scholar,—the lofty imaginings of the poet,—the deep resea",
    "source": "wikipedia",
    "title": "Conversazione",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_75707236",
    "text": "Darwin's Ghosts: The Secret History of Evolution is a nonfiction history of science book by British author Rebecca Stott. It was published in the United States in 2012 by Spiegel & Grau, the international version is subtitled differently: Darwin's Ghosts: In Search of the First Evolutionists. It is written in 12 distinct chapters that highlight persons that contributed to the pre-history of evolution by natural selection, published by Charles Darwin in On the Origin of Species in 1859. The book contains biographical sketches of 12 persons spanning from 344 BC to 19th century contemporaries of Darwin.\nThe book has received reviews from notable reviewers and was included on the New York Times Book Review 100 Notable Books 2012 list.\n\nSynopsis\nTho focus of each chapter is summarized in the table below.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Darwin's Ghosts: The Secret History of Evolution",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_28712146",
    "text": "The Department of History and Philosophy of Science (HPS), of the University of Cambridge is the largest department of history and philosophy of science in the United Kingdom. A majority of its submissions received maximum ratings of 4* and 3* in the 2014 REF (Research Excellence Framework). Located in the historic buildings of the Old Physical Chemistry Laboratories on Free School Lane, Cambridge, the department teaches undergraduate courses towards the Cambridge Tripos and graduate courses including a taught Masters and PhD supervision in the field of HPS.  The department shares its premises with the Whipple Museum and Whipple Library which provide important resources for its teaching and research.\n\nAcademic staff\nThe Department of HPS at Cambridge employs fifteen full-time teaching staff, approximately thirty research staff, numerous supervisors and research associates from departments and colleges across the University of Cambridge, in addition to external supervisors and examiners. A long-standing head of department was the noted Professor Peter Lipton, who served until his unexpected death in 2007. He was followed as head of department by the late Professor John Forrester, an international authority in the History of Mind, and a leading figure on Sigmund Freud and the history of psychoanalysis. Professor Jim Secord became head of the department in 2013 and was succeeded in 2016 by Professor Liba Taub. The current head is Professor Hasok Chang. Other senior   staff include Professor Tim Lewens, Professor Lauren Kassell, Professor Nick Hopwood and retired Professor Simon Schaffer.\nDegree courses\nThe department offers a nine-month MPhil course in history, philosophy and sociology of science, medicine and technology. It also supervises graduate students for the Cambridge PhD in HPS and provides advisors in the related fields of research in history, philosophy and social science.  Together with the Departments of Sociology and Social Anthropology, it also sponsors a nine-month MPhil in health, medicine and society.\nUndergraduate teaching and supervision is provided for students who have completed their first year at Cambridge.  Due to the interdisciplinary nature of the Cambridge Tripos system, undergraduates from a wide range of fields may study HPS, although entry is predominantly through the Natural Sciences Tripos. The resources of the Whipple Museum provide for first-hand study of scientific instruments which often provide topics for student dissertations.\n",
    "source": "wikipedia",
    "title": "Department of History and Philosophy of Science, University of Cambridge",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_29647899",
    "text": "The Dibner Institute for the History of Science and Technology (1992–2006) was a research institute established at MIT, and housed in a renovated building (E56) on campus at 38 Memorial Drive, overlooking the Charles River.\n\nDescription\nAt the heart of the Institute was the Burndy Library on the ground floor, initially containing 37,000 volumes on the history of science and technology  collected by the Dibner Fund.  The Library also possessed a large collection of antique scientific instruments, such as astrolabes, telescopes, microscopes, early spectrometers, and a Wimshurst machine, which were on public display in a dedicated gallery outside the library.  Also on display was a large collection of antique incandescent light bulbs, gas discharge tubes, electronic vacuum tubes, and other early examples of electrical and electronic technology.  The Library would mount occasional special exhibits, such as The Afterlife of Immortality: Obelisks Outside Egypt.\nThe building was a modest Art Deco structure, fronting on Memorial Drive and the Charles River. Above the Library and display space, on the second and third floor were offices and lecture and seminar rooms.  The Institute held regular lectures, seminars, study programs, and an annual symposium in the history of science and technology.  Over the period of its existence, the Institute supported over 340 short- and longer-term fellowships.\nHistory and development\nThe Institute was named in honor of Bern Dibner (1897–1988), who had conceived of it before his death. The Institute was developed and supported by the Dibner Fund he had established in 1957, directed by his son David Dibner. The institute, from its inception, was run by executive director Evelyn Simha. On the academic side, the Institute was supported by a consortium of MIT, Boston University, Brandeis University and Harvard University.\nIn 1995, the 600-volume Babson Collection of historical material related to Isaac Newton was placed on permanent deposit with the Burndy Library.  The collection had been assembled by Roger Babson, founder of Babson College in Wellesley, Massachusetts, and was previously housed at the College. In 1999, the addition of the 7,000-volume Volterra Collection from Italy increased the Burndy Library collection by more than a third.\nIn 2004 MIT decided not to renew its affiliation, and the Dibner family began looking for a new location to house the collection.  David Dibner died unexpectedly in 2005.   The Dibner Institute closed in 2006, and the Burndy Library and associated collections were transferred to The Huntington Library in San Marino, California, which now offers a Dibner History of Science Program to fund fellowships, a lecture series and annual conference. The acquisition of the Burndy Library (by then numbering 67,000 volumes) transformed the Huntington Library's collections in the history of science and technology into one of the world's largest in that field.\nThe Huntington houses a permanent exhib",
    "source": "wikipedia",
    "title": "Dibner Institute for the History of Science and Technology",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_38596702",
    "text": "Below are discoveries in science that involve chance circumstances in a particularly salient way. This page should not list all chance involved in all discoveries (i.e. it should focus on discoveries reported for their notable circumstances).\n\nOverview\nRoyston Roberts says that various discoveries required a degree of genius, but also some lucky element for that genius to act on. Richard Gaughan writes that accidental discoveries result from the convergence of preparation, opportunity, and desire.\nMajor everyday discoveries that were helped by luck in some way include products like vulcanized rubber, teflon, nylon, penicillin, cyanoacrylate (Super Glue), the implantable pacemaker, the microwave oven, Scotchgard, Saran wrap, Silly Putty, Slinky, safety glass, propeller, snowmaking, stainless steel, Perkin's mauve, and popsicles. Most artificial sweeteners have been discovered when accidentally tasted, including aspartame and saccharin.\nIdeas include the theory of the Big Bang, tissue culture, radio astronomy, and the discovery of DNA.\nSuch archeological discoveries as the Rosetta Stone, the Dead Sea Scrolls and the ruins of Pompeii also emerged partly out of serendipity.\nMany relevant and well known scientific theories were developed by chance at some degree along history. According to a legend, Archimedes realized his principle on hydrostatics when he entered in a bath full of water, which overflows (he then shouted out his famous \"Eureka!\"). And the unexpected, negative results of the Michelson–Morley experiment in their search of the luminiferous aether ultimately led to the special theory of relativity by Albert Einstein.\nThe optical illusion called the \"flashed face distortion effect\" suggests a new area of research in the neurology of face perception.\nDetailed examples\n\n",
    "source": "wikipedia",
    "title": "List of discoveries influenced by chance circumstances",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_33809974",
    "text": "The discovery of human antiquity was a major achievement of science in the middle of the 19th century, and the foundation of scientific paleoanthropology. The antiquity of man, human antiquity, or in simpler language the age of the human race, are names given to the series of scientific debates it involved, which with modifications continue in the 21st century. These debates have clarified and given scientific evidence, from a number of disciplines, towards solving the basic question of dating the first human being.\nControversy was very active in this area in parts of the 19th century, with some dormant periods also. A key date was the 1859 re-evaluation of archaeological evidence that had been published 12 years earlier by Boucher de Perthes. It was then widely accepted, as validating the suggestion that man was much older than had previously been believed, for example than the 6,000 years implied by some traditional chronologies.\nIn 1863 T. H. Huxley argued that man was an evolved species; and in 1864 Alfred Russel Wallace combined natural selection with the issue of antiquity. The arguments from science for what was then called the \"great antiquity of man\" became convincing to most scientists, over the following decade. The separate debate on the antiquity of man had in effect merged into the larger one on evolution, being simply a chronological aspect. It has not ended as a discussion, however, since the current science of human antiquity is still in flux.\n\nContemporary formulations\nModern science has no single answer to the question of how old humanity is. What the question now means indeed depends on choosing genus or species in the required answer. It is thought that the genus of man has been around for ten times as long as our species. Currently, fresh examples of (extinct) species of the genus Homo are still being discovered, so that definitive answers are not available. The consensus view is that human beings are one species, the only existing species of the genus. With the rejection of polygenism for human origins, it is asserted that this species had a definite and single origin in the past. (That assertion leaves aside the point whether the origin meant is of the current species, however. The multiregional hypothesis allows the origin to be otherwise.) The hypothesis of recent African origin of modern humans is now widely accepted, and states that anatomically modern humans had a single origin, in Africa.\nThe genus Homo is now estimated to be about 2.3 to 2.4 million years old, with the appearance of H. habilis; meaning that the existence of all types of humans has been within the Quaternary.\n\nOnce the question is reformulated as dating the transition of the evolution of H. sapiens from a precursor species, the issue can be refined into two further questions. These are: the analysis and dating of the evolution of Archaic Homo sapiens, and of the evolution from \"archaic\" forms of the species H. sapiens sapiens. The second question is ",
    "source": "wikipedia",
    "title": "Discovery of human antiquity",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_480936",
    "text": "The doctrine of signatures, also known as the doctrine of correspondences, is a biomedicinal theory of pseudoscience. It states that herbs or animals have physical or behavioral traits that mirror the ailment it can successfully treat. Theological justifications, such as that of botanist William Cole, were that God would want to show men what plants would be useful for. The doctrine of signatures has a debated origin. Many historians believe it begins with primitive thinking methods, while other historians believe it originated with Dioscorides and was popularized in the 16th and 17th centuries after Jakob Böhme coined the doctrine of signatures in his book The Signature of All Things. \nThis theory is a possible explanation for the ancient discovery of medicinal properties; however, there is no definitive proof as to whether the medicinal property or the connection in physical/behavioral traits was realized first. The theory later became a scientific basis for trying new remedies solely based upon their qualities in an attempt to find new medicines. While there are some homeopathic remedies that are still used today which have been connected to this theory, there are also remedies from this theory which have been found harmful. For instance, birthwort (so-called because of its resemblance to the uterus) was once used widely for pregnancies, but is carcinogenic and very damaging to the kidneys, owing to its aristolochic acid content. As a defense against predation, many plants contain toxic chemicals, the action of which is not immediately apparent or easily tied to the plant rather than other factors.\n\nHistory\nThe origins of the doctrine of signatures are debated by historians. The concept of the doctrine of signatures dates back to Hippocratic medicine and the belief that \"cures for human ills were divinely revealed in nature, often through plants.\" The concept would be further developed by Dioscorides. Dioscorides would provide ample descriptions of plant medications through various drawings, detailing the importance of their look, name, shelf life, how to tell when plants have gone bad, and how to properly harvest the crop for medical use. Paracelsus (1493–1541) developed the concept further, writing that \"nature marks each growth ... according to its curative benefit\", and it was further developed by Giambattista della Porta in his Phytognomonica (1588).\nThe writings of Jakob Böhme (1575–1624) coined the term \"doctrine of signatures\" within his book The Signature of All Things (or Signatura Rerum), published in 1621. He suggested that God marked objects with a sign, or \"signature\", for their purpose, specifically that \"to that Signature, his inward form is noted in the form of his face; and thus also is a beast, an herb, and the trees; every thing as it is inwardly [in its innate virtue and quality] so it is outwardly signed\". Plants bearing parts that resembled human body parts, animals, or other objects were thought to have useful relevance",
    "source": "wikipedia",
    "title": "Doctrine of signatures",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_8586",
    "text": "A Dyson sphere is a hypothetical megastructure that encompasses a star and captures a large percentage of its power output. The concept is a thought experiment that attempts to imagine how a spacefaring civilization would meet its energy requirements once those requirements exceed what can be generated from the home planet's resources alone. Because only a tiny fraction of a star's energy emissions reaches the surface of any orbiting planet, building structures encircling a star would enable a civilization to harvest far more energy.\nThe earliest modern imagining of such a structure was by Olaf Stapledon in his science fiction novel Star Maker (1937). The same concept was later used by physicist Freeman Dyson in his 1960 satirical paper \"Search for Artificial Stellar Sources of Infrared Radiation\". Dyson speculated that such structures would be the logical consequence of the escalating energy needs of a technological civilization and would be a necessity for its long-term survival. A signature of such spheres detected in astronomical searches would be an indicator of extraterrestrial intelligence.\nSince Dyson's paper, many variant designs involving an artificial structure or series of structures to encompass a star have been proposed in exploratory engineering or described in science fiction, often under the name \"Dyson sphere\". Fictional depictions often describe a solid shell of matter enclosing a star – an arrangement considered by Dyson himself to be impossible.\n\nOrigins\nInspired by the 1937 science fiction novel Star Maker by Olaf Stapledon, the physicist and mathematician Freeman Dyson was the first to formalize the concept of what became known as the \"Dyson sphere\" in his 1960 Science paper \"Search for Artificial Stellar Sources of Infra-Red Radiation\". Dyson theorized that as the energy requirements of an advanced technological civilization increased, there would come a time when it would need to systematically harvest the energy from its local star on a large scale. He speculated that this could be done via a system of structures orbiting the star, designed to intercept and collect its energy. He argued that as the structure would result in the large-scale conversion of starlight into far-infrared radiation, an earth-based search for sources of infrared radiation could identify stars supporting intelligent life.\nDyson did not detail how such a system could be constructed, simply referring to it in the paper as a \"shell\" or \"biosphere\". He later clarified that he did not have in mind a solid structure, saying: \"A solid shell or ring surrounding a star is mechanically impossible. The form of 'biosphere' which I envisaged consists of a loose collection or swarm of objects traveling on independent orbits around the star.\" Such a concept has often been referred to as a Dyson swarm; however, in 2013, Dyson said he had come to regret that the concept had been named after him. In an interview with Robert Wright in 2003, Dyson referred to his pap",
    "source": "wikipedia",
    "title": "Dyson sphere",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_15732918",
    "text": "Julian Huxley used the phrase \"the eclipse of Darwinism\" to describe the state of affairs prior to what he called the \"modern synthesis\". During the \"eclipse\", evolution was widely accepted in scientific circles but relatively few biologists believed that natural selection was its primary mechanism. Historians of science such as Peter J. Bowler have used the same phrase as a label for the period within the history of evolutionary thought from the 1880s to around 1920, when alternatives to natural selection were developed and explored—as many biologists considered natural selection to have been a wrong guess on Charles Darwin's part, or at least to be of relatively minor importance. \nFour major alternatives to natural selection were in play in the 19th century:\n\nTheistic evolution, the belief that God directly guided evolution\nNeo-Lamarckism, the idea that evolution was driven by the inheritance of characteristics acquired during the life of the organism\nOrthogenesis, the belief that organisms were affected by internal forces or laws of development that drove evolution in particular directions\nMutationism, the idea that evolution was largely the product of mutations that created new forms or species in a single step.\nTheistic evolution had largely disappeared from the scientific literature by the end of the 19th century as direct appeals to supernatural causes came to be seen as unscientific. The other alternatives had significant followings well into the 20th century; mainstream biology largely abandoned them only when developments in genetics made them seem increasingly untenable, and when the development of population genetics and the modern synthesis demonstrated the explanatory power of natural selection. Ernst Mayr wrote that as late as 1930 most textbooks still emphasized such non-Darwinian mechanisms.\n\nContext\nEvolution was widely accepted in scientific circles within a few years after the publication of On the Origin of Species, but there was much less acceptance of natural selection as its driving mechanism. Six objections were raised to the theory in the 19th century:\n\nThe fossil record was discontinuous, suggesting gaps in evolution.\nThe physicist Lord Kelvin calculated in 1862 that the Earth would have cooled in 100 million years or less from its formation, too little time for evolution.\nIt was argued that many structures were nonadaptive (functionless), so they could not have evolved under natural selection.\nSome structures seemed to have evolved on a regular pattern, like the eyes of unrelated animals such as the squid and mammals.\nNatural selection was argued not to be creative, while variation was admitted to be mostly not of value.\nThe engineer Fleeming Jenkin correctly noted in 1868, reviewing The Origin of Species, that the blending inheritance favoured by Charles Darwin would oppose the action of natural selection.\nBoth Darwin and his close supporter Thomas Henry Huxley freely admitted, too, that selection might not be the who",
    "source": "wikipedia",
    "title": "The eclipse of Darwinism",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_60880859",
    "text": "The Eddington experiment was an observational test of general relativity, organised by the British astronomers Frank Watson Dyson and Arthur Stanley Eddington in 1919. Observations of the total solar eclipse of 29 May 1919 were carried out by two expeditions, one to the West African island of Príncipe, and the other to the Brazilian town of Sobral. The aim of the expeditions was to measure the gravitational deflection of starlight passing near the Sun. The amount of deflection was predicted by Albert Einstein in a 1911 paper; however, his initial prediction proved inaccurate because it was based on an incomplete theory of general relativity. Einstein improved his prediction after finalizing his theory in 1915 and obtaining the solution to his equations by Karl Schwarzschild. Following the return of the expeditions, the results were presented by Eddington to the Royal Society of London and, after some deliberation, were accepted. Widespread newspaper coverage of the results led to worldwide fame for Einstein and his theories.\n\nBackground\nOne of the first considerations of gravitational deflection of light was published in 1801, when Johann Georg von Soldner pointed out that Newtonian gravity predicts that starlight will be deflected when it passes near a massive object. Initially, in a paper published in 1911, Einstein had incorrectly calculated that the amount of light deflection was the same as the Newtonian value, that is 0.83 seconds of arc for a star that would be just on the limb of the Sun in the absence of gravity.\nIn October 1911, responding to Einstein's encouragement, German astronomer Erwin Freundlich contacted solar eclipse expert Charles D. Perrine in Berlin to inquire as to the suitability of existing solar eclipse photographs to prove Einstein's prediction of light deflection. Perrine, the director of the Argentine National Observatory at Cordoba, had participated in four solar eclipse expeditions while at the Lick Observatory in 1900, 1901, 1905, and 1908. He did not believe existing eclipse photos would be useful. In 1912 Freundlich asked if Perrine would include observation of light deflection as part of the Argentine Observatory's program for the solar eclipse of 10 October 1912 in Brazil. W. W. Campbell, director of the Lick Observatory, loaned Perrine its intramercurial camera lenses. Perrine and the Cordoba team were the only eclipse expedition to construct specialized equipment dedicated to observe light deflection. Unfortunately all the expeditions suffered from torrential rains which prevented any observations. Nevertheless, Perrine was the first astronomer to make a dedicated attempt to observe light deflection to test Einstein's prediction. Eddington had taken part in a British expedition to Brazil to observe the 1912 eclipse but was interested in different measurements. Eddington and Perrine spent several days together in Brazil and may have discussed their observation programs including Einstein's prediction of light ",
    "source": "wikipedia",
    "title": "Eddington experiment",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_10174",
    "text": "In philosophy, empiricism is an epistemological view which holds that true knowledge or justification comes either only or primarily from sensory experience and empirical evidence. It is one of several competing views within epistemology, along with rationalism and skepticism. Empiricists argue that empiricism is a more reliable method of finding the truth than relying purely on logical reasoning, because humans have cognitive biases and limitations which lead to errors of judgement. Empiricism emphasizes the central role of empirical evidence in the formation of ideas, rather than innate ideas or traditions. Empiricists may argue that traditions (or customs) arise due to relations of previous sensory experiences. \nHistorically, empiricism was associated with the \"blank slate\" concept (tabula rasa), according to which the human mind is \"blank\" at birth and develops its thoughts only through later experience. \nEmpiricism in the philosophy of science emphasizes evidence, especially as discovered in experiments. It is a fundamental part of the scientific method that all hypotheses and theories must be tested against observations of the natural world rather than resting solely on a priori reasoning, intuition, or revelation.\nEmpiricism, often used by natural scientists, holds that \"knowledge is based on experience\" and that \"knowledge is tentative and probabilistic, subject to continued revision and falsification\". Empirical research, including experiments and validated measurement tools, guides the scientific method.\n\nEtymology\nThe English term empirical derives from the Ancient Greek word ἐμπειρία, empeiria, which is cognate with and translates to the Latin experientia, from which the words experience and experiment are derived.\nBackground\nA central concept in science and the scientific method is that conclusions must be empirically based on the evidence of the senses. Both natural and social sciences use working hypotheses that are testable by observation and experiment. The term semi-empirical is sometimes used to describe theoretical methods that make use of basic axioms, established scientific laws, and previous experimental results to engage in reasoned model building and theoretical inquiry.\nPhilosophical empiricists hold no knowledge to be properly inferred or deduced unless it is derived from one's sense-based experience. In epistemology (theory of knowledge) empiricism is typically contrasted with rationalism, which holds that knowledge may be derived from reason independently of the senses, and in the philosophy of mind it is often contrasted with innatism, which holds that some knowledge and ideas are already present in the mind at birth. However, many Enlightenment rationalists and empiricists still made concessions to each other. For example, the empiricist John Locke admitted that some knowledge (e.g. knowledge of God's existence) could be arrived at through intuition and reasoning alone. Similarly, Robert Boyle, a prominent advocate ",
    "source": "wikipedia",
    "title": "Empiricism",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_11470059",
    "text": "In the history of physics, the history of energy examines the gradual development of energy as a central scientific concept. Classical mechanics was initially understood through the study of motion and force by thinkers like Galileo Galilei and Isaac Newton, the importance of the concept of energy was made clear in the 19th century with the principles of thermodynamics, particularly the conservation of energy which established that energy cannot be created or destroyed, only transformed. In the 20th century Albert Einstein's mass–energy equivalence expanded this understanding by linking mass and energy, and quantum mechanics introduced quantized energy levels. Today, energy is recognized as a fundamental conserved quantity across all domains of physics, underlying both classical and quantum phenomena.\n\nAntiquity\nThe word energy derives from Greek word \"energeia\" (Greek: ἐνέργεια) meaning actuality, which appears for the first time in the 4th century BCE in various works of Aristotle when discussing potentiality and actuality including Physics, Metaphysics, Nicomachean Ethics and On the Soul.\nKinetic energy\nThe modern concept of kinetic energy emerged from the idea of vis viva (living force), which Gottfried Wilhelm Leibniz defined over the period 1676–1689 as the product of the mass of an object and its velocity squared; he believed that total vis viva was conserved. To account for slowing due to friction, Leibniz claimed that heat consisted of the random motion of the constituent parts of matter — a view described by Francis Bacon in Novum Organon to illustrate inductive reasoning and shared by Isaac Newton, although it would be more than a century until this was generally accepted.\nÉmilie du Châtelet in her book Institutions de Physique (\"Lessons in Physics\"), published in 1740, incorporated the idea of Leibniz with practical observations of Willem 's Gravesande to show that the \"quantity of motion\" of a moving object is proportional to its mass and its velocity squared (not the velocity itself as Newton taught—what was later called momentum).\nDaniel Bernoulli extended the vis viva principle into the Bernoulli's principle for fluids in his book in his work Hydrodynamica of 1738.\nIn 1802 lectures to the Royal Society, Thomas Young was the first to use the term energy to refer to kinetic energy in its modern sense, instead of vis viva. In the 1807 publication of those lectures, he wrote,\n\nThe product of the mass of a body into the square of its velocity may properly be termed its energy.\nGustave-Gaspard Coriolis described \"kinetic energy\" in 1829 in its modern sense,\n",
    "source": "wikipedia",
    "title": "History of energy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_59691369",
    "text": "Epistemic cultures (often used in plural form) is a concept developed in the nineties by anthropologist Karin Knorr Cetina in her book Epistemic Cultures: How the Sciences Make Knowledge. Opposed to a monist vision of scientific activity (according to which, would exist  a unique scientific method), Knorr Cetina defines the concept of epistemic cultures as a diversity of scientific activities according to different scientific fields, not only in methods and tools, but also in types of reasonings, ways to establish evidence, and relationships between theory and empiry. Knorr Cetina's work is seminal in questioning the so-called unity of science.\n\nKnorr Cetina's anthropology\nIn practice, Knorr Cetina compares two contemporary important scientific fields: High energy physics and molecular biology. She worked as an anthropologist within two laboratories, along the line of the laboratory anthropology work by Latour and Woolgar. Her anthropological work is comparative and the two chosen scientific fields are highly mediaticized and easily distinguishable.\nEpistemic cultures as a philosophical concept has been perused by numerous philosophical, anthropological or historical studies of science.\nTwo distinct publication regimes\nHigh energy physics and molecular biology are very different as scientific fields belonging to two different epistemic cultures. They also are very different in terms of academic authorship. Biagioli describes this difference in terms of publications culture regarding number of authors per paper, distribution of contributorship within authors, preprint policy and he precisely chooses to oppose the very same domains.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Epistemic cultures",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_31642145",
    "text": "The era of European and American voyages of scientific exploration followed the Age of Discovery and were inspired by a new confidence in science and reason that arose in the Age of Enlightenment. Maritime expeditions in the Age of Discovery were a means of expanding colonial empires, establishing new trade routes and extending diplomatic and trade relations to new territories, but with the Enlightenment scientific curiosity became a new motive for exploration to add to the commercial and political ambitions of the past. See also List of Arctic expeditions and List of Antarctic expeditions.\n\nMaritime exploration in the Age of Discovery\nFrom the early 15th century to the early 17th century the Age of Discovery had, through Portuguese seafarers, and later, Spanish, Dutch, French and English, opened up southern Africa, the Americas (New World), Asia and Oceania to European eyes: Bartholomew Dias had sailed around the Cape of southern Africa in search of a trade route to India; Christopher Columbus, on four journeys across the Atlantic, had prepared the way for European colonisation of the New World; Ferdinand Magellan had commanded the first expedition to sail across the Atlantic and Pacific oceans to reach the Maluku Islands and was continued by Juan Sebastián Elcano, completing the first circumnavigation of the Earth.\nThe Francisco Hernández expedition (1570–1577) (Spanish: Comisión de Francisco Hernández a Nueva España) is considered to be the first scientific expedition to the New World, led by Francisco Hernández de Toledo, a naturalist and physician of the Court of King Philip II, who was highly regarded in Spain because of his works on herbal medicine.\nAmong some of the most important achievements of the expedition were the discovery and subsequent introduction in Europe of a number of new plants that did not exist in the Old World, but that quickly gained acceptance and become very popular among European consumers, such as pineapples, cocoa, corn, and many others.\nDuring the 17th century the naval hegemony started to shift from the Portuguese and Spanish to the Dutch and then the British and French. The new era of scientific exploration began in the late 17th century as scientists, and in particular natural historians, established scientific societies that published their researches in specialist journals. The British Royal Society was founded in 1660 and encouraged the scientific rigour of empiricism with its principles of careful observation and deduction. Activities of early members of the Royal Society served as models for later maritime exploration. Hans Sloane (1650–1753) was elected a member in 1685 and travelled to Jamaica from 1687 to 1689 as physician to the Duke of Albemarle (1653–1688) who had been appointed Governor of Jamaica. In Jamaica Sloane collected numerous specimens which were carefully described and illustrated in a published account of his stay. Sloane bequeathed his vast collection of natural history 'curiosities' and",
    "source": "wikipedia",
    "title": "European and American voyages of scientific exploration",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_36920393",
    "text": "The history of experimental research is long and varied. Indeed, the definition of an experiment itself has changed in responses to changing norms and practices within particular fields of study. This article documents the history and development of experimental research from its origins in Galileo's study of gravity into the diversely applied method in use today.\n\nIbn al-Haytham\nThe Arab physicist Ibn al-Haytham (Alhazen) used experimentation to obtain the results in his Book of Optics (1021). He combined observations, experiments and rational arguments to support his intromission theory of vision, in which rays of light are emitted from objects rather than from the eyes. He used similar arguments to show that the ancient emission theory of vision supported by Ptolemy and Euclid (in which the eyes emit the rays of light used for seeing), and the ancient intromission theory supported by Aristotle (where objects emit physical particles to the eyes), were both wrong.\nExperimental evidence supported most of the propositions in his Book of Optics and grounded his theories of vision, light and colour, as well as his research in catoptrics and dioptrics.  His legacy was elaborated through the 'reforming' of his Optics by Kamal al-Din al-Farisi (d. c. 1320) in the latter's Kitab Tanqih al-Manazir (The Revision of [Ibn al-Haytham's] Optics).\nAlhazen viewed his scientific studies as a search for truth: \"Truth is sought for its own sake. And those who are engaged upon the quest for anything for its own sake are not interested in other things. Finding the truth is difficult, and the road to it is rough. ...\nAlhazen's work included the conjecture that \"Light travels through transparent bodies in straight lines only\", which he was able to corroborate only after years of effort. He stated, \"[This] is clearly observed in the lights which enter into dark rooms through holes. ... the entering light will be clearly observable in the dust which fills the air.\" He also demonstrated the conjecture by placing a straight stick or a taut thread next to the light beam.\nIbn al-Haytham employed scientific skepticism, emphasizing the role of empiricism and explaining the role of induction in syllogism. He went so far as to criticize Aristotle for his lack of contribution to the method of induction, which Ibn al-Haytham regarded as being not only superior to syllogism but the basic requirement for true scientific research.\nSomething like Occam's razor is also present in the Book of Optics. For example, after demonstrating that light is generated by luminous objects and emitted or reflected into the eyes, he states that therefore \"the extramission of [visual] rays is superfluous and useless.\" He may also have been the first scientist to adopt a form of positivism in his approach. He wrote that \"we do not go beyond experience, and we cannot be content to use pure concepts in investigating natural phenomena\", and that the understanding of these cannot be acquired without mathem",
    "source": "wikipedia",
    "title": "History of experiments",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_318374",
    "text": "The following is a list of historically important scientific experiments and observations demonstrating something of great scientific interest, typically in an elegant or clever manner.\n\nAstronomy\nOle Rømer makes the first quantitative estimate of the speed of light in 1676 by timing the motions of Jupiter's satellite Io with a telescope\nArno Penzias and Robert Wilson detect the cosmic microwave background radiation, giving support to the theory of the Big Bang (1964)\nKerim Kerimov launches Kosmos 186 and Kosmos 188 as experiments on automatic docking eventually leading to the development of space stations (1967)\nThe Supernova Cosmology Project and the High-Z Supernova Search Team discover, by observing Type Ia supernovae, that the expansion of the Universe is accelerating (1998)\nGalileo Galilei uses a telescope to observe that the moons of Jupiter appear to circle Jupiter. This evidence supports the heliocentric model, and weakens the geocentric model of the cosmos (1609)\nBiology\nRobert Hooke, using a microscope, observes cells (1665).\nAnton van Leeuwenhoek discovers microorganisms (1674–1676).\nJames Lind, publishes 'A Treatise of the Scurvy' which describes a controlled shipboard experiment using two identical populations but with only one variable, the consumption of citrus fruit (1753).\nEdward Jenner tests his hypothesis for the protective action of mild cowpox infection for smallpox, the first vaccine (1796).\nGregor Mendel's experiments with the garden pea led him to surmise many of the fundamental laws of genetics (dominant vs recessive genes, the 1–2–1 ratio, see Mendelian inheritance) (1856–1863).\nCharles Darwin demonstrates evolution by natural selection using many examples (1859).\nLouis Pasteur uses S-shaped flasks to prevent spores from contaminating broth. This disproves the theory of Spontaneous generation (1861) extending the rancid meat experiment of Francesco Redi (1668) to the micro scale.\nCharles Darwin and his son Francis, using dark-grown oat seedlings, discover the stimulus for phototropism is detected at the tip of the shoot (the coleoptile tip), but the bending takes place in the region below the tip (1880).\nEmil von Behring and Kitasato Shibasaburō demonstrate passive immunity, protection of animals from infection by injection of immune serum (1890).\nThomas Hunt Morgan identifies a sex chromosome linked gene in Drosophila melanogaster (1910) and his student Alfred Sturtevant develops the first genetic map (1913).\nAlexander Fleming demonstrates that the zone of inhibition around a growth of penicillin mould on a culture dish of bacteria is caused by a diffusible substance secreted by the mould (1928).\nFrederick Griffith demonstrates (Griffith's experiment) that living cells can be transformed via a transforming principle, later discovered to be DNA (1928).\nKarl von Frisch decodes the waggle dance honey bees use to communicate the location of flowers (1940).\nGeorge Wells Beadle and Edward Lawrie Tatum moot the \"one gene-one ",
    "source": "wikipedia",
    "title": "List of experiments",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_59616",
    "text": "In biochemistry, fermentation theory refers to the historical study of models of natural fermentation processes, especially alcoholic and lactic acid fermentation. Notable contributors to the theory include Justus Von Liebig and Louis Pasteur, the latter of whom developed a purely microbial basis for the fermentation process based on his experiments. Pasteur's work on fermentation later led to his development of the germ theory of disease, which put the concept of spontaneous generation to rest. Although the fermentation process had been used extensively throughout history prior to the origin of Pasteur's prevailing theories, the underlying biological and chemical processes were not fully understood. In the contemporary, fermentation is used in the production of various alcoholic beverages, foodstuffs, and medications.\n\nOverview of fermentation\nFermentation is the anaerobic metabolic process that converts sugar into acids, gases, or alcohols in oxygen starved environments. Yeast and many other microbes commonly use fermentation to carry out anaerobic respiration necessary for survival. Even the human body carries out fermentation processes from time to time, such as during long-distance running; lactic acid will build up in muscles over the course of long-term exertion. Within the human body, lactic acid is the by-product of ATP-producing fermentation, which produces energy so the body can continue to exercise in situations where oxygen intake cannot be processed fast enough. Although fermentation yields less ATP than aerobic respiration, it can occur at a much higher rate. Fermentation has been used by humans consciously since around 5000 BCE, evidenced by jars recovered in the Iran Zagros Mountains area containing remnants of microbes similar those present in the wine-making process.\nHistory\nPrior to Pasteur's research on fermentation, there existed some preliminary competing notions of it. One scientist who had a substantial degree of influence on the theory of fermentation was Justus von Liebig. Liebig believed that fermentation was largely a process of decomposition as a consequence of the exposure of yeast to air and water. This theory was corroborated by Liebig's observation that other decomposing matter, such as rotten plant and animal parts, interacted with sugar in a similar manner as yeast. That is, the decomposition of albuminous matter (i.e. water-soluble proteins) caused sugar to transform to alcohol. Liebig held this view until his death in 1873. A different theory was supported by Charles Cagniard de la Tour and cell theorist Theodor Schwann, who claimed that alcoholic fermentation depended on the biological processes carried out by brewer's yeast.\nLouis Pasteur's interest in fermentation began when he noticed some remarkable properties of amyl alcohol—a by-product of lactic acid and alcohol fermentation—during his biochemical studies. In particular, Pasteur noted its ability to “rotate the plane of polarized light”, and its “unsy",
    "source": "wikipedia",
    "title": "Fermentation theory",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_18134112",
    "text": "The golden age of cosmology is a term often used to describe the period from 1992 to the present in which important advances in observational cosmology have been made. Prior to the golden age of cosmology, the understanding of the universe was limited to what scientists could observe through telescopes and other instruments. Theories and models were developed based on limited data and observations, and there was much speculation and debate regarding the true nature of the universe.\nThe golden age of cosmology has also seen the development of new observational techniques and technologies. For example, the use of telescopes in space has revolutionized our ability to observe the universe. Space-based observatories such as the Hubble Space Telescope (launched in 1990) and the James Webb Space Telescope (launched in 2021) have provided stunning images and data that have expanded our understanding of the universe. \nIn addition, ground-based telescopes have also undergone significant improvements in recent years. For example, the Atacama Large Millimeter Array (ALMA) in Chile is a revolutionary new telescope that is able to observe the universe in unprecedented detail. It has already made significant contributions to our understanding of star formation and the early universe.\n\nLambda-CDM model\nIn 1992, however, the situation changed dramatically with the launch of the Cosmic Background Explorer (COBE) satellite. This mission was designed to study the cosmic microwave background (CMB) radiation, which is the leftover radiation from the Big Bang. The COBE mission made the first precise measurements of the CMB, and these measurements provided evidence in support of the Big Bang theory. The COBE mission also discovered small fluctuations in the CMB radiation, which were believed to be the seeds of galaxy formation. This discovery was a major breakthrough in our understanding of the early universe, as it provided evidence for the inflationary universe model. This model suggests that the universe underwent a rapid expansion in the first few moments after the Big Bang, which would have caused the tiny fluctuations in the CMB.\nIn the years following the COBE mission, there were several other important discoveries in observational cosmology. One of the most significant was the discovery of dark matter. This mysterious substance makes up approximately 27% of the universe, yet it cannot be observed directly. Its existence was inferred from its gravitational effects on visible matter.\nThe discovery of dark matter was followed by the discovery of dark energy, which makes up approximately 68% of the universe. Dark energy is believed to be responsible for the accelerated expansion of the universe, which was first observed in 1998 by two independent teams of astronomers.\nThe discovery of dark matter and dark energy, along with the observations of the CMB and the large-scale structure of the universe, have led to the development of the Lambda-CDM model of the universe. ",
    "source": "wikipedia",
    "title": "Golden age of cosmology",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_38078804",
    "text": "Oktōēchos (here transcribed \"Octoechos\"; Greek: ὁ Ὀκτώηχος pronounced in koine: Ancient Greek pronunciation: [okˈtóixos]; from ὀκτώ \"eight\" and ἦχος \"sound, mode\" called echos; Slavonic: Осмогласие, Osmoglasie from о́смь \"eight\" and гласъ \"voice, sound\") is the name of the eight mode system used for the composition of religious chant in most Christian churches during the Middle Ages. In a modified form, the octoechos is still regarded as the foundation of the tradition of monodic Orthodox chant today (Neobyzantine Octoechos).\nThe octoechos as a liturgical concept which established an organization of the calendar into eight-week cycles, was the invention of monastic hymnographers at Mar Saba in Palestine, at the Patriarchates of Antiochia and of Constantinople. It was officially announced as the modal system of hymnography at the Quinisext Council in 692.\nA similar eight-mode system was established in Western Europe during the Carolingian reform, and particularly at the Second Council of Nicaea in 787 AD which decanonised the former iconoclastic council in 754 and confirmed earlier ones. Quite possibly this was an attempt to follow the example of the Eastern Church by an octoechos reform, even if it was rather a transfer of knowledge with an introduction of a new book called \"tonary\" which introduced into a Western octoechos of its own design.\nIt had a list of incipits of chants ordered according to the intonation formula of each tone in its psalmody. Later on, fully notated and theoretical tonaries were also written. The Byzantine book octoechos (9th century) was one of the first hymn books with musical notation and its earliest surviving copies date from the 10th century.\n\nOrigins\nStudents of Orthodox chant today often study the history of Byzantine chant in three periods, identified by the names John of Damascus (675/676-749) as the \"beginning\", John Koukouzeles (c. 1280–1360) as the \"flower\" (Papadic Octoechos), and Chrysanthos of Madytos (c. 1770-c. 1840) as the master of the living tradition today (Neobyzantine Octoechos). The latter has the reputation that he once connected in his time the current tradition with the past of Byzantine chant, which was in fact the work of at least four generations of teachers at the New Music School of the Patriarchate.\nThis division of the history into three periods begins quite late with the 8th century, despite the fact that the octoechos reform was already accepted some decades earlier, before John and Cosmas entered the monastery Mar Saba in Palestine. The earliest sources which gave evidence of the octoechos' use in Byzantine chant, can be dated back to the 6th century.\n",
    "source": "wikipedia",
    "title": "Hagiopolitan Octoechos",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1653438",
    "text": "Jennifer Michael Hecht (born November 23, 1965) is a teacher, author, poet, historian, and philosopher. She was an associate professor of history at Nassau Community College (1994–2007) and most recently taught at The New School in New York City.\nHecht has seven published books, her scholarly articles have been published in many journals and magazines, and her poetry has appeared in The New Yorker, The New Republic, Ms. Magazine, and Poetry Magazine, among others. She has also written essays and book reviews for The New York Times, The Washington Post, The Philadelphia Inquirer, The American Scholar, The Boston Globe and other publications. She has written several columns for The New York Times online \"Times Select.\" In 2010 Hecht was one of the five nonfiction judges for the National Book Award.\nHecht is a longtime blogger for The Best American Poetry series web site and maintains a personal blog on her website. She resides in Brooklyn, New York.\n\nBackground\nBorn in Glen Cove, New York on Long Island, Hecht attended Adelphi University, where she earned a BA in history, for a time studying at the Université de Caen, and the Université d'Angers. She earned her PhD in the history of science from Columbia University in 1995 and taught at Nassau Community College from 1994 to 2007, finally as a tenured associate professor of history. Hecht has taught in the MFA programs at The New School and Columbia University, and is a fellow of the New York Institute for the Humanities.\nHecht is married and has two children.\nShe has appeared on television on the Discovery Channel, The Morning Show with Marcus Smith, Road to Reason and MSNBC's Hardball, and on radio on The Brian Lehrer Show, The Leonard Lopate Show, On Being (formerly known as Speaking of Faith), All Things Considered, The Joy Cardin Show, and others.\nIntellectual interests and writings\nOf her three major intellectual interests, she ranks them, \"Poetry came first, then historical scholarship, then public atheism, and they probably remain in that order in my dedication to them.\"\nOriginally intending to be a poet, she was drawn to the history of science. Her first book, The End of the Soul: Scientific Modernity, Atheism, and Anthropology in France, 1876-1936, grew out of her dissertation on some late 19th-century anthropologists who formed the Society of Mutual Autopsy. The members would dissect each other's brains after death, and Hecht, having noticed their atheism, came to understand that this was being done not only for the sake of scientific finds, but perhaps to prove to the Catholic Church that the soul does not exist.\nWhile researching her first book, she came to realize that there was no sufficient history of atheism, and that led to her second book, Doubt: A History.\nWhile writing Doubt, she found that many atheists went beyond simply stating that there are no gods and also made profound suggestions about how people should think of life and how we should live. That led to her third book, Th",
    "source": "wikipedia",
    "title": "Jennifer Michael Hecht",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_42748548",
    "text": "Hellenophilia is the idea that all western science began in Greek tradition. This is discussed in length by David Pingree in his address to colleagues. Hellenophilia is a way of thought that allows those who look into the history of science to be blinded to science born in other cultures. Pingree states, in explanation of the term that \"a Hellenophile suffers from a form of madness that blinds him or her to historical truth\" (Pingree, 1992, p. 554) He continues by explaining the main symptoms of Hellenophilia \"the first of these is that the Greeks invented science; the second is that they discovered a way to truth, the scientific method, that we are now successfully following; the third is that the only real sciences are those that began in Greece; and the fourth (and last?) is that the true definition of science is just that which scientists happen to be doing now, following a method or methods adumbrated by the Greeks, but never fully understood or utilized by them\" (Pingree, 1992, p. 555).\nAn anthropological etiology of Greek innovation in natural science is advanced by sociologist Michael G. Horowitz in \"The Scientific Dialectic of Ancient Greece and the Cultural Tradition of Indo-European Speakers\" (Journal of Indo-European Studies, 24(3-4):409-19 [1996]).\nAlthough Hellenophilia relates directly to the history of science, it is important to look at it through aspects of history that lend to the habit, other than the symptoms listed by Pingree. One of these habits, as described by David C. Lindberg is looking at the history of science as starting with writing in fully syllabic systems. According to Lindberg the beginning of syllabic writing was around 1500 B.C. However, fully alphabetic writing was apparent in Greece in 800 B.C. (Linberg, 2007, p. 10).\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Hellenophilia",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_81581326",
    "text": "Information System \"History of Geology and Mining\" (Russian: Информационная система «История геологии и горного дела») is a scientific information system (knowledge base, bibliographic database and non-commercial website) containing biographical, bibliographical data and documents about scientists, scientific organizations, and publications related to geological and mining sciences. It is a joint project of the Geological Institute and the Library for Natural Sciences of the Russian Academy of Sciences. It represents the first attempt to systematize and provide access to a large array of data in the field of the history of geology and mining. It was created to support and facilitate scientific research in the history of science.\n\nHistory\nThe information system is a continuation of the printed publications in the series “Materials on the History of Geology in the USSR”.\nVersions of the \"History of Geology and Mining\" information system:\n\nTest version on the Library for Natural Sciences of the Russian Academy of Sciences website (2011—2015 — scirus.benran.ru/higeo.\nAt the Geological Institute RAS: 2015–2025 at higeo.ginras.ru. New test version at https://higeo.ru\nDescription and Structure\nThe system was created based on the customizable network software complex \"SciRus\" developed at the Library for Natural Sciences of the Russian Academy of Sciences (part of the Information and Library Council of the RAS). Software development by: N. Kalenov, A. Senko, and M. Yakshin.\nThe information system on the history of geology includes core data about scientists:\n\nData for scientific biographies of scientists\nOrganizations (academies, educational institutions, institutes, and scientific societies), geography and research directions\nPrinted sources (journals, newspapers, and other serial publications)\nBrief scientific biographies of scientists (under development)\nMajor scientific works and references to them (URLs and DOIs are provided where available)\nLiterature about scientists and references to it (URLs are provided where available)\nDocuments related to scientists (questionnaires, manuscripts, event programs, correspondence, and other documents)\nLinks to portraits of scientists, group photographs, engravings, and other images.\nThe data array is centered around the scientist's profile. A large portion of the documents are still being processed or are currently in closed access: about 10,000 folders on scientists, >15,000 photographs and other images.\nInternal integrated search across many parameters is the primary functionality of the Information System.\n",
    "source": "wikipedia",
    "title": "History of Geology and Mining (Information System)",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_33561186",
    "text": "The history of scholarship is the historical study of fields of study which are not covered by the English term \"science\" (cf., history of science), but are covered by, for example, the German term \"Wissenschaft\" (i.e., all kinds of academic studies). Examples include the history of classical studies, philosophy, religion, biblical studies, historiography, music, art, and literature. \nIt is a field which has recently undergone a complete renewal and is now a major branch of research. In 2015, the Society for the History of the Humanities was established, coinciding with the launch of the journal History of Humanities in 2016. Both developments reflect the field’s growing institutional presence and international scholarly collaboration.\n\nClassical scholarship\nRudolph Pfeiffer (1968) describes the history of classical scholarship from its revival inspired by Petrarch to the achievements of the Italian humanists and the independent movement in Holland (including Erasmus) and the German scholar-reformers. Pfeiffer traces the development of classical scholarship in the countries of Western Europe through the next two centuries, with particular attention to sixteenth-century France and eighteenth-century England. Finally he provides an account of the new approach made by Winckelmann and his successors in Germany.\nPhilosophers, scholars, polymaths, and scientists\nThe word scientist was coined by the English philosopher and historian of science William Whewell in 1833. Until then there was no differentiation between the history of science, the history of philosophy, and the history of scholarship.\nBefore 1700 the fields of scholarship were not of a size that made academic specialisation necessary. Academic disciplines as we know them today did not exist. Scholars were generally active in both the sciences and what are now called the Arts and Humanities.\nSee also\nArt history\nCultural history\nHistoric recurrence\nHistory of archaeology\nHistory of books\nHistory of education\nHistory of European universities\nHistory of knowledge\nHistory of mathematics\nHistory of writing\nHuman science\nIntellectual history\nMedieval university\nScholarly method\n",
    "source": "wikipedia",
    "title": "History of scholarship",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_48395726",
    "text": "The presence of women in science spans the earliest times of the history of science wherein they have made substantial contributions. Historians with an interest in gender and science have researched the scientific endeavors and accomplishments of women, the barriers they have faced, and the strategies implemented to have their work peer-reviewed and accepted in major scientific journals and other publications. The historical, critical, and sociological study of these issues has become an academic discipline in its own right.\nThe involvement of women in medicine occurred in several early Western civilizations, and the study of natural philosophy in ancient Greece was open to women. Women contributed to the proto-science of alchemy in the first or second centuries CE During the Middle Ages, religious convents were an important place of education for women, and some of these communities provided opportunities for women to contribute to scholarly research. The 11th century saw the emergence of the first universities; women were, for the most part, excluded from university education. Outside academia, botany was the science that benefitted most from the contributions of women in early modern times. The attitude toward educating women in medical fields appears to have been more liberal in Italy than elsewhere. The first known woman to earn a university chair in a scientific field of studies was eighteenth-century Italian scientist Laura Bassi.\nGender roles were largely deterministic in the eighteenth century and women made substantial advances in science.  During the nineteenth century, women were excluded from most formal scientific education, but they began to be admitted into learned societies during this period. In the later nineteenth century, the rise of the women's college provided jobs for women scientists and opportunities for education. Marie Curie paved the way for scientists to study radioactive decay and discovered the elements radium and polonium. Working as a physicist and chemist, she conducted pioneering research on radioactive decay and was the first woman to receive a Nobel Prize in Physics and became the first person to receive a second Nobel Prize in Chemistry. Sixty women have been awarded the Nobel Prize between 1901 and 2022. Twenty-four women have been awarded the Nobel Prize in physics, chemistry, physiology or medicine.\n\nCross-cultural perspectives\nIn the 1970s and 1980s, many books and articles about women scientists were appearing; virtually all of the published sources ignored women of color and women outside of Europe and North America. The formation of the Kovalevskaia Fund in 1985 and the Organization for Women in Science for the Developing World in 1993 gave more visibility to previously marginalized women scientists, but even today there is a dearth of information about current and historical women in science in developing countries. According to academic Ann Hibner Koblitz:\n\nMost work on women scientists has focused",
    "source": "wikipedia",
    "title": "Women in science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_57975652",
    "text": "The human body has been subject of much debate. How people are defined, and what defined them – be it their anatomy or their energy or both – depends on culture and time. Culture not only defines how sex is perceived but also how gender is defined.  Today gender, sex, and identity continue to be of much debate and change based on what place and people are being examined.\nThe early modern idea of the body was a cultural ideal, an understanding and approach to how the body works and what place that body has in the world.  All cultural ideals of the body in the early modern period deal with deficiencies and disorders within a body, commonly told through a male ideal.  Ideas of the body in the early modern period form the history of how bodies should be and how to correct the body when something has gone wrong.  Therefore, early modern conceptions of the body were not biological as there was not a restrictive biological view of the human body as established by modern science.\nConceptions of the body are primarily either eastern, based in China and involving practices such as Traditional chinese medicine, or western, which follows the Greek traditions of science and is more closely related to modern science despite original anatomists and ideas of the body being just as unscientific as Chinese practices.\n\nHistoriography\nIn Western historical research, scholars began investigating the cultural history of the human body in detail in the 1980s. The movement is particularly associated with the historian of medicine Roy Porter, whose 1991 article 'History of the Body' was a seminal study. 1995 saw the foundation of the journal Body and Society, by which time the field of the history of the body was already extensive and diverse.\nPorter pointed out that Western historiography had previously assumed mind–body dualism (i.e. that the body is fundamentally separate from the mind or soul) and therefore that the cultural history of bodies as material objects had been overlooked: 'given the abundance of evidence available, we remain remarkably ignorant about how individuals and social groups have experienced, controlled, and projected their embodied selves. How have people made sense of the mysterious link between \"self\" and its extensions? How have they managed the body as an intermediary between self and society?' He emphasised that the history of the body is important to understanding histories of coercion and control, sex and gender, and other important but culturally varied aspects of human experience.\nAnother prominent voice in the field at the same time was Caroline Walker Bynum, whose 1988 Holy Feast and Holy Fast became a landmark study. Both Bynum and Porter noted that during the 1980s Western history of the body research drew on post-structuralist thought, such as Michel Foucault's ideas of biopolitics and biopower, which emphasised that state power is not abstract, but exercised through and over human bodies. But both expressed a concern that research ",
    "source": "wikipedia",
    "title": "History of beliefs about the human body",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_15361",
    "text": "An ice age is a term describing periods of time when the reduction in the temperature of Earth's surface and atmosphere results in the presence or expansion of continental and polar ice sheets and alpine glaciers. The term is applied in several different senses to very long and comparatively short periods of cooling. Colder periods are called glacials or ice ages, and warmer periods are called interglacials.\nEarth's climate alternates between icehouse and greenhouse periods based on whether there are glaciers on the planet, and for most of Earth's history it has been in a greenhouse period with little or no permanent ice. Over the very long term, Earth is currently in an icehouse period called the Late Cenozoic Ice Age, which started 34 million years ago. There have been colder and warmer periods within this ice age, and the term is also applied to the Quaternary glaciation, which started 2.58 million years ago. Within this period, the Last Interglacial ended 115,000 years ago, and was followed by the Last Glacial Period (LGP), which gave way to the current warm Holocene, which started 11,700 years ago. The most severe cold period of the LGP was the Last Glacial Maximum, which reached its maximum between 26,000 and 20,000 years ago. The most recent glaciation was the Younger Dryas between 12,800 and 11,700 years ago.\n\nHistory of research\nIn 1742, Pierre Martel (1706–1767), an engineer and geographer living in Geneva, visited the valley of Chamonix in the Alps of Savoy. Two years later he published an account of his journey. He reported that the inhabitants of that valley attributed the dispersal of erratic boulders to the glaciers, saying that they had once extended much farther. Later similar explanations were reported from other regions of the Alps. In 1815 the carpenter and chamois hunter Jean-Pierre Perraudin (1767–1858) explained erratic boulders in the Val de Bagnes in the Swiss canton of Valais as being due to glaciers previously extending further. An unknown woodcutter from Meiringen in the Bernese Oberland advocated a similar idea in a discussion with the Swiss-German geologist Jean de Charpentier (1786–1855) in 1834. Comparable explanations are also known from the Val de Ferret in the Valais and the Seeland in western Switzerland and in Goethe's scientific work. Such explanations could also be found in other parts of the world. When the Bavarian naturalist Ernst von Bibra (1806–1878) visited the Chilean Andes in 1849–1850, the natives attributed fossil moraines to the former action of glaciers.\nMeanwhile, European scholars had begun to wonder what had caused the dispersal of erratic material. From the middle of the 18th century, some discussed ice as a means of transport. The Swedish mining expert Daniel Tilas (1712–1772) was, in 1742, the first person to suggest drifting sea ice was a cause of the presence of erratic boulders in the Scandinavian and Baltic regions. In 1795, the Scottish philosopher and gentleman naturalist, James Hutto",
    "source": "wikipedia",
    "title": "Ice age",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_71465383",
    "text": "Indigenous science is the application and intersection of Indigenous knowledge and science. This field is based on careful observation of the environment, and through experimentation. It is a holistic field, informed by physical, social, mental and cultural knowledge. When applied to ecology and the environment, it can be sometimes termed traditional ecological knowledge. Indigenous science involves the knowledge systems and practices of Indigenous peoples, which are rooted in their cultural traditions and relationships to their indigenous context. There are some similar methods of Western science including (but not limited to): observation, prediction, interpretation, and questioning. There are also some areas in which Western science and Indigenous science differ. Indigenous knowledge is place and case-specific and does not attempt to label or generalize natural processes. Western science strives to find commonalities and theories that can be applied to all areas, such as Newton's Laws of Physics. This is because most Indigenous knowledge stems from the relationship humans have with their environment, which is passed down through stories or is discovered through observation. Western knowledge takes a different approach by isolating targets to study, splitting them from their surroundings and making sets of assumptions and theories. Community is a larger aspect of Indigenous science, and conclusions are shared through oral tradition and family knowledge, whereas most Western science research is published in a journal specific to that scientific field, and may restrict access to various papers.\nThere is a history of oppression against Native Americans beginning when settlers came to America, and this has carried into the field of Indigenous science as American scientists and academics have overlooked the findings and knowledge of Indigenous people. Multiple studies found that Indigenous perspectives are rarely represented in empirical studies, and has led to the underrepresentation of Native people in research fields. In addition, Western researchers have benefitted from the research they do about Indigenous nations, while the tribes do not receive compensation for their work and information.\nHigher recognition and advocacy of Indigenous people in the 21st century has increased the visibility of this field. There has been a growing recognition of the potential benefits of incorporating Indigenous perspectives and knowledge, particularly in fields such as ecology and environmental management.\n\nOral traditions in Indigenous science\nIndigenous knowledge and experiences are often passed down orally from generation to generation. Indigenous knowledge has an empirical basis and has traditionally been used to predict and understand the world. Such knowledge has informed studies of human management of natural processes.\nThis oral knowledge is embedded in songs and dances, which allows for accurate information to be passed down for centuries as songs and ",
    "source": "wikipedia",
    "title": "Indigenous science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_82060664",
    "text": "The Institute for the History of Natural Sciences (IHNS CAS; Chinese: 中国科学院自然科学史研究所; pinyin: Zhōngguó Kēxuéyuàn Zìrán Kēxuéshǐ Yánjiūsuǒ) is a leading research institution in China dedicated to the study of the history of science, technology, and medicine. It is affiliated with the Chinese Academy of Sciences (CAS).\n\nHistory\nThe Institute for the History of Natural Sciences was founded in 1957 with the active involvement of the renowned British biochemist and historian of science Joseph Needham (Chinese: 李约瑟, Lǐ Yuēsè) and a group of Chinese scholars. Its establishment was initiated to systematically research China's rich scientific and technological heritage and its place in world history. Initially, the institute was located in the Gulou district of Beijing.\nResearch Focus\nThe institute's primary mission is to conduct fundamental and applied research on the history of science and technology in China and the world, and to promote the development of this discipline.\nKey research areas include:\n\nHistory of Science and Technology in China: Studying traditional Chinese science, technology, medicine, astronomy, mathematics, and their interaction with society and culture.\nHistory of World Science: Comparative studies, history of scientific exchanges, study of the Scientific Revolution and the development of modern science.\nTheoretical Studies in History of Science: Methodology, philosophy, and sociology of science.\nPreservation of Scientific and Technological Heritage: Identifying and researching tangible and intangible objects of scientific and technological heritage.\nScience, Technology and Society (STS): Investigating the interrelations between scientific/technological progress and social development.\nThe IHNS plays a central role in developing and institutionalizing the history of science as an academic discipline in China. Its research contributes significantly to the global understanding of the history of science and highlights China's contributions to scientific and technological development.\n",
    "source": "wikipedia",
    "title": "Institute for the History of Natural Sciences",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_37092860",
    "text": "The International Conference on Cold Fusion (ICCF) (also referred to as Annual Conference on Cold Fusion in 1990-1991 and mostly as International Conference on Condensed Matter Nuclear Science since 2007) is an annual or biennial conference on the topic of cold fusion. An international conference on cold fusion was held in Santa Fe, New Mexico, USA in 1989. However, the first ICCF conference (ICCF1) took place in 1990 in Salt Lake City, Utah, USA, under the title \"First Annual Conference on Cold Fusion\". Its location has since rotated between Russia, the USA, Europe, and Asia. It was held in India for the first time in 2011. The conferences have been criticized as events which attract \"crackpots\" and \"pseudo-scientists\".\n\nReception\nThe First Annual Conference on Cold Fusion was held in March 1990 in Salt Lake City, Utah, United States. Robert L. Park of the American Physical Society derisively referred to it as a \"seance of true believers.\" The conference was attended by more than 200 researchers from the United States, Italy, Japan, India and Taiwan and dozens of reporters from all over the U.S. and abroad.\nThe Third International Conference on Cold Fusion was held in 1992 in Nagoya, Japan. It was described by The New York Times, \"depending on one's point of view\" as \"either a turning point in which evidence was presented that will convince the skeptics that cold fusion exists or a religious revival where claims of miracles were lapped up by ardent believers.\" The conference was sponsored by seven Japanese scientific societies, it was attended by 200 Japanese scientists and more than 100 from abroad. Tomohiro Taniguchi, then director of the Electric Power Technology Division at Japan's Ministry of International Trade and Industry, reportedly said that the Ministry of International Trade and Industry was willing to finance research in the field in view of \"encouraging evidence, especially after the conference.\" The conference was also covered by the Associated Press.\nA journalist for the Wired magazine attended the 1998 conference in Vancouver—apparently the only mainstream journalist who attended—and reported that he found there \"about 200 extremely conventional-looking scientists, almost all of them male and over 50\" with some apparently over 70. He then inferred that \"[the] younger ones had bailed years ago, fearing career damage from the cold fusion stigma.\" He reported seeing \"highly technical presentations\" and \"was amazed by the quantity of the work, its quality, and the credentials of the people pursuing it\", whereas \"[a] few obvious pseudoscientists, promoting their ideas in an adjoining room used for poster sessions, were politely ignored.\"\nBy 1999, attendance by researchers at the ICCF meetings drew comment from the field of science studies. Although scientific debate over cold fusion had effectively ended in 1990, attendance at the ICCF meetings for the next 8 years had been relatively stable at between 100 and 300. Sociologist Bart S",
    "source": "wikipedia",
    "title": "International Conference on Cold Fusion",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_57241168",
    "text": "The International Scientific Committee on Price History was created in 1929 by William Beveridge and Edwin Francis Gay after receiving a five-year grant from the Rockefeller Foundation. The national representatives were William Beveridge for Great Britain, Moritz John Elsas for Germany, Edwin Francis Gay for the United States, Earl J. Hamilton for Spain, Henri Hauser for France and Alfred Francis Pribram for Austria; later, Franciszek Bujak for Poland and Nicolaas Wilhelmus Posthumus for the Netherlands also joined; Arthur H. Cole was in charge of finances for the whole project.\n\nBooks by the committee\nHamilton (Earl J.), American Treasure and the Price Revolution in Spain (1501–1650), 1934.\nHamilton (Earl J.), Money, Prices and Wages in Valencia, Aragon and Navarre (1351–1500), 1936.\nHauser (Henri), Recherches et documents sur l’histoire des prix en France de 1500 à 1800, 1936.\nElsas (Moritz John), Umriß einer Geschichte der Preise und Löhne in Deutschland vom ausgehenden Mittelalter bis zum Beginn des 19. Jarhunderts, 3 vol., 1936–1949.\nPřibram (Alfred Francis), Materialien zur Geschichte der Preise und Löhne in Österreich, 1938.\nCole (Arthur Harrison), Wholesale Commodity Prices in the United States 1700–1861, 1938.\nBeveridge (William H.), Prices and Wages in England from the 12th to the 19th Century, 1939.\nPosthumus (Nicolaas), Nederlandsche Prijsgeschiedenis, 1943–1964.\nHamilton (Earl J.), War and Prices in Spain (1651–1800), 1947.\nReferences\n\nOlivier Dumoulin, \"Aux origines de l'histoire des prix\", Annales. Économies, sociétés, civilisations, 45/2, 1990, p. 507-522[1].\nJulien Demade, Produire un fait scientifique. Beveridge et le Comité international d'histoire des prix, Paris, Publications de la Sorbonne, 2018.\n",
    "source": "wikipedia",
    "title": "International scientific committee on price history",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_76635267",
    "text": "Isis was an encyclopedic journal that focused on articles on natural science, medicine, technology, economics as well as art and history. It also published important articles on science policy and the organization of science. Edited by Lorenz Oken and published by Friedrich Arnold Brockhaus, Isis was the first interdisciplinary journal in the German-speaking world.\nThe 41 volumes of the journal named after the Egyptian goddess Isis were nominally published from 1817 to 1848. However, the first issue appeared on August 1, 1816, while the printing of the last issue was delayed until February 1850. Until 1832, Isis bore the title Encyclopädische Zeitung. After the focus of the articles published in it had changed, Oken changed the title to Encyclopädische Zeitschrift, vorzüglich für Naturgeschichte, vergleichende Anatomie und Physiologie in 1833. Initially printed in Jena, the journal was banned in the Grand Duchy of Saxe-Weimar-Eisenach and from the summer of 1819 was produced in nearby Rudolstadt in the court printing works of the Principality of Schwarzburg-Rudolstadt. The magazine's original print run of 1,500 copies fell rapidly in the first few years of its existence and amounted to around 200 copies in the last few years.\nOriginally conceived as a non-political journal, Oken was forced to vehemently defend the freedom of the press in the first years of Isis' existence. This resulted in numerous lawsuits against Oken, some of which overlapped in time, which led to temporary bans on Isis in the Grand Duchy of Saxe-Weimar-Eisenach. In the run-up to the Carlsbad Decrees, this led to Oken's dismissal as a professor at the University of Jena at the end of June 1819 under pressure from the states of the Holy Alliance.\nFrom 2006 to 2013, a project funded by the German Research Foundation at the Friedrich Schiller University Jena studied the significance of Isis for scientific communication and the popularization of the natural sciences in the first half of the 19th century.\n\nOrigin\nIn a letter dated April 11, 1814, Lorenz Oken contacted the publisher Friedrich Arnold Brockhaus for the first time and offered him his publication Neue Bewaffnung, neues Frankreich, neues Theutschland for printing. Brockhaus did not print it, but Oken subsequently contributed to Brockhaus' Conversations-Lexikon and, from June 1815 at the latest, was a contributor to the Deutsche Blätter, which Brockhaus had been publishing since October 1813 following the Battle of Leipzig and which became the most important journal in central Germany in 1813/1814. Presumably at the end of June/beginning of July 1815, Oken took over the editorship of the Tagesgeschichte, a supplement to the Deutsche Blätter, which was dedicated to daily politics and for which he wrote and edited numbers 1 to 16. With the end of the Wars of Liberation, the focus of the Deutsche Blätter shifted from war reporting to general daily politics, which was associated with a considerable decline in circulation from",
    "source": "wikipedia",
    "title": "Isis (journal, 1816)",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_78066172",
    "text": "The Islamic Scientific Manuscripts Initiative (ISMI) (Arabic: مبادرة المخطوطات العلمية الإسلامية) is an online database that supports research on mathematics history in the Islamic world to 1350 CE. The initiative aims to provide accessible information on all Islamic manuscripts in the exact sciences, including astronomy, mathematics, theories, mathematical geography, music, mechanics, and related subjects.\nIt is an initiative of the Max Planck Institute for the History of Science (MPIWG), which is dedicated to advancing scientific knowledge and research.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Islamic Scientific Manuscripts Initiative",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_69664295",
    "text": "Languages of science are vehicular languages used by one or several scientific communities for international communication. According to the science historian Michael Gordin, scientific languages are \"either specific forms of a given language that are used in conducting science, or they are the set of distinct languages in which science is done.\" These two meanings are different, since the first describes a distinct prose in a given language (i.e., scientific writing), while the second describes which languages are used in mainstream science.\n\nUntil the 19th century, classical languages—such as Latin, Classical Arabic, Sanskrit, Classical Malay and Classical Chinese—were commonly used across Afro-Eurasia for international scientific communication. A combination of structural factors, the emergence of nation-states in Europe, the Industrial Revolution, and the expansion of colonization entailed the global use of three European national languages: French, German, and English. Yet new languages of science, such as Russian and Italian, had started to emerge by the end of the 19th century—to the point that international scientific organizations began promoting the use of constructed languages such as Esperanto as a non-national global standard.\nAfter the First World War, English gradually outpaced French and German; it became the leading language of science, but not the only international standard. Research in the Soviet Union (USSR) rapidly expanded in the years after the Second World War, and access to Russian journals became a major policy issue in the United States, prompting the early development of machine translation. In the last decades of the 20th century, an increasing number of scientific publications were written primarily in English, in part due to the preeminence of English-speaking scientific infrastructure, indexes, and metrics such as the Science Citation Index. Local languages remain largely relevant for science in major countries and world regions such as China, Latin America, and Indonesia. Disciplines and fields of study with a significant degree of public engagement—such as social sciences, environmental studies, and medicine—have also maintained the relevance of local languages.\n\nThe development of open science has revived the debate over linguistic diversity in science, as social and local impact has become an important objective of open science infrastructure and platforms. In 2019, 120 international research organizations cosigned the Helsinki Initiative on Multilingualism in Scholarly Communication; they also called for supporting multilingualism and the development of an \"infrastructure of scholarly communication in national languages\". In 2021, UNESCO's Recommendation for Open Science included \"linguistic diversity\" as one of the core features of open science, since this diversity aims to \"make multilingual scientific knowledge openly available, accessible and reusable for everyone.\" In 2022, the Council of the European Un",
    "source": "wikipedia",
    "title": "Languages of science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_73378834",
    "text": "Leith AGCM is a climate model that was developed by Cecil Leith beginning in 1958; it is likely the oldest atmospheric general circulation model. Leith published videos of its model output, inspiring other scientists to do the same. Today it has been superseded by climate models developed from different base codes; as such, it is little known.\n\nHistory and development\nEfforts to calculate the behaviour of the weather system commenced in the 1920s with a seminal paper by Lewis Fry Richardson. By the 1950s and 1960s several groups were involved in making climate models, with major efforts taking place at several US universities that eventually gave rise to the well-known GFDL, UCLA, and NCAR models. Today climate models are an important enterprise with significant impact on public policy, where hundreds of scientists and institutions participate worldwide.\nThe researcher Cecil E. “Chuck” Leith (1923–2016) is well-known for his research on fluid mechanics. After an initial career on the Manhattan Project, which resulted in the invention of nuclear bombs, he joined the Lawrence Radiation Laboratory after 1946 and in 1968 the National Center for Atmospheric Research. Beginning in 1958, he began to work on a climate model that was later named the \"Leith atmospheric model\" or \"Livermore atmospheric model\". Its existence was barely reported at that time, with only several contemporary journal articles mentioning it. According to interviews with Leith, he was inspired to work on climate modelling by the noted scientist Edward Teller and by the idea to put his knowledge on nuclear explosions to use in a field that wouldn't be hindered by nuclear test bans. The model was written in assembly language, which may have given it a headstart compared to other climate model projects that were undergoing in Livermore at the time and which relied on compiler language. There appear to have been four versions, based on reports of improvement work on the code, and Leith publicized numerous videos (at the time called \"movies\") of the output of his model. Today, the readable presentation of the often enormous quantities of data output by climate models is a major problem in climate modelling; and Leith's example inspired other scientists to make videos as well. Leith apparently relied on a private company, Pacific Title, which worked in the entertainment industry at Hollywood, and one video displaying the output of hid model.\n",
    "source": "wikipedia",
    "title": "Leith AGCM",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_18403",
    "text": "Logical positivism, also known as logical empiricism or neo-positivism, was a philosophical movement, in the empiricist tradition, that sought to formulate a scientific philosophy in which philosophical discourse would be, in the perception of its proponents, as authoritative and meaningful as empirical science.\nLogical positivism's central thesis was the verification principle, also known as the \"verifiability criterion of meaning\", according to which a statement is cognitively meaningful only if it can be verified through empirical observation or if it is a tautology (true by virtue of its own meaning or its own logical form). The verifiability criterion thus rejected statements of metaphysics, theology, ethics and aesthetics as cognitively meaningless in terms of truth value or factual content. Despite its ambition to overhaul philosophy by mimicking the structure and process of empirical science, logical positivism became erroneously stereotyped as an agenda to regulate the scientific process and to place strict standards on it.\nThe movement emerged in the late 1920s among philosophers, scientists and mathematicians congregated within the Vienna Circle and Berlin Circle and flourished in several European centres through the 1930s. By the end of World War II, many of its members had settled in the English-speaking world and the project shifted to less radical goals within the philosophy of science.\nBy the 1950s, problems identified within logical positivism's central tenets became seen as intractable, drawing escalating criticism among leading philosophers, notably from Willard Van Orman Quine and Karl Popper, and even from within the movement, from Carl Hempel. These problems would remain unresolved, precipitating the movement's eventual decline and abandonment by the 1960s. In 1967, philosopher John Passmore pronounced logical positivism \"dead, or as dead as a philosophical movement ever becomes\".\n\nOrigins\nLogical positivism emerged in Germany and Austria amid a cultural background characterised by the dominance of Hegelian metaphysics and the work of Hegelian successors such as F. H. Bradley, whose metaphysics portrayed the world without reference to empirical observation. The late 19th century also saw the emergence of neo-Kantianism as a philosophical movement, in the rationalist tradition.\nThe logical positivist program established its theoretical foundations in the empiricism of David Hume, Auguste Comte and Ernst Mach, along with the positivism of Comte and Mach, defining its exemplar of science in Einstein's general theory of relativity. In accordance with Mach's phenomenalism, whereby material objects exist only as sensory stimuli rather than as observable entities in the real world, logical positivists took all scientific knowledge to be only sensory experience. Further influence came from Percy Bridgman's operationalism—whereby a concept is not knowable unless it can be measured experimentally—as well as Immanuel Kant's perspective",
    "source": "wikipedia",
    "title": "Logical positivism",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_76589675",
    "text": "The materialism controversy (German: Materialismusstreit) was a public debate in the mid-19th century about how new developments in the natural sciences might affect existing worldviews. During the 1840s, a new form of materialism emerged, shaped by advances in biology and the decline of idealistic philosophy. This form of materialism sought to explain human beings and their behavior through scientific methods. The central question of the debate was whether scientific discoveries were compatible with traditional ideas such as the existence of an immaterial soul, a personal God, and human free will. The discussion also touched on deeper philosophical issues, such as what kind of knowledge a materialist or mechanical view of the world could offer.\nIn his Physiologische Briefe from 1846, zoologist Carl Vogt argued that mental processes were entirely physical, famously stating that \"thoughts stand in the same relation to the brain as bile does to the liver or urine to the kidneys.\" In 1854, the physiologist Rudolf Wagner criticized this view in a speech to the Göttingen Naturalists' Assembly. He argued that religious belief and science belonged to separate areas of understanding, and that natural science could not answer questions about God, the soul, or free will.\nWagner’s comments were strongly worded, accusing materialists of trying to undermine spiritual values. His attacks sparked sharp responses from Vogt and others. The materialist position was later defended by figures such as physiologist Jakob Moleschott and physician Ludwig Büchner, brother of writer Georg Büchner. Supporters of materialism saw themselves as opposing what they viewed as outdated philosophical, religious, and political ideas. While their approaches varied, they found growing support among the middle classes. The idea of a scientific worldview became an important feature in the broader cultural debates of the late 19th and early 20th centuries.\n\nDevelopment of natural scientific materialism\n\nCarl Vogt and the political opposition\nThe materialism controversy was sparked in part by the writings of physiologist Carl Vogt, beginning in 1847. His commitment to materialism was shaped by the scientific and political reform movements of the time, as well as his own personal and political development. Vogt was born in Giessen in 1817, into a family with both scientific and revolutionary traditions. His father, Philipp Friedrich Wilhelm Vogt, was a professor of medicine who moved to Bern in 1834 after facing political persecution. On his mother’s side, political activism was also a strong influence: Louise Follen’s three brothers—Adolf, Karl, and Paul Follen—were all involved in nationalist and democratic causes and eventually went into exile.\nIn 1817 Adolf Follen drafted a proposal for a future German constitution and was later arrested for his political activities. He avoided a 10-year prison sentence by fleeing to Switzerland. Karl Follen was suspected of encouraging the assassinat",
    "source": "wikipedia",
    "title": "Materialism controversy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_26692522",
    "text": "The history of metamaterials begins with artificial dielectrics in microwave engineering as it developed just after World War II. Yet, there are seminal explorations of artificial materials for manipulating electromagnetic waves at the end of the 19th century.\nHence, the history of metamaterials is essentially a history of developing certain types of manufactured materials, which interact at radio frequency, microwave, and later optical frequencies.\nAs the science of materials has advanced, photonic materials have been developed which use the photon of light as the fundamental carrier of information. This has led to photonic crystals, and at the beginning of the new millennium, the proof of principle for functioning metamaterials with a negative index of refraction in the microwave- (at 10.5 Gigahertz) and optical  range. This was followed by the first proof of principle for metamaterial cloaking (shielding an object from view), also in the microwave range, about six years later. However, a cloak that can conceal objects across the entire electromagnetic spectrum is still decades away. Many physics and engineering problems need to be solved.\nNevertheless, negative refractive materials have led to the development of metamaterial antennas and metamaterial microwave lenses for miniature wireless system antennas which are more efficient than their conventional counterparts. Also, metamaterial antennas are now commercially available. Meanwhile, subwavelength focusing with the superlens is also a part of present-day metamaterials research.\n\nEarly wave studies\nClassical waves transfer energy without transporting matter through the medium (material). For example, waves in a pond do not carry the water molecules from place to place; rather the wave's energy travels through the water, leaving the water molecules in place. Additionally, charged particles, such as electrons and protons create electromagnetic fields when they move, and these fields transport the type of energy known as electromagnetic radiation, or light. A changing magnetic field will induce a changing electric field and vice versa—the two are linked. These changing fields form electromagnetic waves. Electromagnetic waves differ from mechanical waves in that they do not require a medium to propagate. This means that electromagnetic waves can travel not only through air and solid materials, but also through the vacuum of space.\nThe \"history of metamaterials\" can have a variety starting points depending on the properties of interest. Related early wave studies started in 1904 and progressed through more than half of the first part of the twentieth century. This early research included the relationship of the phase velocity to group velocity and the relationship of the wave vector and Poynting vector.\nIn 1904 the possibility of negative phase velocity accompanied by an anti-parallel  group velocity were noted by Horace Lamb (book: Hydrodynamics) and Arthur Schuster (Book: Intro to Optics). Howe",
    "source": "wikipedia",
    "title": "History of metamaterials",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_31743909",
    "text": "The history of the metric system began during the Age of Enlightenment with measures of length and weight derived from nature, along with their decimal multiples and fractions. The system became the standard of France and Europe within half a century.  Other measures with unity ratios were added, and the system went on to be adopted across the world.\nThe first practical realisation of the metric system came in 1799, during the French Revolution, after the existing system of measures had become impractical for trade, and was replaced by a decimal system based on the kilogram and the metre.  The basic units were taken from the natural world. The unit of length, the metre, was based on the dimensions of the Earth, and the unit of mass, the kilogram, was based on the mass of a volume of water of one litre (a cubic decimetre). Reference copies for both units were manufactured in platinum and remained the standards of measure for the next 90 years. After a period of reversion to the mesures usuelles due to unpopularity of the metric system, the metrication of France and much of Europe was complete by the 1850s.\nIn the middle of the 19th century, James Clerk Maxwell conceived a coherent system where a small number of units of measure were defined as base units, and all other units of measure, called derived units, were defined in terms of the base units. Maxwell proposed three base units for length, mass and time. Advances in electromagnetism in the 19th century necessitated additional units to be defined, and multiple incompatible systems of such units came into use; none could be reconciled with the existing dimensional system. The impasse was resolved by Giovanni Giorgi, who in 1901 proved that a coherent system that incorporated electromagnetic units required a fourth base unit, of electromagnetism.\nThe seminal 1875 Treaty of the Metre resulted in the fashioning and distribution of metre and kilogram artefacts, the standards of the future coherent system that became the SI, and the creation of an international body Conférence générale des poids et mesures or CGPM to oversee systems of weights and measures based on them.\nIn 1960, the CGPM launched the International System of Units (in French the Système international d'unités or SI) with six \"base units\": the metre, kilogram, second, ampere, degree Kelvin (subsequently renamed the \"kelvin\") and candela, plus 16 more units derived from the base units. A seventh base unit, the mole, and six other derived units were added later in the 20th century. During this period, the metre was redefined in terms of the speed of light, and the second was redefined based on the microwave frequency of a caesium atomic clock.\nDue to the instability of the international prototype of the kilogram, a series of initiatives were undertaken, starting in the late 20th century, to redefine the ampere, kilogram, mole and kelvin in terms of invariant constants of physics, ultimately resulting in the 2019 revision of the SI, whic",
    "source": "wikipedia",
    "title": "History of the metric system",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_74980602",
    "text": "Mihi itch or Mihisucht is the ambition to describe new species (or other taxa: subspecies, hybrids, genera, etc.) as a means to immortalize one's name. Mihi is the dative form of the Latin word ego, thus \"mihi itch\" means to satisfy one's egotistical  impulses. The expression appeared in print as early as 1884.\nA consequence of the Mihi itch may be the unwarranted description of new taxa, differing only slightly from already established taxa, leading to taxonomic inflation. A more extreme case may be termed taxonomic vandalism when a large number of species are described with limited scientific evidence.\n\nExamples\nLa \"nouvelle école\" in malacology, led by Jules René Bourguignat, was responsible for the description of hundreds of new species of molluscs in Europe at the end of the nineteen century.\nHarold St. John published 440 names in the genus Pandanus, which encompasses c. 600 accepted species, and 283 names in the genus Cyrtandra, which encompasses c. 700 accepted species.\nBetween 2000 and 2011, Raymond Hoser published 582 species names, and 340 generic names of animals (mostly reptiles).\nSee also\nTaxonomic vandalism\nTaxonomic inflation\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Mihi itch",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_166380",
    "text": "Natural history is a domain of inquiry involving organisms, including animals, fungi, and plants, in their natural environment, leaning more towards observational than experimental methods of study. A person who studies natural history is called a naturalist or natural historian.\nNatural history encompasses scientific research but is not limited to it. It involves the systematic study of any category of natural objects or organisms, so while it dates from studies in the ancient Greco-Roman world and the mediaeval Arabic world, through to European Renaissance naturalists working in near isolation, today's natural history is a cross-discipline umbrella of many specialty sciences; e.g., geobiology has a strong multidisciplinary nature.\n\nDefinitions\n\nHistory\n\nMuseums\nNatural history museums, which evolved from cabinets of curiosities, played an important role in the emergence of professional biological disciplines and research programs. Particularly in the 19th century, scientists began to use their natural history collections as teaching tools for advanced students and the basis for their own morphological research.\nSocieties\nThe term \"natural history\" alone, or sometimes together with archaeology, forms the name of many national, regional, and local natural history societies that maintain records for animals—including birds (ornithology), insects (entomology) and mammals (mammalogy)—fungi (mycology), plants (botany), and other organisms. They may also have geological and microscopical sections.\nExamples of these societies in Britain include the Natural History Society of Northumbria founded in 1829, London Natural History Society (1858), Birmingham Natural History Society (1859), British Entomological and Natural History Society founded in 1872, Glasgow Natural History Society, Manchester Microscopical and Natural History Society established in 1880, Whitby Naturalists' Club founded in 1913, Scarborough Field Naturalists' Society and the Sorby Natural History Society, Sheffield, founded in 1918. The growth of natural history societies was also spurred due to the growth of British colonies in tropical regions with numerous new species to be discovered. Many civil servants took an interest in their new surroundings, sending specimens back to museums in the Britain. (See also: Indian natural history)\nSocieties in other countries include the American Society of Naturalists and Polish Copernicus Society of Naturalists. The Ecological Society of America launched its \"Natural History Section\" in 2010, using the tagline \"the heart and soul of ecology.\"\nProfessional societies have recognized the importance of natural history and have initiated new sections in their journals specifically for natural history observations to support the discipline. These include \"Natural History Field Notes\" of Biotropica, \"The Scientific Naturalist\" of Ecology, \"From the Field\" of Waterbirds, and the \"Natural History Miscellany section\" of the American Naturalist.\n",
    "source": "wikipedia",
    "title": "Natural history",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_10804088",
    "text": "Natural magic in the context of Renaissance magic is that part of the occult which deals with natural forces directly, as opposed to ceremonial magic which deals with the summoning of spirits. Natural magic sometimes makes use of physical substances from the natural world such as stones or herbs.\nNatural magic so defined includes astrology, alchemy, and certain disciplines that would today be considered fields of natural science, such as astronomy and chemistry (divergently evolved from astrology and alchemy, respectively) or botany (from herbology). Jesuit scholar Athanasius Kircher wrote that \"there are as many types of natural magic as there are subjects of applied sciences\".\nHeinrich Cornelius Agrippa discusses natural magic in his Three Books of Occult Philosophy (1533), where he calls it \"nothing else but the highest power of natural sciences\". The Italian Renaissance philosopher Giovanni Pico della Mirandola, who founded the tradition of Christian Kabbalah, argued that natural magic was \"the practical part of natural science\" and was lawful rather than heretical.\n\nSee also\nKitāb al-nawāmīs – Arabic book of magic\nGiambattista della Porta – Italian polymath (1535–1615)\nMagia Naturalis – Book by Giambattista della Porta\nProtoscience – Research field that may become a science\nThomas Vaughan – Welsh philosopher (1621–1666)\nWhite magic – Magic used for selfless purposes\nReferences\n\nFurther reading\nNauert, Charles G. (1957). \"Magic and Skepticism in Agrippa's Thought\". Journal of the History of Ideas: 176.\nStark, Ryan J. (2009). Rhetoric, Science, and Magic in Seventeenth-Century England. Washington, DC: The Catholic University of America Press.\nExternal links\n The dictionary definition of natural magic at Wiktionary.\n",
    "source": "wikipedia",
    "title": "Natural magic",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_382251",
    "text": "Natural philosophy, philosophy of nature (from Latin philosophia naturalis), or experimental philosophy is the philosophical study of nature and the physical universe, while ignoring any supernatural influence. Until the late modern period, the term natural philosophy referred to the branch of philosophy (a broader term then, meaning all rational fields of study and contemplation) that explored topics now considered scientific, such as physics, biology, and astronomy. Thus, natural philosophy served as the precursor to, and has been mostly supplanted by, modern science.\nFrom the ancient world (at least since Aristotle) until the 19th century, natural philosophy was the common term for the study of physics (nature), a broad term that included botany, zoology, anthropology, and chemistry as well as what is now called physics. It was in the 19th century that the concept of science received its modern shape, with different subjects within science emerging, such as astronomy, biology, and physics. Institutions and communities devoted to science were founded. Isaac Newton's book Philosophiæ Naturalis Principia Mathematica (1687) (English: Mathematical Principles of Natural Philosophy) reflects the use of the term natural philosophy in the 17th century. Even in the 19th century, the work that helped define much of modern physics bore the title Treatise on Natural Philosophy (1867).\nIn the German tradition, Naturphilosophie (philosophy of nature) persisted into the 18th and 19th centuries as an attempt to achieve a speculative unity of nature and spirit, after rejecting the scholastic tradition and replacing Aristotelian metaphysics, along with those of the dogmatic churchmen, with Kantian rationalism. Some of the greatest names in German philosophy are associated with this movement, including Goethe, Hegel, and Schelling. Naturphilosophie was associated with Romanticism and a view that regarded the natural world as a kind of giant organism, as opposed to the philosophical approach of figures such as John Locke and others espousing a more mechanical philosophy of the world, regarding it as being like a machine.\n\nOrigin and evolution of the term\nThe term natural philosophy preceded current usage of natural science (i.e. empirical science). Empirical science historically developed out of philosophy or, more specifically, natural philosophy. Natural philosophy was distinguished from the other precursor of modern science, natural history, in that natural philosophy involved reasoning and explanations about nature (and after Galileo, quantitative reasoning), whereas natural history was essentially qualitative and descriptive.\nGreek philosophers defined natural philosophy as the combination of beings living in the universe, ignoring things made by humans. The other definition refers to human nature.\nIn the 14th and 15th centuries, natural philosophy was one of many branches of philosophy, but was not a specialized field of study. The first person appointed as ",
    "source": "wikipedia",
    "title": "Natural philosophy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_17902329",
    "text": "The one-sex and two-sex models are historiographical concepts introduced by historian Thomas Laqueur in his 1990 book Making Sex: Body and Gender from the Greeks to Freud. Laqueur proposed that Western medical and philosophical thought underwent a fundamental shift in the 18th century: from a \"one-sex model\" in which female anatomy was understood as an inverted, inferior version of male anatomy, to a \"two-sex model\" treating men and women as anatomically distinct and opposite. While Making Sex has been highly influential across academic disciplines, Laqueur's thesis has attracted substantial criticism from historians of medicine and science who argue that it oversimplifies the historical record, misreads primary sources, and imposes an artificial chronological divide.\n\nLaqueur's thesis\n\nCriticism\nLaqueur's thesis has been subject to extensive criticism from historians of medicine and science. Critics have challenged both his reading of primary sources and the accuracy of his proposed chronology.\nInfluence and legacy\nDespite scholarly criticisms, Making Sex has remained highly influential, particularly in gender studies, literary criticism, and cultural history. The book helped establish the broader argument that sex, like gender, is historically and culturally constructed rather than a timeless biological given. This insight has proven productive for scholars even when they reject Laqueur's specific historical claims.\nThe ongoing debate over Making Sex also illustrates broader methodological questions in the history of science: how to interpret historical texts without imposing modern categories, how to balance sweeping narratives against the complexity of historical evidence, and how disciplinary popularity can sustain a thesis despite sustained criticism from specialists.\nSee also\nHistory of biology\nHistory of medicine\nHistory of sexuality\nSex and gender distinction\nSocial construction of gender\nThomas Laqueur\nReferences\n\nFurther reading\nFletcher, Anthony (1995). Gender, Sex and Subordination in England 1500–1800. New Haven: Yale University Press. ISBN 978-0-300-06531-2.\nHarvey, Karen (2002). \"The Century of Sex? Gender, Bodies, and Sexuality in the Long Eighteenth Century\". The Historical Journal. 45 (4): 899–916. doi:10.1017/S0018246X02002728.\nSchiebinger, Londa (1993). Nature's Body: Gender in the Making of Modern Science. Boston: Beacon Press. ISBN 978-0813535319.\n",
    "source": "wikipedia",
    "title": "One-sex and two-sex theories",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_31657115",
    "text": "An Oral History of British Science is an oral history project conducted by National Life Stories at the British Library.  The project began in 2009 with funding from the Arcadia Fund, the Royal Commission for the Exhibition of 1851 and a number of other private donors and focuses on audio interviews with British science and engineering figures.\n\nProject background\nThe project focused on 200 video interviews lasting 8–15 hours, with four themes: Made in Britain, A Changing Planet, Cosmologies and Biomedicine.   The project Advisory Committee included Jon Agar, Alec Broers, Tilly Blyth, Georgina Ferry, Dame Julia Higgins, Maja Kominko, Sir Harry Kroto, John Lynch, Chris Rapley and Simone Turchetti.\nAn Oral History of British Science was conducted by National Life Stories (NLS) at the British Library, and formed part of a wider institutional initiative to better document contemporary history of science and technology through the addition of audio visual sources as well as written sources.\nMethodology\nThe oral history of British science follows the biographical, or life story, oral history approach with each audio interview averaging 8 to 15 hours in length. The interviews cover the individual’s career history, education, background and family.\nAccess to interviews\nAll interviews are catalogued on the Sound and Moving Image Catalogue.  Interviews which are complete and open are accessible onsite at the Library in St Pancras, London and in Boston Spa, Yorkshire via the Library’s Listening & Viewing Service.   Interviews which are open are also made accessible via the Archival Sound Recordings website under the ‘Oral history of British science’ content package.\nPeople interviewed\nInterviewed for ‘A Changing Planet’:\n\nBarbara Bowen (Geophysics technician/ research assistant)\nJoe Farman (Geophysicist)\nJohn Glen (Glaciologist)\nA.T. (Dick) Grove (Geographer/ geomorphologist)\nDavid Jenkinson (Soil Scientist)\nDesmond King-Hele (Physicist)\nJohn Kington (Meteorologist and climatologist)\nJames Lovelock (Geochemist)\nMelvyn Mason (Technician in seismic refraction)\nDan McKenzie (geophysicist)\nStephen Moorbath (Geologist and Geochronologist)\nJohn Nye (scientist) (Physicist, Theoretical glaciologist)\nCharles Swithinbank (Glaciologist)\nJanet Thomson (Geologist)\nSue Vine (Geophysicist technician/ research assistant)\nRichard West (Botanist and Quaternary Geologist)\nInterviewed for ‘Made in Britain’:\n\nRaymond Bird (Computer Engineer)\nTony Brooker (Computer Scientist)\nMary Coombs (Computer Programmer)\nSir Alan Cottrell (Metallurgist and Physicist)\nDai Edwards (Computer Engineer);\nRoy Gibson (Aerospace Engineer)\nAndy Hopper (Computer Engineer)\nFrank Land (Computer Scientist)\nBob Parkinson (Aerospace Engineer)\nDame Stephanie Shirley (Computer Scientist)\nGeoff Tootill (Computer Engineer)\nMaurice Wilkes (Computer Engineer)\nInterviewed under ‘Biomedicine’:\n\nSammy Lee (scientist) (Clinical embryologist)\n",
    "source": "wikipedia",
    "title": "Oral History of British Science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_36506774",
    "text": "\"The Parable of the Sunfish\" is an anecdote with which Ezra Pound opens ABC of Reading, a 1934 work of literary criticism. Pound uses this anecdote to emphasize an empirical approach for learning about art, in contrast to relying on commentary rooted in abstraction. While the parable is based on students' recollections of Louis Agassiz's teaching style, Pound's retelling diverges from these sources in several respects. The parable has been used to illustrate the benefits of scientific thinking, but more recent literary criticism has split on whether the parable accurately reflects the scientific process and calls into question Pound's empirical approach to literature.\n\nThe Parable\nThe text of the parable below is excerpted from Pound's ABC of Reading.\nContext\n\nSources\nLouis Agassiz was a Swiss-born scientist at Harvard University who, by 1896, had established a reputation for \"lock[ing] a student up in a room full of turtle-shells, or lobster-shells, or oyster-shells, without a book or a word to help him, and not let[ting] him out till he had discovered all the truths which the objects contained.\" Several students of Agassiz who went on to prominence recorded this rite of passage, including Henry Blake, David Starr Jordan, Addison Emery Verrill, and Burt Green Wilder. American literary critic Robert Scholes traces the parable's source to two narratives in particular: those of former students Nathaniel Southgate Shaler and Samuel Hubbard Scudder. Their anecdotes were reprinted in Lane Cooper's Louis Agassiz as a Teacher: Illustrative Extracts on his Method of Instruction. Their separate accounts differ markedly from Pound's: both students provide oral reports with a wealth of detail after being initially forbidden from consulting outside sources.\nInterpretation and criticism\n\nNotes\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Parable of the Sunfish",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_35659147",
    "text": "Patterns in nature are visible regularities of form found in the natural world. These patterns recur in different contexts and can sometimes be modelled mathematically. Natural patterns include symmetries, trees, spirals, meanders, waves, foams, tessellations, cracks and stripes. Early Greek philosophers studied pattern, with Plato, Pythagoras and Empedocles attempting to explain order in nature. The modern understanding of visible patterns developed gradually over time.\nIn the 19th century, the Belgian physicist Joseph Plateau examined soap films, leading him to formulate the concept of a minimal surface. The German biologist and artist Ernst Haeckel painted hundreds of marine organisms to emphasise their symmetry. Scottish biologist D'Arcy Thompson pioneered the study of growth patterns in both plants and animals, showing that simple equations could explain spiral growth. In the 20th century, the English mathematician Alan Turing predicted mechanisms of morphogenesis which give rise to patterns of spots and stripes. The Hungarian biologist Aristid Lindenmayer and the French American mathematician Benoît Mandelbrot showed how the mathematics of fractals could create plant growth patterns.\nMathematics, physics and chemistry can explain patterns in nature at different levels and scales. Patterns in living things are explained by the biological processes of natural selection and sexual selection. Studies of pattern formation make use of computer models to simulate a wide range of patterns.\n\nHistory\nEarly Greek philosophers attempted to explain order in nature, anticipating modern concepts. Pythagoras (c. 570–c. 495 BC) explained patterns in nature like the harmonies of music as arising from number, which he took to be the basic constituent of existence. Empedocles (c. 494–c. 434 BC) to an extent anticipated Darwin's evolutionary explanation for the structures of organisms. Plato (c. 427–c. 347 BC) argued for the existence of natural universals. He considered these to consist of ideal forms (εἶδος eidos: \"form\") of which physical objects are never more than imperfect copies. Thus, a flower may be roughly circular, but it is never a perfect circle. Theophrastus (c. 372–c. 287 BC) noted that plants \"that have flat leaves have them in a regular series\"; Pliny the Elder (23–79 AD) noted their patterned circular arrangement. Centuries later, Leonardo da Vinci (1452–1519) noted the spiral arrangement of leaf patterns, that tree trunks gain successive rings as they age, and proposed a rule purportedly satisfied by the cross-sectional areas of tree-branches.\nIn 1202, Leonardo Fibonacci introduced the Fibonacci sequence to the western world with his book Liber Abaci. Fibonacci presented a thought experiment on the growth of an idealized rabbit population. Johannes Kepler (1571–1630) pointed out the presence of the Fibonacci sequence in nature, using it to explain the pentagonal form of some flowers. In 1658, the English physician and philosopher Sir Thomas B",
    "source": "wikipedia",
    "title": "Patterns in nature",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_48799229",
    "text": "Physica speculatio is a text of scientific character written by Alonso de la Vera Cruz in 1557 in the capital of New Spain. It was the first published work in the American continent that specifically addressed the study of physics, and was written to teach the students of the Real University of Mexico.\nIt introduced the main theoretical concepts of geocentric astronomy and references the heliocentric model.\nFray Alonso de la Vera Cruz published in the capital of New Spain a Course of Arts, constituted in three volumes in Latin. The first form in 1553 under the title of Recognitio Summularum, that had like purpose help to the students of the Real University of Mexico to understand the philosophy by means of the understanding of the formal logic. A year afterwards appeared the second called Dialectica Resolutio, that was a continuation of the previous. The last was Physica speculatio.\nThey did  four editions, the last 3 of which were for use of the salmantino students and were abbreviated versions of the Mexican one.\n\nSubjects\nThe Physica speculatio has by object the study or \"investigation\" -speculatio- and the exhibition, in general, of subjects of physics  on the nature -Physica-, treated by fray Alonso de la Vera Cruz basically from the philosophical perspective, characteristic of Aristotle and traditional in the Half Age.\nIt talks about, in what can be considered like the first part, the subjects treated by Aristotle in the Eight books of physics, as they are the essence of the physical or natural being, the movement and the infinite, the extension, the continuous, the space, the time, the first engine, etc. The second part treats of the subjects of the generation and the corruption of the living beings, of the mixed and composed being, of the primary qualities and of the elements and their properties. In the third part it exposes the doctrines on the meteors, it talks about the stars and their influence on humans, of the three regions of the air or atmosphere, of the comets, of the tides, of the ray and of a lot of other atmospheric phenomena. The fourth part devotes fray Alonso to comment the books De Anima by Aristotle. To end the Physica speculatio, there are some reflections on the treatise De Caelo by Aristotle.\n",
    "source": "wikipedia",
    "title": "Physica speculatio",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1692795",
    "text": "The Physics (Ancient Greek: Φυσικής ἀκρόασις, romanized: Physikḗs akróasis, or: Φυσικής ακροάσεως, Physikḗs akroáseōs; Latin: Physica or Naturales Auscultationes, possibly meaning \"Lectures on nature\") is a named text, written in ancient Greek, collated from a collection of surviving manuscripts known as the Corpus Aristotelicum, attributed to the 4th-century BC philosopher Aristotle.\n\nThe meaning of physics in Aristotle\nIt is a collection of treatises or lessons that deals with the most general (philosophical) principles of natural or moving things, both living and non-living, rather than physical theories (in the modern sense) or investigations of the particular contents of the universe. The chief purpose of the work is to discover the principles and causes of (and not merely to describe) change, or movement, or motion (κίνησις kinesis), especially that of natural wholes (mostly living things, but also inanimate wholes like the cosmos). In the conventional Andronicean ordering of Aristotle's works, it stands at the head of, as well as being foundational to, the long series of physical, cosmological and biological treatises, whose ancient Greek title, τὰ φυσικά, means \"the [writings] on nature\" or \"natural philosophy\".\nDescription of the content\nThe Physics is composed of eight books, which are further divided into chapters. This system is of ancient origin, now obscure. In modern languages, books are referenced with Roman numerals, standing for ancient Greek capital letters (the Greeks represented numbers with letters, e.g. A for 1). Chapters are identified by Arabic numerals, but the use of the English word \"chapter\" is strictly conventional. Ancient \"chapters\" (capita) are generally very short, often less than a page. Additionally, the Bekker numbers give the page and column (a or b) used in the Prussian Academy of Sciences' edition of Aristotle's works, instigated and managed by Bekker himself. These are evident in the 1831 2-volume edition. Bekker's line numbers may be given. These are often given, but unless the edition is the Academy's, they do match any line counts.\n",
    "source": "wikipedia",
    "title": "Physics (Aristotle)",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_75138286",
    "text": "Pictet's experiment is the demonstration of the reflection of heat and the apparent reflection of cold in a series of experiments performed in 1790 (reported in English in 1791 in An Essay on Fire) by Marc-Auguste Pictet—ten years before the discovery of infrared heating of the Earth by the Sun. The apparatus for most of the experiments used two concave mirrors facing one another at a distance. An object placed at the focus of one mirror would have heat and light reflected by the mirror and focused. An object at the focus of the counterpart mirror would do the same. Placing a hot object at one focus and a thermometer at the other would register an increase in temperature on the thermometer. This was sometimes demonstrated with the explosion of a flammable mix of gasses in a blackened balloon, as described and depicted by John Tyndall in 1863.\nAfter \"demonstrating that radiant heat, even when it was not accompanied by any light, could be reflected and focused like light\", Pictet used the same apparatus to demonstrate the apparent reflection of cold in a similar manner. This demonstration was important to Benjamin Thompson, Count Rumford who argued for the existence of \"frigorific rays\" conveying cold. Rumford's continuation of the experiments and promotion of the topic caused the name to be attached to the experiment.\nThe apparent reflection of cold if a cold object is placed in one focus surprised Pictet and two scholars writing about the experiment in 1985 noted \"most physicists, on seeing it demonstrated for the first time, find it surprising and even puzzling.\" The confusion may be resolved by understanding that all objects in the system—including the thermometer—are constantly radiating heat. Pictet described this as \"the thermometer acts the same part relatively to the snow as the bullet [heat source] in relation to the thermometer.\" Addition of a very cold object adds an effective heat sink versus a room temperature object which would not, in the net, cool or warm a thermometer in the other focus.\n\nModern replications and demonstrations\nThere are relatively few published examples of demonstrations or recreation of the experiment. Two physicists in the University of Washington system reported on demonstrations to students and colleagues and produced directions for re-creating the experiment in 1985 as part of an investigation into the role of the experiment in the history of physics. Physicists at Sofia University in Bulgaria reported on reproducing the experiment for high school students in 2017.\n",
    "source": "wikipedia",
    "title": "Pictet's experiment",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_31732740",
    "text": "Of the 118 chemical elements, 41 are named after, or have names associated with, places around the world or among astronomical objects. 32 of these have names tied to the Earth and the other 10 have names connected to bodies in the Solar System.\nThe first table below lists terrestrial locations (excluding the entire Earth taken as a whole) and the last table lists astronomical objects which the chemical elements are named after.\n\nTerrestrial locations\n\nAstronomical objects\n* - The element mercury was named directly for the deity, with only indirect naming connection to the planet (see etymology of mercury).\n** - Phosphorus was the Ancient Greek name for the planet Venus. (see history of phosphorus).\nSee also\nList of chemical elements named after people\nList of chemical element name etymologies\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "List of chemical elements named after places",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_5788461",
    "text": "Through history, the systems of economic support for scientists and their work have been important determinants of the character and pace of scientific research.  The ancient foundations of the sciences were driven by practical and religious concerns and or the pursuit of philosophy more generally.  From the Middle Ages until the Age of Enlightenment, scholars sought various forms of noble and religious patronage or funded their own work through medical practice.  In the 18th and 19th centuries, many disciplines began to professionalize, and both government-sponsored \"prizes\" and the first research professorships at universities drove scientific investigation.  In the 20th century, a variety of sources, including government organizations, military funding, patent profits, corporate sponsorship, and private philanthropies, have shaped scientific research.\n\nAncient science\nMost early advances in mathematics, astronomy and engineering were byproducts of more immediate and practical goals.  Surveying and accounting needs drove ancient Egyptian, Babylonian, Chinese, and Indian mathematics, while calendars created for religious and agricultural purposes drove early astronomy.\nModern science owes much of its heritage to ancient Greek philosophers; influential work in astronomy, mechanics, geometry, medicine, and natural history was part of the general pursuit of philosophy.  Architectural knowledge, especially in ancient Greece and Rome, also contributed to the development of mathematics, though the extent of the connection between architectural knowledge and more abstract mathematics and mechanics is unclear.\nState policy has influenced the funding of public works and science for thousands of years, dating at least from the time of the Mohists, who inspired the study of logic  during the period of the Hundred Schools of Thought, and the study of defensive fortifications during the Warring States period in China. General levies of labor and grain were collected to fund great public works in China, including the accumulation of grain for distribution in times of famine, for the building of levees to control flooding by the great rivers of China, for the building of canals and locks to connect rivers of China, some of which flowed in opposite directions to each other, and for  the building of bridges across these rivers. These projects required a civil service, the scholars, some of whom demonstrated great mastery of hydraulics.\n",
    "source": "wikipedia",
    "title": "History of science policy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_24241",
    "text": "In the philosophy of science, protoscience (adj. protoscientific) is a research field that has the characteristics of an undeveloped science that may ultimately develop into an established science.  Philosophers use protoscience to understand the history of science and distinguish protoscience from science and pseudoscience.\nThe word \"protoscience\" is a hybrid Greek-Latin compound of the roots proto- + scientia, meaning a first or primeval rational knowledge.\nExamples of protoscience include alchemy, Wegener's original theory of continental drift and political economy (the predecessor to the modern economic sciences).\n\nHistory\nProtoscience as a research field with the characteristics of an undeveloped science appeared in the early 20th century. In 1910, Jones described the field of political economy as it began the transition to the modern field of economics:\n\nI confess to a personal predilection for some term such as proto-science, pre-science, or nas-science, to give expression to what I conceive to be the true state of affairs, which I take to be this, that economics and kindred subjects are not sciences, but are on the way to become sciences.\nThomas Kuhn later provided a more precise description, protoscience as a field that generates testable conclusions, faces \"incessant criticism and continually strive for a fresh start,\" but currently, like art and philosophy, appears to have failed to progress in a way similar to the progress seen in the established sciences.  He applies protoscience to the fields of natural philosophy, medicine and the crafts in the past that ultimately became established sciences.  Philosophers later developed more precise criteria to identify protoscience using the cognitive field concept.\nThe historian Scott Hendrix argued that the English word \"science\" as it is used by 21st century English speakers means modern science and that the use of the word to describe pre-modern scholars is misleading. \"[E]ven an astute reader is prompted to classify intellectual exercises of the past as 'scientific'...based upon how closely those activities appear to mirror the activities of a modern scientist.\" Noting that natural philosophy was a far more neutral term than \"science\", Hendrix recommended that term be used instead when discussing pre-modern scholars of the natural world. \"[T]here are sound reasons for a return to the use of the term natural philosophy that, for all its imprecision, reveals rather than imposes meaning on the past.\"\n",
    "source": "wikipedia",
    "title": "Protoscience",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1674353",
    "text": "The history of pseudoscience is the study of pseudoscientific theories over time. A pseudoscience is a set of ideas that presents itself as science, while it does not meet the criteria to properly be called such.\nDistinguishing between proper science and pseudoscience is sometimes difficult. One popular proposal for demarcation between the two is the falsification criterion, most notably contributed to by the philosopher Karl Popper. In the history of pseudoscience it can be especially hard to separate the two, because some sciences developed from pseudosciences. An example of this is the science chemistry, which traces its origins from the protoscience of alchemy.\nThe vast diversity in pseudosciences further complicates the history of pseudoscience. Some pseudosciences originated in the pre-scientific era, such as astrology and acupuncture. Others developed as part of an ideology, such as Lysenkoism, or as a response to perceived threats to an ideology. An example of this is creationism, which was developed as a response to the scientific theory of evolution.\nDespite failing to meet proper scientific standards, many pseudosciences survive. This is usually due to a persistent core of devotees who refuse to accept scientific criticism of their beliefs, or due to popular misconceptions. Sheer popularity is also a factor, as is attested by astrology which remains popular despite being rejected by a large majority of scientists.\n\n19th century\nAmong the most notable developments in the history of pseudoscience in the 19th century are the rise of Spiritualism (traced in America to 1848), homeopathy (first formulated in 1796), and phrenology (developed around 1800). Another popular pseudoscientific belief that arose during the 19th century was the idea that there were canals visible on Mars. A relatively mild Christian fundamentalist backlash against the scientific theory of evolution foreshadowed subsequent events in the 20th century.\nThe study of bumps and fissures in people's skulls to determine their character, phrenology, was originally considered a science. It influenced psychiatry and early studies into neuroscience. As science advanced, phrenology was increasingly viewed as a pseudoscience. Halfway through the 19th century, the scientific community had prevailingly abandoned it, although it was not comprehensively tested until much later.\nHalfway through the century, iridology was invented by the Hungarian physician Ignaz von Peczely. The theory would remain popular throughout the 20th century as well.\n\nSpiritualism (sometimes referred to as \"Modern Spiritualism\" or \"Spiritism\")  or \"Modern American Spiritualism\" grew phenomenally during the period. The American version of this movement has been traced to the Fox sisters who in 1848 began claiming the ability to communicate with the dead. The religious movement would remain popular until the 1920s, when renowned magician Harry Houdini began exposing famous mediums and other performers as frauds ",
    "source": "wikipedia",
    "title": "History of pseudoscience",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_26476831",
    "text": "In ancient history, the concepts of chance and randomness were intertwined with that of fate. Many ancient peoples threw dice to determine fate, and this later evolved into games of chance. At the same time, most ancient cultures used various methods of divination to attempt to circumvent randomness and fate.  Beyond religion and games of chance, randomness has been attested for sortition since at least ancient Athenian democracy in the form of a kleroterion.\nThe formalization of odds and chance was perhaps earliest done by the Chinese 3,000 years ago. The Greek philosophers discussed randomness at length, but only in non-quantitative forms. It was only in the sixteenth century that Italian mathematicians began to formalize the odds associated with various games of chance. The invention of modern calculus had a positive impact on the formal study of randomness. In the 19th century the concept of entropy was introduced in physics.\nThe early part of the twentieth century saw a rapid growth in the formal analysis of randomness, and mathematical foundations for probability were introduced, leading to its axiomatization in 1933. At the same time, the advent of quantum mechanics changed the scientific perspective on determinacy. In the mid to late 20th-century, ideas of algorithmic information theory introduced new dimensions to the field via the concept of algorithmic randomness.\nAlthough randomness had often been viewed as an obstacle and a nuisance for many centuries, in the twentieth century computer scientists began to realize that the deliberate introduction of randomness into computations can be an effective tool for designing better algorithms. In some cases, such randomized algorithms are able to outperform the best deterministic methods.\n\nAntiquity to the Middle Ages\nPre-Christian people along the Mediterranean threw dice to determine fate, and this later evolved into games of chance. There is also evidence of games of chance played by ancient Egyptians, Hindus and\nChinese, dating back to 2100 BC. The Chinese used dice before the Europeans, and have a long history of playing games of chance.\nOver 3,000 years ago, the problems concerned with the tossing of several coins were considered in the I Ching, one of the oldest Chinese mathematical texts, that probably dates to 1150 BC. The two principal elements yin and yang were combined in the I Ching in various forms to produce Heads and Tails permutations of the type HH, TH, HT, etc. and the Chinese seem to have been aware of Pascal's triangle long before the Europeans formalized it in the 17th century. However, Western philosophy focused on the non-mathematical aspects of chance and randomness until the 16th century.\nThe development of the concept of chance throughout history has been very gradual. Historians have wondered why progress in the field of randomness was so slow, given that humans have encountered chance since antiquity. Deborah J. Bennett suggests that ordinary people face an inheren",
    "source": "wikipedia",
    "title": "History of randomness",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_42877569",
    "text": "The relationship between mathematics and physics has been a subject of study of philosophers, mathematicians and physicists since antiquity, and more recently also by historians and educators. Generally considered a relationship of great intimacy, mathematics has been described as \"an essential tool for physics\" and physics has been described as \"a rich source of inspiration and insight in mathematics\".\nSome of the oldest and most discussed themes are about the main differences between the two subjects, their mutual influence, the role of mathematical rigor in physics, and the problem of explaining the effectiveness of mathematics in physics.\nIn his work Physics, one of the topics treated by Aristotle is about how the study carried out by mathematicians differs from that carried out by physicists. Considerations about mathematics being the language of nature can be found in the ideas of the Pythagoreans: the convictions that \"Numbers rule the world\" and \"All is number\", and two millennia later were also expressed by Galileo Galilei: \"The book of nature is written in the language of mathematics\".\n\nHistorical interplay\nBefore giving a mathematical proof for the formula for the volume of a sphere, Archimedes used physical reasoning to discover the solution (imagining the balancing of bodies on a scale). Aristotle classified physics and mathematics as theoretical sciences, in contrast to practical sciences (like ethics or politics) and to productive sciences (like medicine or botany).\nFrom the seventeenth century, many of the most important advances in mathematics appeared motivated by the study of physics, and this continued in the following centuries (although in the nineteenth century mathematics started to become increasingly independent from physics). The creation and development of calculus were strongly linked to the needs of physics: There was a need for a new mathematical language to deal with the new dynamics that had arisen from the work of scholars such as Galileo Galilei and Isaac Newton. The concept of derivative was needed, Newton did not have the modern concept of limits, and instead employed infinitesimals, which lacked a rigorous foundation at that time. During this period there was little distinction between physics and mathematics; as an example, Newton regarded geometry as a branch of mechanics. \nNon-Euclidean geometry, as formulated by Carl Friedrich Gauss, János Bolyai, Nikolai Lobachevsky, and Bernhard Riemann, freed physics from the limitation of a single Euclidean geometry. A version of non-Euclidean geometry, called Riemannian geometry, enabled Albert Einstein to develop general relativity by providing the key mathematical framework on which he fit his physical ideas of gravity.\nIn the 19th century Auguste Comte in his hierarchy of the sciences, placed physics and astronomy as less general and more complex than mathematics, as both depend on it. In 1900, David Hilbert in his 23 problems for the advancement of mathematical s",
    "source": "wikipedia",
    "title": "Relationship between mathematics and physics",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_29266",
    "text": "The relationship between science and religion involves discussions that interconnect the study of the natural world, history, philosophy, and theology. Even though the ancient and medieval worlds did not have conceptions resembling the modern understandings of \"science\" or of \"religion\", certain elements of modern ideas on the subject recur throughout history. The pair-structured phrases \"religion and science\" and \"science and religion\" first emerged in the literature during  the 19th century. This coincided with the refining of \"science\" (from the studies of \"natural philosophy\") and of \"religion\" as distinct concepts in the preceding few centuries—partly due to professionalization of the sciences, the Protestant Reformation, colonization, and globalization. Since then the relationship between science and religion has been characterized in terms of \"conflict\", \"harmony\", \"complexity\", and \"mutual independence\", among others.\nBoth science and religion are complex social and cultural endeavors that may vary across cultures and change over time. Most scientific and technical innovations until the scientific revolution were achieved by societies organized by religious traditions. Ancient pagan, Islamic, and Christian scholars pioneered individual elements of the scientific method. Roger Bacon, often credited with formalizing the scientific method, was a Franciscan friar and medieval Christians who studied nature emphasized natural explanations. Confucian thought, whether religious or non-religious in nature, has held different views of science over time. Many 21st-century Buddhists view science as complementary to their beliefs, although the philosophical integrity of such Buddhist modernism has been challenged. While the classification of the material world by the ancient Indians and Greeks into air, earth, fire, and water was more metaphysical, and figures like Anaxagoras questioned certain popular views of Greek divinities, medieval Middle Eastern scholars empirically classified materials.\nEvents in Europe such as the Galileo affair of the early 17th century, associated with the scientific revolution and the Age of Enlightenment, led scholars such as John William Draper to postulate (c. 1874) a conflict thesis, suggesting that religion and science have been in conflict methodologically, factually, and politically throughout history. Some contemporary philosophers and scientists, such as Richard Dawkins, Lawrence Krauss, Peter Atkins, and Donald Prothero subscribe to this thesis; however, such views have not been held by historians of science for a very long time.\nMany scientists, philosophers, and theologians throughout history, from Augustine of Hippo to Thomas Aquinas to Francisco Ayala, Kenneth R. Miller, and Francis Collins, have seen compatibility or interdependence between religion and science. Biologist Stephen Jay Gould regarded religion and science as \"non-overlapping magisteria\", addressing fundamentally separate forms of knowledge and ",
    "source": "wikipedia",
    "title": "Relationship between science and religion",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_4175709",
    "text": "During the Renaissance, great advances occurred in geography, astronomy, chemistry, physics, mathematics, manufacturing, anatomy and engineering. The collection of ancient scientific texts began in earnest at the start of the 15th century and continued up to the Fall of Constantinople in 1453, and the invention of printing allowed a faster propagation of new ideas. Nevertheless, some have seen the Renaissance, at least in its initial period, as one of scientific backwardness. Historians like George Sarton and Lynn Thorndike criticized how the Renaissance affected science, arguing that progress was slowed for some amount of time. Humanists favored human-centered subjects like politics and history over study of natural philosophy or applied mathematics. More recently, however, scholars have acknowledged the positive influence of the Renaissance on mathematics and science, pointing to factors like the rediscovery of lost or obscure texts and the increased emphasis on the study of language and the correct reading of texts.\nMarie Boas Hall coined the term Scientific Renaissance to designate the period leading up to the Scientific Revolution. More recently, Peter Dear has argued for a two-phase model of early modern science: a Scientific Renaissance of the 15th and 16th centuries, focused on the restoration of the natural knowledge of the ancients; and a Scientific Revolution of the 17th century, when scientists shifted from recovery to innovation.\n\nContext\nDuring and after the Renaissance of the 12th century, Europe experienced an intellectual revitalization, especially with regard to the investigation of the natural world. In the 14th century, however, a series of events that would come to be known as the Crisis of the Late Middle Ages was underway. When the Black Death came, it wiped out so many lives it affected the entire system. It brought a sudden end to the previous period of massive scientific change. The plague killed 25–50% of the people in Europe, especially in the crowded conditions of the towns, where the heart of innovations lay. Recurrences of the plague and other disasters caused a continuing decline of population for a century.\n",
    "source": "wikipedia",
    "title": "Science in the Renaissance",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1107345",
    "text": "Many fields of scientific research in the Soviet Union were banned or suppressed with various justifications. All humanities and social sciences were tested for strict accordance with dialectical materialism. These tests served as a cover for political suppression of scientists who engaged in research labeled as \"idealistic\" or \"bourgeois\". Many scientists were fired, others were arrested and sent to Gulags. The suppression of scientific research began during the Stalin era and continued after his death.\nThe ideologically motivated persecution damaged many fields of Soviet science.\n\nExamples\n\nTheme in literature\nVladimir Dudintsev, White Garments (1987), a fictionalized story about Soviet geneticists working during the Lysenkoism era\nSee also\nAcademic freedom\nAntiscience\nAnti-intellectualism\nBourgeois pseudoscience\nCensorship in the Soviet Union\nDeutsche Physik\nFirst Department\nHistorical negationism\nPolitical correctness\nPoliticization of science\nScience and technology in the Soviet Union\nSoviet historiography\nAlexander Veselovsky, a case of suppressed literary research\nStalin and the Scientists\nReferences\n\nЯ. В. Васильков, М. Ю. Сорокина (eds.), Люди и судьбы. Биобиблиографический словарь востоковедов  жертв политического террора в советский период (1917–1991) (\"People and Destiny. Bio-Bibliographic Dictionary of Orientalists – Victims of the political terror during the Soviet period (1917–1991)\"),   Петербургское Востоковедение (2003). online edition\n",
    "source": "wikipedia",
    "title": "Repression of science in the Soviet Union",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_35114916",
    "text": "The role of chance, or \"luck\", in science comprises all ways in which unexpected discoveries are made.\nMany domains, especially psychology, are concerned with the way science interacts with chance –particularly \"serendipity\" (accidents that, through sagacity, are transformed into opportunity). Psychologist Kevin Dunbar and colleagues estimate that between 30% and 50% of all scientific discoveries are accidental in some sense (see examples below). Scientists themselves in the 19th and 20th century acknowledged the role of fortunate luck or serendipity in discoveries.\nPsychologist Alan A. Baumeister says a scientist must be \"sagacious\" (attentive and clever) to benefit from an accident. Dunbar quotes Louis Pasteur's saying that \"Chance favors only the prepared mind\". The prepared mind, Dunbar suggests, is one trained for observational rigor. Dunbar adds that there is a great deal of writing about the role that serendipity (\"happy accidents\") plays in the scientific method.\nResearch suggests that scientists are taught various heuristics and practices that allow their investigations to benefit, and not suffer, from accidents. First, careful control conditions allow scientists to properly identify something as \"unexpected\". Once a finding is recognized as legitimately unexpected and in need of explaining, researchers can attempt to explain it: They work across various disciplines, with various colleagues, trying various analogies in order to understand the first curious finding.\n\nPreparing to make discoveries\nAccidental discoveries have been a topic of discussion especially from the 20th century onwards. Kevin Dunbar and Jonathan Fugelsang say that somewhere between 33% and 50% of all scientific discoveries are unexpected. This helps explain why scientists often call their discoveries \"lucky\", and yet scientists themselves may not be able to detail exactly what role luck played (see also introspection illusion). Dunbar and Fugelsang believe scientific discoveries are the result of carefully prepared experiments, but also \"prepared minds\".\nThe author Nassim Nicholas Taleb calls science \"anti-fragile\". That is, science can actually use—and benefit from—the chaos of the real world. While some methods of investigation are fragile in the face of human error and randomness, the scientific method relies on randomness in many ways. Taleb believes that the more anti-fragile the system, the more it will flourish in the real world. According to  M. K. Stoskopf, it is in this way that serendipity is often the \"foundation for important intellectual leaps of understanding\" in science.\nThe word \"Serendipity\" is frequently understood as simply \"a happy accident\", but Horace Walpole used the word 'serendipity' to refer to a certain kind of happy accident: the kind that can only be exploited by a \"sagacious\" or clever person. Thus Dunbar and Fugelsang talk about, not just luck or chance in science, but specifically \"serendipity\" in science.\nDunbar and Fugelsang suggest",
    "source": "wikipedia",
    "title": "Role of chance in scientific discoveries",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_76454060",
    "text": "Roman Science: Origins, Development, and Influence to the Later Middle Ages is a book by science historian William Harris Stahl, published in 1962 by University of Wisconsin Press.\n\nSynopsis\nThis book covers the history of science in the Latin-speaking  West from its Greek origins to the time of the Graeco-Arabic revival, focusing on the influence of Greek science in the Latin world, and on how this influence shaped both scientific education and scientific culture all the way to the Middle Ages. The volume follows what the author calls \"the handbooks movement\", the production of encyclopedic material originating with Greek authors, such as Posidonius and Theon of Smyrna, and follows this tradition among the Romans. Stahl devotes specific chapters to Pliny, Solinus, Chalcidius, Macrobius, Capella, Boethius, Cassiodorus, Isidore, Bede, and other authors till about 1250, and discusses the genesis and subsequent development of the liberal arts in the Quadrivium and Trivium from the age of Plato (428–424 BC) and Isocrates (436–338 BC) till the Middle Ages and Renaissance.\nContent\nThe initial section on \"Classical Greek Origins\" treats the discoveries of Aristarchus of Samos, Pythagora, the Sophists Hippias of Elis and  Isocrates, Plato, the mathematician Eudoxus – credited with the invention of the Method of exhaustion – \nand Aristotle. The mathematicians Euclid, Archimedes, Apollonius of Perga and Hipparchus are described in the section of the early Hellenistic tradition, together with the early botanist Theophrastus who headed the Peripatetic school after Aristotle, and Eratosthenes of Cyrenes. The first section of the work of Stahl ends with a chapter entitled \"The Posidonian Age\", from Posidonius (c. 135-51 BC) that marks the period when a Greek, mostly Stoic tradition, opens a \"lengthy period of mutual admiration\"  between the Greek and Roman intellectuals. The chapter tells how the historian Polybius and the Stoic philosopher Panaetius were invited to the Scipionic Circle and of the friendship between Posidonius and Cicero. Though no work of Posidonius has reached us, his writings were used extensively by Cicero in his works, and influenced later authors such as Marcus Aurelius and  Seneca.  \nAuthors treated in the central section of Roman science, beside Pliny, include Cato the Elder, Cicero, Varro, Lucretius, Pomponius Mela, Vitruvius, Celsus, and Lucius Annaeus Seneca. Marcus Agrippa has a special mention for his approach of measuring the  length and breadth of each province of the Roman Empire by computing distances recorded on the milestones on the imperial highways. \nNicomachus and Apuleius are treated in the chapter on the second century AD, while Latin neoplatonist encyclopedists Solinus, Calcidius, Macrobius, and Martianus Capella are treated in the chapter on Third- and Fourth-Century Cosmography.\nThe last part of the volume describes the short period of Ostrogothic renaissance, with Boethius and Cassiodorus, then moves to Isidore of S",
    "source": "wikipedia",
    "title": "Roman Science: Origins, Development, and Influence to the Later Middle Ages",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_8255091",
    "text": "19th-century science was greatly influenced by Romanticism (or the Age of Reflection, c. 1800–1840), an intellectual movement that originated in Western Europe as a counter-movement to the late-18th-century Enlightenment.  Romanticism incorporated many fields of study, including politics, the arts, and the humanities.\n\nIn contrast to the Enlightenment's mechanistic natural philosophy, European scientists of the Romantic period held that observing nature implied understanding the self and that knowledge of nature \"should not be obtained by force\".  They felt that the Enlightenment had encouraged the abuse of the sciences, and they sought to advance a new way to increase scientific knowledge, one that they felt would be more beneficial not only to mankind but to nature as well.\nRomanticism advanced a number of themes: it promoted anti-reductionism (that the whole is more valuable than the parts alone) and epistemological optimism (man was connected to nature), and encouraged creativity, experience, and genius. It also emphasized the scientist's role in scientific discovery, holding that acquiring knowledge of nature meant understanding man as well; therefore, these scientists placed a high importance on respect for nature.\nRomanticism declined beginning around 1840 as a new movement, positivism, took hold of intellectuals, and lasted until about 1880.  As with the intellectuals who earlier had become disenchanted with the Enlightenment and had sought a new approach to science, people now lost interest in Romanticism and sought to study science using a stricter process.\n\nRomantic science vs. Enlightenment science\nAs the Enlightenment had a firm hold in France during the last decades of the 18th century, the Romantic view on science was a movement that flourished in Great Britain and especially Germany in the first half of the 19th century.  Both sought to increase individual and cultural self-understanding by recognizing the limits in human knowledge through the study of nature and the intellectual capacities of man.  The Romantic movement, however, resulted as an increasing dislike by many intellectuals for the tenets promoted by the Enlightenment; it was felt by some that Enlightened thinkers' emphasis on rational thought through deductive reasoning and the mathematization of natural philosophy had created an approach to science that was too cold and that attempted to control nature, rather than to peacefully co-exist with nature.\nAccording to the philosophes of the Enlightenment, the path to complete knowledge required dissection of information on any given subject and a division of knowledge into subcategories of subcategories, known as reductionism.  This was considered necessary in order to build upon the knowledge of the ancients, such as Ptolemy, and Renaissance thinkers, such as Copernicus, Kepler, and Galileo. It was widely believed that man's sheer intellectual power alone was sufficient to understanding every aspect of nature. Examples o",
    "source": "wikipedia",
    "title": "Romanticism in science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_66569125",
    "text": "The Royal Commission on Animal Magnetism involved two entirely separate and independent French Royal Commissions, each appointed by Louis XVI in 1784, that were conducted simultaneously by a committee composed of four physicians from the Paris Faculty of Medicine (Faculté de médecine de Paris) and five scientists from the Royal Academy of Sciences (Académie des sciences) (the \"Franklin Commission\", named for Benjamin Franklin), and a second committee composed of five physicians from the Royal Society of Medicine (Société Royale de Médecine) (the \"Society Commission\").\nEach Commission took five months to complete its investigations. The \"Franklin\" Report was presented to the King on 11 August 1784 – and was immediately published and very widely circulated throughout France and neighbouring countries – and the \"Society\" Report was presented to the King five days later on 16 August 1784.\nThe \"Franklin Commission's\" investigations are notable as a very early \"classic\" example of a systematic controlled trial, which not only applied \"sham\" and \"genuine\" procedures to patients with \"sham\" and \"genuine\" disorders, but, significantly, was the first to use the \"blindfolding\" of both the investigators and their subjects.\n\nThe report of the [\"Franklin\"] Royal Commission of 1784 ... is a masterpiece of its genre, and enduring testimony to the power and beauty of reason. ... Never in history has such an extraordinary and luminous group [as the \"Franklin Commission\"] been gathered together in the service of rational inquiry by  the methods of experimental science. For this reason alone the [Report of the \"Franklin Commission\"] ... is a key document in the history of human reason. It should be rescued from obscurity, translated into all languages, and reprinted by organizations dedicated to the unmasking of quackery and the defense of rational thought.\nBoth sets of Commissioners were specifically charged with investigating the claims made by Charles-Nicolas d’Eslon (1750–1786) for the existence of a substantial (rather than metaphorical) \"animal magnetism\", le magnétisme animal, and of a similarly (non-metaphorical) physical \"magnetic fluid\", le fluide magnétique. Further, having completed their investigations into the claims of d'Eslon – that is, they did not examine Franz Mesmer, Mesmer's theories, Mesmer's principles, Mesmer's practices, Mesmer's techniques, Mesmer's apparatus, Mesmer's claims, Mesmer's \"cures\" or, even, \"mesmerism\" itself – they were each required to make \"a separate and distinct report\".\n\nBefore the [\"Franklin\" Commission's] investigations began, [Antoine Lavoisier] had studied the writings of d'Eslon and [had] drawn up a plan for the conduct of the inquiry. He decided that the commissioners should not study any of the alleged cures, but [that] they should determine whether animal magnetism existed by trying to magnetize a person without his knowledge or making him think that he had been magnetized when in fact he had not. This plan was ad",
    "source": "wikipedia",
    "title": "Royal Commission on Animal Magnetism",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_80680176",
    "text": "The Sergey Vavilov Institute for the History of Science and Technology of the Russian Academy of Sciences (or Institute for the History of Science and Technology named after S. I. Vavilov RAS (IHIT or IIET RAS)) is the only research institute in Russia for the study of the history of science and technology. It managed by the Presidium of the Russian Academy of Sciences.\n\nHistory\nIn 1921, the Russian Academy of Sciences established the \"Commission for the Study of the History, Philosophy and Technology\" under the chairmanship of Vladimir Vernadsky (later renamed the Commission on the History of Knowledge).\nFrom 1930, the commission was chaired by N. I. Bukharin.\nStructure\nSt. Petersburg Branch (SPbF IHST RAS; heads: Boris Fedorenko (1953–1955), D.Biol.Sc. P. P. Perfiliev (1956–1962), D.Hist.Sc. A. V. Koltcov (1963–1966, 1972–1973), D.Phil.Sc. Yu. S. Meleshchenko (1967–1972), D.Med.Sc. N. A. Tolokontsev (1973–1975), D.Phil.Sc. B. I. Ivanov (1975–1978), Cand.Eng.Sc. E. P. Karpeev (1978–1987), D.Phil.Sc A. I. Melua (1987–1995), D.Phil.Sc. E. I. Kolchinsky (1995–2015), since 2015 — Cand.Soc.Sc. N. A. Ashcheulova).\nSince 2010, the Exhibition Center of the RAS has been a branch. The center organizes exhibitions of completed works by RAS institutions and the results of the most interesting fundamental research at Russian and international exhibitions in Russia, as well as exhibitions of works by the Russian Academy of Sciences at foreign exhibitions organized by Russian ministries and agencies, foreign companies, and organizations.\nJournals:\n\nStudies in the History of Science and Technology (VIET)\nStudies in the History of Biology (since 2009, quarterly)\nYearbooks:\n\nHistorico-Mathematical Research (since 1948)\nResearch on the History of Physics and Mechanics (since 1986)\nHistorico-Astronomical Research (since 1955).\nCouncils\nThe institute has several dissertation councils in the specialty \"history of science and technology\".\nIHST holds annual conferences on the history of science and technology in Moscow and St. Petersburg. The institute hosts several regular Moscow-wide seminars—on the history of astronomy, the history of physics and mechanics, and the history of the Soviet atomic project.\nIn 2004, the Academic Council and the Council of Young Scientists of IHST RAS established the \"Alexey Karimov Memorial Prize\", awarded to young scientists of the institute for significant contributions to the study of the history of science and technology.\n\nStructure\nDepartment of the History of Technology and Technical Sciences\nDepartment of the History of Physical and Mathematical Sciences\nCenter for the History of the Organization of Science and Science Studies (CHONS)\nEcological Center\nCenter for the History of Socio-Cultural Problems of Science and Technology\nDepartment of Historiography and Source Studies of the History of Science and Technology\nDepartment of Methodological and Social Problems of the Development of Science\nDepartment of the History of Chemical a",
    "source": "wikipedia",
    "title": "S.I. Vavilov Institute for the History of Science and Technology RAS",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_22734942",
    "text": "Africa has the world's oldest record of human technological achievement: the oldest surviving stone tools in the world have been found in eastern Africa, and later evidence for tool production by humans' hominin ancestors has been found across West, Central, Eastern and Southern Africa. The history of science and technology in Africa since then has, however, received relatively little attention compared to other regions of the world, despite notable African developments in mathematics, metallurgy, architecture, and other fields.\n\nEarly humans\nThe Great Rift Valley of Africa provides critical evidence for the evolution of early hominins. The earliest tools in the world can be found there as well:\n\nAn unidentified hominin, possibly Australopithecus afarensis or Kenyanthropus platyops, created stone tools dating to 3.3 million years ago at Lomekwi in the Turkana Basin, eastern Africa.\nHomo habilis, residing in eastern Africa, developed another early toolmaking industry, the Oldowan, around 2.3 million years ago.\nHomo erectus developed the Acheulean stone tool industry, specifically hand-axes, at 1.5 million years ago. This tool industry spread to the Middle East and Europe around 800,000 to 600,000 years ago. Homo erectus also begins using fire.\nHomo sapiens, or modern humans, created bone tools and backed blades around 90,000 to 60,000 years ago, in southern and eastern Africa. The use of bone tools and backed blades eventually became characteristic of Later Stone Age tool industries. The first appearance of abstract art is during the Middle Stone Age, however. The oldest abstract art in the world is a shell necklace dated to 82,000 years ago from the Cave of Pigeons in Taforalt, eastern Morocco. The second oldest abstract art and the oldest rock art is found at Blombos Cave in South Africa, dated to 77,000 years ago. There are evidences that Stone Age humans around 100,000 years ago had an elementary knowledge of chemistry in Southern Africa, and that they used a specific recipe to create a liquefied ochre-rich mixture. According to Henshilwood, \"This isn't just a chance mixture, it is early chemistry. It suggests conceptual and probably cognitive abilities which are the equivalent of modern humans\".\n",
    "source": "wikipedia",
    "title": "History of science and technology in Africa",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_8759532",
    "text": "Science and technology in Germany has a long and illustrious history, and research and development efforts form an integral part of the country's economy. Germany has been the home of some of the most prominent researchers in various scientific disciplines, notably physics, mathematics, chemistry and engineering. Before World War II, Germany had produced more Nobel laureates in scientific fields than any other nation, and was the preeminent country in the natural sciences. Germany is currently the nation with the 3rd most Nobel Prize winners, 115.\nThe German language, along with English and French, was one of the leading languages of science from the late 19th century until the end of World War II. After the war, because so many scientific researchers' and teachers' careers had been ended either by Nazi Germany which started a brain drain, the denazification process, the American Operation Paperclip and Soviet Operation Osoaviakhim which exacerbated the brain drain in post-war Germany, or simply losing the war, \"Germany, German science, and German as the language of science had all lost their leading position in the scientific community.\"\nToday, scientific research in the country is supported by industry, the network of German universities and scientific state-institutions such as the Max Planck Society and the Deutsche Forschungsgemeinschaft. The raw output of scientific research from Germany consistently ranks among the world's highest. Germany was declared the most innovative country in the world in the 2020 Bloomberg Innovation Index and  was ranked 11th in the Global Innovation Index in 2025.\n\nInstitutions\nThe Union of German Academies of Sciences and Humanities (abbreviated Academies Union) is an association of the eight largest academies of sciences in Germany.\nThe Deutsches Museum, 'German Museum' of Masterpieces of Science and Technology in Munich is one of the largest science and technology museums in the world in terms of exhibition space, with about 28,000 exhibited objects from 50 fields of science and technology.\nThe Bundesministerium für Bildung und Forschung, 'Federal Ministry of Education and Research' (BMBF) is a supreme authority of the Federal Republic of Germany for science and technology. The headquarter of the Federal Ministry is located in Bonn, the second office in Berlin. It was founded in 1972 as Federal Ministry of Research and Technology (BMFT) to promote basic research, applied research and technological development.\nFederal Ministry for Economic Affairs and Climate Action (German: Bundesministerium für Wirtschaft und Klimaschutz (BMWK, previous BMWi)\n",
    "source": "wikipedia",
    "title": "Science and technology in Germany",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_17912788",
    "text": "The history of science during the Age of Enlightenment traces developments in science and technology during the Age of Reason, when Enlightenment ideas and ideals were being disseminated across Europe and North America. Generally, the period spans from the final days of the 16th- and 17th-century Scientific Revolution until roughly the 19th century, after the French Revolution (1789) and the Napoleonic era (1799–1815). The scientific revolution saw the creation of the first scientific societies, the rise of Copernicanism, and the displacement of Aristotelian natural philosophy and Galen's ancient medical doctrine. By the 18th century, scientific authority began to displace religious authority, and the disciplines of alchemy and astrology lost scientific credibility.\nWhile the Enlightenment cannot be pigeonholed into a specific doctrine or set of dogmas, science came to play a leading role in Enlightenment discourse and thought. Many Enlightenment writers and thinkers had backgrounds in the sciences and associated scientific advancement with the overthrow of religion and traditional authority in favour of the development of free speech and thought. Broadly speaking, Enlightenment science greatly valued empiricism and rational thought, and was embedded with the Enlightenment ideal of advancement and progress. As with most Enlightenment views, the benefits of science were not seen universally; Jean-Jacques Rousseau criticized the sciences for distancing man from nature and not operating to make people happier.\nScience during the Enlightenment was dominated by scientific societies and academies, which had largely replaced universities as centres of scientific research and development. Societies and academies were also the backbone of the maturation of the scientific profession. Another important development was the popularization of science among an increasingly literate population. Philosophes introduced the public to many scientific theories, most notably through the Encyclopédie and the popularization of Newtonianism by Voltaire as well as by Émilie du Châtelet, the French translator of Newton's Philosophiæ Naturalis Principia Mathematica. Some historians have marked the 18th century as a drab period in the history of science; however, the century saw significant advancements in the practice of medicine, mathematics, and physics; the development of biological taxonomy; a new understanding of magnetism and electricity; and the maturation of chemistry as a discipline, which established the foundations of modern chemistry.\n\nUniversities\nThe number of universities in Paris remained relatively constant throughout the 18th century. Europe had about 105 universities and colleges by 1700. North America had 44, including the newly founded Harvard and Yale. The number of university students remained roughly the same throughout the Enlightenment in most Western nations, excluding Britain, where the number of institutions and students increased. University st",
    "source": "wikipedia",
    "title": "Science in the Age of Enlightenment",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_3143150",
    "text": "The history of scientific method considers changes in the methodology of scientific inquiry, as distinct from the history of science itself. The development of rules for scientific reasoning has not been straightforward; scientific method has been the subject of intense and recurring debate throughout the history of science, and eminent natural philosophers and scientists have argued for the primacy of one or another approach to establishing scientific knowledge.\nRationalist explanations of nature, including atomism, appeared both in ancient Greece in the thought of Leucippus and Democritus, and in ancient India, in the Nyaya, Vaisheshika and Buddhist schools, while Charvaka materialism rejected inference as a source of knowledge in favour of an empiricism that was always subject to doubt. Aristotle pioneered scientific method in ancient Greece alongside his empirical biology and his work on logic, rejecting a purely deductive framework in favour of generalisations made from observations of nature.\nSome of the most important debates in the history of scientific method center on: rationalism, especially as advocated by René Descartes; inductivism, which rose to particular prominence with Isaac Newton and his followers; and hypothetico-deductivism, which came to the fore in the early 19th century.  In the late 19th and early 20th centuries, a debate over realism vs. antirealism was central to discussions of scientific method as powerful scientific theories extended beyond the realm of the observable, while in the mid-20th century some prominent philosophers argued against any universal rules of science at all.\n\nEarly methodology\n\nEmergence of inductive experimental method\nDuring the Middle Ages issues of what is now termed science began to be addressed. There was greater emphasis on combining theory with practice in the Islamic world than there had been in Classical times, and it was common for those studying the sciences to be artisans as well, something that had been \"considered an aberration in the ancient world.\" Islamic experts in the sciences were often expert instrument makers who enhanced their powers of observation and calculation with them. Starting in the early ninth century, early Muslim scientists such as al-Kindi (801–873) and the authors writing under the name of Jābir ibn Hayyān (writings dated to c. 850–950) began to put a greater emphasis on the use of experiment as a source of knowledge. Several scientific methods thus emerged from the medieval Muslim world by the early 11th century, all of which emphasized experimentation as well as quantification to varying degrees.\n",
    "source": "wikipedia",
    "title": "History of scientific method",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_6231816",
    "text": "This is a list of priority disputes in history of science and science-related fields (such as mathematics).\n\nAstronomy\n1558 invention of the geoheliocentric system: Tycho Brahe, Nicolaus Raimarus Ursus\n1609–1610 Galilean moons: Galileo, Simon Marius\n1612 discovery of sunspots: Galileo Galilei, Christoph Scheiner\n1846 prediction of Neptune: Urbain Le Verrier, John Couch Adams\n2004–2005 controversy over the discovery of Haumea: José Luis Ortiz Moreno, Michael E. Brown.\nBiology and medicine\n1652 discovery of the lymphatic system: Olof Rudbeck, Thomas Bartholin\nc. 1660 teaching a deaf-mute person to speak: John Wallis, William Holder\nc. 1667 first human blood transfusion: Richard Lower, Henry Oldenburg, Jean-Baptiste Denys\nc. 1859 development of the theory of evolution: Charles Darwin, Alfred Russel Wallace, Patrick Matthew\n1877–1892 Bone Wars:  Edward Drinker Cope, Othniel Charles Marsh.\n1882–1889: Koch–Pasteur rivalry: Louis Pasteur, Robert Koch.\n1899–1902 discovery of the life cycle of malarial parasite: Giovanni Battista Grassi, Ronald Ross\n1953–1962 discovery of the DNA structure: Francis Crick, James D. Watson, Rosalind Franklin, Erwin Chargaff, Oswald Avery\n1971–1973 discovery of opiate receptors: Candace Pert, Solomon H. Snyder1971–1975 invention of magnetic resonance imaging (MRI): Paul Lauterbur, Peter Mansfield, Raymond Vahan Damadian, and others (see 2003 Nobel Prize in Physiology or Medicine)\n1983 discovery of HIV: Robert Gallo, Luc Montagnier (see 2008 Nobel Prize in Physiology or Medicine)\nChemistry\n1604-1777 discovery of oxygen: Joseph Priestley, Carl Wilhelm Scheele, Antoine Laurent Lavoisier\n1864 synthesis dicarboxylic acids from carboxylic acids (diacids from monoacid reactions): Hugo Müller, Hermann Kolbe, Hans Hübner, Friedrich Konrad Beilstein, Maxwell Simpson.\n1870–1895 development of the periodic table: Dmitri Mendeleev, Lothar Meyer\n1960–1994 Transfermium Wars: Lawrence Berkeley National Laboratory, Joint Institute for Nuclear Research.\nMathematics\n1550–1557 discovery of solutions to cubic equations: Niccolò Tartaglia, Gerolamo Cardano\n1669–1704 discovery of l'Hôpital's rule: Guillaume de l'Hôpital, Johann Bernoulli.\n1699–1716 Leibniz–Newton calculus controversy: Isaac Newton, Gottfried Leibniz\n1949 proof of the prime number theorem: Atle Selberg and/or Paul Erdős\n2002–2003 proof of the Poincaré conjecture: Grigori Perelman or Shing-Tung Yau\n",
    "source": "wikipedia",
    "title": "List of scientific priority disputes",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_64503842",
    "text": "The Second International Congress of the History of Science was held in London from June 29 to July 4, 1931. The Congress was organised by the International Committee of History of Science, in conjunction with the Comité International des Sciences Historiques. The History of Science Society and the Newcomen Society also supported the event. Charles Singer presided over the congress. Although organised by the International Committee of History of Science, it was during this congress that this organisation was transformed into an individual membership organisation called the International Academy of the History of Science.\nThe inaugural session was held in the Great Hall of the Royal Geographical Society. This was opened by Hastings Lees-Smith, President of the Board of Education. The rest of the congress was conducted in four sessions held in the lecture hall of the Science Museum.\n\nSessions\n\nReferences\n\nExternal links\nEn español: Pablo Huerga Melcón \"El Congreso de Londres de 1931\"\n[Kupriyanov, V.A. (2023). The Second Congress on the History of Science and Technology in London, 1931, in the history of science of the first half of the 20th century. Part I. The origin of the idea of international congresses on the history of science. The Digital Scholar: Philosopher's Lab, 6 (4): 127–144. DOI: 10.32326/2618-9267-2023-6-4-127-144]\n[Kupriyanov, V.A. (2023). The Second Congress on the History of Science and Technology in London, 1931 in the history of science of the first half of the XXth century. Part II. The holding of the congress and its significance for historiography.The Digital Scholar: Philosopher's Lab, 6 (4): 145–167. DOI: 10.32326/2618-9267-2023-6-4-145-167]\n",
    "source": "wikipedia",
    "title": "Second International Congress of the History of Science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_10981939",
    "text": "Small science (in contrast to big science) is science performed in a smaller scale, such as by individuals, small teams or within community projects.\nBodies which fund research, such as the National Science Foundation, DARPA, and the EU with its Framework programs, have a tendency to fund larger-scale research projects. Reasons include the idea that ambitious research needs significant resources devoted for its execution and the reduction of administrative and overhead costs on the funding body side. However, small science which has data that is often local and is not easily shared is funded in many areas such as chemistry and biology by these funding bodies.\n\nImportance\nSmall Science helps define the goals and directions of large scale scientific projects. In turn, results of large scale projects are often best synthesized and interpreted by the long-term efforts of the Small Science community. In addition, because Small Science is typically done at universities, it provides students and young researchers with an integral involvement in defining and solving scientific problems. Hence, small science can be seen as an important factor for bringing together science and society.\nAccording to the Chronicle for Higher Education,  James M. Caruthers, a professor of chemical engineering at Purdue University, data from Big Science is highly organized on the front end where researchers define it before it even starts rolling off the machines, making it easier to handle, understand, and archive. Small Science is \"horribly heterogeneous,\" and far more vast. In time, Small Science will generate two to three times more data than Big Science.\nThe American Geophysical Union stresses the importance of small science in a position statement.\nExamples of results with high impact\nMany historical examples show that results of Small Science can have enormous impacts:\n\nGalois theory, one of the foundational theories of abstract algebra was developed by Évariste Galois within just weeks.\nAlbert Einstein developed his theory of special relativity as a hobby while working full-time in a patent office.\nRobert Goddard invented the liquid propelled and multi stage rockets largely on his own.  These breakthroughs lead to the German V2 and the Apollo Saturn V rockets.\n",
    "source": "wikipedia",
    "title": "Small science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_56111973",
    "text": "Stalin and the Scientists: A History of Triumph and Tragedy 1905–1953 is a 2016 popular science non-fiction book on the history of science in the Soviet Union under Joseph Stalin by English novelist and science writer, Simon Ings. It is Ings' second non-fiction book, the first being The Eye: A Natural History (2007). He had previously published eight novels.\nStalin and the Scientists was longlisted for the 2016 Baillie Gifford Prize for Non-Fiction.\n\nBackground\nIngs' inspiration for Stalin and the Scientists came from Soviet psychologist, Alexander Luria's book Mind of a Mnemonist, about the life of Russian journalist and mnemonist, Solomon Shereshevsky. Ings said in 2016 interviews that Luria is often referred to as the founder of modern neuroanatomy and \"the godfather of the literary genre we call popular science\". \"Luria's account more or less set the template for modern popular science and ... pretty much set me on the path I'm on now.\" Ings had considered writing a biography about Luria, but felt that while Luria's achievements were \"extraordinary\", considering the climate of political repression he worked in, Ings was concerned that Western readers would consider his career too ordinary, and would miss the context in which it unfolded. Ings' passion for popular science and the need to explain the context within which Luria and other Soviet scientists worked, changed what would have been a one-year \"modest biography\" into a \"five-year behemoth\" that \"burned through three editors\" and, Ings added, \"nearly killed me\".\nIngs said, as a novelist, he was \"absurdly under-qualified\" to tackle a book like Stalin and the Scientists, but added that only a novelist could be so \"ridiculously ambitious\" and \"naive enough to stick his or her neck out so far\". Ings felt that given the kind of science prevalent in Russia at the time, perhaps this \"really has to be the job of a novelist rather than a historian\". Responding to statements that this is \"the first history\" of Soviet science, Ings said, \"Certainly no-one's been foolish enough to attempt to tell the whole story of science under Stalin in a single volume, but be assured I didn't dig this entire thing single-handed from virgin ground.\"\n",
    "source": "wikipedia",
    "title": "Stalin and the Scientists",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_224636",
    "text": "Supersymmetry is a theoretical framework in physics that suggests the existence of a symmetry between particles with integer spin (bosons) and particles with half-integer spin (fermions). It proposes that for every known particle, there exists a partner particle with different spin properties. There have been multiple experiments on supersymmetry that have failed to provide evidence that it exists in nature. If evidence is found, supersymmetry could help explain certain phenomena, such as the nature of dark matter and the hierarchy problem in particle physics. \nA supersymmetric theory is a theory in which the equations for force and the equations for matter are identical. In theoretical and mathematical physics, any theory with this property has the principle of supersymmetry (SUSY). Dozens of supersymmetric theories exist. In theory, supersymmetry is a type of spacetime symmetry between two basic classes of particles: bosons, which have an integer-valued spin and follow Bose–Einstein statistics, and fermions, which have a half-integer-valued spin and follow Fermi–Dirac statistics. The names of bosonic partners of fermions are prefixed with s-, because they are scalar particles. For example, if the electron existed in a supersymmetric theory, then there would be a particle called a selectron (superpartner electron), a bosonic partner of the electron.\nIn supersymmetry, each particle from the class of fermions would have an associated particle in the class of bosons, and vice versa, known as a superpartner. The spin of a particle's superpartner is different by a half-integer. In the simplest supersymmetry theories, with perfectly \"unbroken\" supersymmetry, each pair of superpartners would share the same mass and internal quantum numbers besides spin. More complex supersymmetry theories have a spontaneously broken symmetry, allowing superpartners to differ in mass.\nSupersymmetry has various applications to different areas of physics, such as quantum mechanics, statistical mechanics, quantum field theory, condensed matter physics, nuclear physics, optics, stochastic dynamics, astrophysics, quantum gravity, and cosmology. Supersymmetry has also been applied to high-energy physics, where a supersymmetric extension of the Standard Model is a possible candidate for physics beyond the Standard Model. However, no supersymmetric extensions of the Standard Model have been experimentally verified, and some physicists are saying the theory is dead.\n\nHistory\nA supersymmetry relating mesons and baryons was first proposed, in the context of hadronic physics, by Hironari Miyazawa in 1966. This supersymmetry did not involve spacetime, that is, it concerned internal symmetry, and was broken badly. Miyazawa's work was largely ignored at the time.\nJ. L. Gervais and B. Sakita (in 1971), Yu. A. Golfand and E. P. Likhtman (also in 1971), and D. V. Volkov and V. P. Akulov (1972), independently rediscovered supersymmetry in the context of quantum field theory, a radically n",
    "source": "wikipedia",
    "title": "Supersymmetry",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_71515213",
    "text": "The history of synthetic-aperture radar begins in 1951, with the invention of the technology by mathematician Carl A. Wiley, and its development in the following decade. Initially developed for military use, the technology has since been applied in the field of planetary science.\n\nInvention\nCarl A. Wiley, a mathematician at Goodyear Aircraft Company in Litchfield Park, Arizona, invented synthetic-aperture radar in June 1951 while working on a correlation guidance system for the Atlas ICBM program. In early 1952, Wiley, together with Fred Heisley and Bill Welty, constructed a concept validation system known as DOUSER (\"Doppler Unbeamed Search Radar\"). During the 1950s and 1960s, Goodyear Aircraft (later Goodyear Aerospace) introduced numerous advancements in SAR technology, many with help from Don Beckerleg.\nIndependently of Wiley's work, experimental trials in early 1952 by Sherwin and others at the University of Illinois' Control Systems Laboratory showed results that they pointed out \"could provide the basis for radar systems with greatly improved angular resolution\" and might even lead to systems capable of focusing at all ranges simultaneously.\nIn both of those programs, processing of the radar returns was done by electrical-circuit filtering methods. In essence, signal strength in isolated discrete bands of Doppler frequency defined image intensities that were displayed at matching angular positions within proper range locations. When only the central (zero-Doppler band) portion of the return signals was used, the effect was as if only that central part of the beam existed. That led to the term Doppler Beam Sharpening. Displaying returns from several adjacent non-zero Doppler frequency bands accomplished further \"beam-subdividing\" (sometimes called \"unfocused radar\", though it could have been considered \"semi-focused\"). Wiley's patent, applied for in 1954, still proposed similar processing. The bulkiness of the circuitry then available limited the extent to which those schemes might further improve resolution.\nThe principle was included in a memorandum authored by Walter Hausz of General Electric that was part of the then-secret report of a 1952 Dept. of Defense summer study conference called TEOTA (\"The Eyes of the Army\"), which sought to identify new techniques useful for military reconnaissance and technical gathering of intelligence. A follow-on summer program in 1953 at the University of Michigan, called Project Wolverine, identified several of the TEOTA subjects, including Doppler-assisted sub-beamwidth resolution, as research efforts to be sponsored by the Department of Defense (DoD) at various academic and industrial research laboratories. In that same year, the Illinois group produced a \"strip-map\" image exhibiting a considerable amount of sub-beamwidth resolution.\n",
    "source": "wikipedia",
    "title": "History of synthetic-aperture radar",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_2202274",
    "text": "Table-turning (also known as table-tapping, table-tipping or table-tilting) is a type of séance in which participants sit around a table, place their hands on it, and wait for rotations. The table was purportedly made to serve as a means of communicating with the spirits; the alphabet would be slowly spoken aloud and the table would tilt at the appropriate letter, thus spelling out words and sentences. The process is similar to that of a Ouija board. Scientists and skeptics consider table-turning to be the result of the ideomotor effect, or of conscious trickery.\n\nHistory\nWhen the movement of spiritualism first reached Europe from America in the winter of 1852–1853, the most popular method of consulting the spirits was for several persons to sit round a table, with their hands resting on it, and wait for the table to move. If the experiment was successful, the table would rotate with considerable rapidity and would occasionally rise in the air, or perform other movements.\nWhilst most spiritualists ascribed the table movements to the agency of spirits, two investigators, count de Gasparin and professor Thury (father of René Thury) of Geneva, conducted a careful series of experiments. They claimed to have demonstrated that the movements of the table were due to a physical force emanating from the bodies of the sitters, for which they proposed the name ectenic force. Their conclusion rested on the supposed elimination of all known physical causes for the movements; but it is doubtful from the description of the experiments whether the precautions taken were sufficient to exclude unconscious muscular action (the ideomotor effect) or even deliberate fraud.\nIn England, table-turning became a fashionable diversion and was practised all over the country in the year 1853. John Elliotson and his followers attributed the phenomena to mesmerism. The general public were content to find the explanation of the movements in spirits, animal magnetism, Odic force, galvanism, electricity, or even the rotation of the earth. Some Evangelical clergymen alleged that the spirits who caused the movements were of a diabolic nature. In France, Allan Kardec studied the phenomenon and concluded in The Book on Mediums that some communications were caused by an outside intelligence, as the message contained information that was not known to the group.\n",
    "source": "wikipedia",
    "title": "Table-turning",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_49535",
    "text": "A thought experiment is an imaginary scenario that is meant to elucidate or test an argument or theory. It is often an experiment that would be hard, impossible, or unethical to actually perform. It can also be an abstract hypothetical that is meant to test our intuitions about morality or other fundamental philosophical questions.\n\nHistory\nThe ancient Greek δείκνυμι, deiknymi, 'thought experiment', \"was the most ancient pattern of mathematical proof\", and existed before Euclidean mathematics, where the emphasis was on the conceptual, rather than on the experimental part of a thought experiment.\nJohann Witt-Hansen established that Hans Christian Ørsted was the first to use the equivalent German term Gedankenexperiment c. 1812. Ørsted was also the first to use the equivalent term Gedankenversuch in 1820.\nBy 1883, Ernst Mach used Gedankenexperiment in a different sense, to denote exclusively the imaginary conduct of a real experiment that would be subsequently performed as a real physical experiment by his students. Physical and mental experimentation could then be contrasted: Mach asked his students to provide him with explanations whenever the results from their subsequent, real, physical experiment differed from those of their prior, imaginary experiment.\nThe English term thought experiment was coined as a calque of Gedankenexperiment, and it first appeared in the 1897 English translation of one of Mach's papers. Prior to its emergence, the activity of posing hypothetical questions that employed subjunctive reasoning had existed for a very long time for both scientists and philosophers. The irrealis moods are ways to categorize it or to speak about it. This helps explain the extremely wide and diverse range of the application of the term thought experiment once it had been introduced into English.\n\nGalileo's demonstration that falling objects must fall at the same rate regardless of their masses was a significant step forward in the history of modern science. This is widely thought to have been a straightforward physical demonstration, involving climbing up the Leaning Tower of Pisa and dropping two heavy weights off it, whereas in fact, it was a logical demonstration, using the thought experiment technique. The experiment is described by Galileo in his 1638 work Two New Sciences thus:\n\nSalviati: If then we take two bodies whose natural speeds are different, it is clear that on uniting the two, the more rapid one will be partly retarded by the slower, and the slower will be somewhat hastened by the swifter. Do you not agree with me in this opinion?Simplicio: You are unquestionably right.Salviati: But if this is true, and if a large stone moves with a speed of, say, eight while a smaller moves with a speed of four, then when they are united, the system will move with a speed less than eight; but the two stones when tied together make a stone larger than that which before moved with a speed of eight. Hence the heavier body moves with less speed th",
    "source": "wikipedia",
    "title": "Thought experiment",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_3373310",
    "text": "Traditional knowledge (TK), indigenous knowledge (IK), folk knowledge, and local knowledge generally refers to knowledge systems embedded in the cultural traditions of regional, indigenous, or local communities.\nTraditional knowledge includes types of knowledge about traditional technologies of areas such as subsistence (e.g. tools and techniques for hunting or agriculture), midwifery, ethnobotany and ecological knowledge, traditional medicine, celestial navigation, craft skills, ethnoastronomy, climate, and others. These systems of knowledge are generally based on accumulations of empirical observation of and interaction with the environment, transmitted orally across generations. \nThe World Intellectual Property Organization (WIPO) and the United Nations (UN) include traditional cultural expressions (TCE) in their respective definitions of indigenous knowledge. Traditional knowledge systems and cultural expressions exist in the forms of culture, stories, legends, folklore, rituals, songs, and laws, languages, songlines, dance, games, mythology, designs, visual art and architecture.\n\nCharacteristics and related concepts\nA report of the International Council for Science (ICSU) Study Group on Science and Traditional Knowledge characterises traditional knowledge as:\n\na cumulative body of knowledge, know-how, practices and representations maintained and developed by peoples with extended histories of interaction with the natural environment. These sophisticated sets of understandings, interpretations and meanings are part and parcel of a cultural complex that encompasses language, naming and classification systems, resource use practices, ritual, spirituality and worldview.\n\nTraditional knowledge typically distinguishes one community from another. In some communities, traditional knowledge takes on personal and spiritual meanings. Traditional knowledge can also reflect a community's interests. Some communities depend on their traditional knowledge for survival. Traditional knowledge regarding the environment, such as taboos, proverbs and cosmological knowledge systems, may provide a conservation ethos for biodiversity preservation. This is particularly true of traditional environmental knowledge, which refers to a \"particular form of place-based knowledge of the diversity and interactions among plant and animal species, landforms, watercourses, and other qualities of the biophysical environment in a given place\". As an example of a society with a wealth of traditional ecological knowledge (TEK), the South American Kayapo people, have developed an extensive classification system of ecological zones of the Amazonian tropical savannah (i.e., campo / cerrado) to better manage the land.\nSome social scientists conceptualise knowledge within a naturalistic framework and emphasize the gradation of recent knowledge into knowledge acquired over many generations. These accounts use terms like adaptively acquired knowledge, socially constructed knowledge, and o",
    "source": "wikipedia",
    "title": "Traditional knowledge",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_63988166",
    "text": "The Troughton scale is a measurement scale that de facto served as the first national standard of length in the United States, from 1832 until 1856.\n\nPhysical description\nThe measurement scale spans 82 inches and is subdivided to tenths of inches. It is marked on a silver inlay in a brass bar; the bar itself is about 86 inches long.\nHistory\nThe scale was prepared for the Office of Coast Survey by Troughton of London and was brought to the United States in 1815 by F. R. Hassler, who a year later became first Superintendent of the Survey of the Coast and, in 1832, first Superintendent of Weights and Measures.\nAt the time, the United States Government was principally financed by duties on imports and exports (the federal income tax did not become a permanent feature of the US system until 1913). The appropriate import and export taxes on commercial items were determined at customhouses maintained by the federal government at various ports of entry. A reliable and uniform system of weights and measure was necessary for this system to work, as well as for settling commercial disputes.\nIn 1830, the US Senate requested the Secretary of the Treasury to ‘cause a comparison to be made of the standards of weight and measure now used at the principal custom houses in the United States, and report to the Senate at the next session of Congress.’  To carry out this mandate, the Treasury Secretary appointed Hassler, who found that (as was suspected) large discrepancies existed among the weights and measures in use at the principal customhouses at different US ports.\nThe Treasury Department immediately started constructing new necessary weights and measures for the customs service. For this purpose, the Treasury Department had to choose standards, and the standard yard adopted was the 36 inches comprised between the 27th and the 63rd inches of the Troughton scale. This 36-inch space was supposed to be identical with the English standard at 62 °F, though it had never been directly compared with that standard. The original English standard, in turn, was made in 1758, but was then damaged beyond the point of usability in the great fire of 1834.\nIn 1856, the US received two copies of the new British standard yard after Britain completed the manufacture of new imperial standards to replace those lost in 1834. As standards of length, the new yards, especially bronze No. 11, were far superior to the Troughton scale. They were therefore accepted by the Office of Weights and Measures (a predecessor of NIST) as length standards of the United States.\n",
    "source": "wikipedia",
    "title": "Troughton scale",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_1232743",
    "text": "Universal science (German: Universalwissenschaft; Latin: scientia generalis, scientia universalis) is a branch of metaphysics, dedicated to the study of the underlying principles of all science. Instead of viewing knowledge as being separated into branches, Universalists view all knowledge as being part of a single category. Universal science is related to, but distinct from, universal language.\n\nPrecursors\nLogic and rationalism lie at the foundation of the ideas of universal science. In a broad sense, logic is the study of reasoning. Although there were individuals that implicitly utilized logical methods prior to Aristotle, it is generally agreed he was the originator of modern systems of logic. The Organon, Aristotle's books on logic, details this system. In Categories, Aristotle separates everything into 10 \"categories\": substance, quantity, quality, relation, place, time, position, state, action, and passion. In De Interpretatione, Aristotle studied propositions, detailing what he determined were the most basic propositions and the relationships between them. The Organon had several other books, which further detailed the process of constructing arguments, deducing logical consequences, and even contained the foundations of the modern scientific method. \nThe most immediate predecessor to universal science is the system of formal logic, which is the study of the abstract notions of propositions and arguments, usually utilizing symbols to represent these structures. Formal logic differs from previous systems of logic by looking exclusively at the structure of an argument, instead of at the specific aspects of each statement. Thus, while the statements \"Jeff is shorter than Jeremy and Jeremy is shorter Aidan, so Jeff is shorter than Aidan\" and \"Every triangle has less sides than every rectangle and every rectangle has less sides than every pentagon, so every triangle has less sides than every pentagon\" deal with different specific information, they are both are equivalent in formal logic to the expression\n\n  \n    \n      \n        ∀\n        x\n        ∈\n        X\n        ,\n        y\n        ∈\n        Y\n        ,\n        z\n        ∈\n        Z\n        ,\n        \n        x\n        <\n        y\n        ∧\n        y\n        <\n        z\n        \n        ⟹\n        \n        x\n        <\n        z\n      \n    \n    {\\displaystyle \\forall x\\in X,y\\in Y,z\\in Z,\\quad x<y\\wedge y<z\\implies x<z}\n  \n.                                  \nBy abstracting away from the specifics of each statement and argument, formal logic allows the overarching structure of logic to be studied. This viewpoint inspired later logicians to seek out a set of minimal size containing all of the requisite knowledge from which everything else could be derived and is the fundamental idea behind universal science.\n",
    "source": "wikipedia",
    "title": "Universal science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_35308606",
    "text": "The Welbeck Academy or Welbeck Circle is a name that has been given to the loose intellectual grouping around William Cavendish, 1st Duke of Newcastle-upon-Tyne in the first half of the 17th century. It takes its name from Welbeck Abbey, a country house in Nottinghamshire that was a Cavendish family seat. Another term used is Newcastle Circle. The geographical connection is, however, more notional than real; and these terms have been regarded also as somewhat misleading. Cavendish was Viscount Mansfield in 1620, and moved up the noble ranks to Duke, step by step; \"Newcastle\" applies by 1628.\nNewcastle was a royalist exile in continental Europe in the latter part of the First English Civil War and the Interregnum. He then returned to England and lived to 1676. His life shows many instances of cultural and intellectual patronage.\n\nScience and mathematics\nA scientific interest was optics. The group involved in these studies included Charles Cavendish (William's brother), Thomas Hobbes, Robert Payne and Walter Warner. This core \"academy\" group was disrupted when Newcastle took on responsibility for the Prince of Wales, in 1638. At a later point John Pell was in Newcastle's service.\nCharles Cavendish's circle included Henry Bond, Richard Reeve or Reeves the instrument-maker, John Twysden and John Wallis. He was a patron of William Oughtred.\nLiterature and the arts\nNewcastle in the 1630s became a major patron to Ben Jonson. His second wife was Margaret Cavendish, née Lucas, the writer. Newcastle was called \"our English Maecenas\" by Gerard Langbaine the Younger; he was a patron after the Restoration to both John Dryden and Thomas Shadwell. Other writers he supported included William Davenant, William Sampson, James Shirley and John Suckling. He bought sculptures by Francesco Fanelli for Welbeck.\nIn exile\nAs a consequence of the royalist defeat at the Battle of Marston Moor in 1644, Newcastle and some of his entourage went into exile. He returned to England only with the Restoration of 1660. Initially he went to Hamburg. By 1645 Newcastle was in Paris: his circle had contacts in Marin Mersenne and Claude Mydorge, whom Charles Cavendish had met in France at least 15 years earlier. In France Newcastle met and married that year Margaret Lucas who was with the exiled court of Queen Henrietta Maria. She studied with Charles Cavendish, and became a writer on natural philosophy, initially a proponent of atomism. Besides Hobbes, who joined them in Paris, the Cavendishes knew at this period René Descartes, Kenelm Digby, and Christiaan Huygens. Much of the latter part of their exile was spent at Antwerp; there, though in debt, they lived in the Rubenshuis. Other associations were with Walter Charleton who came to know Margaret Cavendish (not necessarily abroad, since she returned to England for a time), and William Brereton, 3rd Baron Brereton.\n",
    "source": "wikipedia",
    "title": "Welbeck Academy",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_81304225",
    "text": "Western esotericism and psychology surveys the documented exchanges between Western esotericism—including Westernized hybrids of Asian traditions—and selected areas of psychology, psychotherapy, and popular psychology. From the late eighteenth century onward, conduits such as animal magnetism and early hypnosis (reinterpreted from mesmeric “somnambulism”), Spiritualism/psychical research, and fin de siècle occultism and comparative projects created channels by which esoteric repertoires (e.g., alchemy, astrology, and subtle body schemes) were translated into psychological idioms or embedded in therapeutic and self-development techniques. In the twentieth century, these exchanges were variously articulated in analytical psychology (including Jung’s alchemical hermeneutics), humanistic workshop cultures and the human potential movement, transpersonal psychology, and symbolic counselling that repurposed oracular media (e.g., tarot, astrology, or the I Ching).\nRather than a single genealogy, historians emphasise plural processes of transmission, translation, and hybridisation across specific networks and publics—among them Theosophy (with codified chakras and subtle bodies), Anthroposophy (linking esoteric doctrines to pedagogical and para-clinical projects), the Eranos circle (mediating Jungian hermeneutics and history-of-religions), and late-modern markets often labelled “New Age”. Sociological accounts frame the broader diffusion via the late-modern “cultic milieu” and “occulture”, which describe how esoteric symbols and narratives circulate beyond formal religion through publishing, workshops, retreats and wellness/coaching niches, where psychologised self-work became a prominent vector of reception.\n\nScope and definitions\n\nHistoriography and frameworks\nModern scholarship generally treats Faivre’s componential definition as a heuristic grid rather than as an ontology for what esotericism “is”. In this perspective, the grid helps historians describe how symbolic mediations and techniques of transformation were translated into psychological idioms in specific periods and publics, while avoiding essentialist claims.\nPlaced within nineteenth- and twentieth-century contexts, Wouter J. Hanegraaff proposes an etic use of “occultism” for modern currents that explored interfaces between science, comparative religion and esoteric practice, and describes a two-way traffic between religious and psychological languages—“psychologization of religion” and “sacralization of psychology”—to account for the reception of alchemy, astrology and subtle-body maps in theory, therapy and self-work.\nFor diffusion beyond academic or ecclesiastical institutions, historians of contemporary religion draw on sociological models. The notion of the cultic milieu proposed by Colin Campbell designates an environment in which heterodox repertoires (e.g., astrology, trance, subtle bodies) persist, recirculate and recombine; Christopher Partridge’s “occulture” points to a cultural re",
    "source": "wikipedia",
    "title": "Western esotericism and psychology",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_81007565",
    "text": "The relationship between Western esotericism and science (and particularly the origins of experimental science) is a historiographical overview intersecting academic study of Western esotericism and history of science about how learned esoteric currents (e.g., natural magic, alchemy, hermeticism) interacted with natural philosophy, artisanal knowledge, and later scientific institutions from Antiquity to the twentieth century. It summarizes major debates (e.g., the “Yates thesis”), the role of printing and learned/artisanal networks, and the transformations that led from alchemy and chymistry to early modern chemistry, it also traces nineteenth–twentieth-century continuities in mesmerism, spiritualism, and psychical research.\n\nScope and definitions\nThe scope covers learned currents conventionally grouped under Western esotericism and their interactions with natural-philosophical, artisanal, and later scientific practices. In current scholarship, “Western esotericism” functions as an analytic label devised by historians of ideas rather than a stable emic category across periods. Within this remit fall astrology (including astral/astrological magic), alchemy/chymistry, hermetic and theurgic philosophies, “natural magic”, Christian Kabbalah and related Christianized appropriations, and selected nineteenth–twentieth-century continuities (e.g., mesmerism, spiritualisms, psychical research) insofar as they engaged scientific methods, publics, or institutions.\nFollowing standard usage, esotericism is treated as a family-resemblance category centered on literate, textually mediated, often elite discourses and practices, rather than a catch-all for folk religion or popular magic. Vernacular healing, charms, and “cunning” practices are distinguished from the theorized “occult sciences” of the medieval and early modern Latin worlds; points of contact—such as the diffusion of printed “books of secrets” to artisanal publics—are noted as channels of exchange.\nThe term “science” is used heuristically with attention to historical vocabulary. Up to the seventeenth century the principal comparandum is natural philosophy and adjacent artisanal or medical know-how; only gradually did experimental and mathematical cultures crystallize into formations recognizable as “science,” often discussed under the Scientific Revolution. Modern disciplinary boundaries were themselves constructed through demarcation and boundary-work within the sociology of scientific knowledge, differentiating legitimate inquiry from “occult” pursuits. Historically sensitive labels are used where helpful: chymistry for the mixed alchemical–chemical enterprise c. 1400–1700, and “natural magic” for learned techniques operating through hidden properties and sympathies (see sympathetic magic for the anthropological sense).\nFor definitional clarity, Western esotericism denotes a historically connected set of learned currents characterized—in varying constellations—by ideas of correspondences, a living ",
    "source": "wikipedia",
    "title": "Western esotericism and science",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_57537760",
    "text": "The William Phelps Ornithological Collection, also known as the Phelps Ornithological Museum, is a museum of natural sciences dedicated to the study, exhibition and preservation of the birds of Venezuela and the rest of Latin America. The collection is located east of Caracas and in the geographic center of Greater Caracas, in the heart of the Sabana Grande district. The William Phelps ornithological collection is the most important in Latin America and it is also the most important private collection in the world in its research area.\nIn this private museum one will find important Phelps family study books, as well as 8000 scientific volumes in the library, more than 83,000 anatomical specimens, more than 80,000 skins, etc.  For the year 1990, it was said that the William Phelps Ornithological Collection contained more than 76,300 skins and a small number of anatomical specimens, in the Gran Sabana Building of Sabana Grande. The Phelps library in 1990 already had 6,000 books, 800 journals and 5,500 reprints, mostly from natural sciences.\n\nHistory\nThe ornithological collection was born in 1938, although it did not have its own headquarters on the Boulevard of Sabana Grande until 1949. At the beginning of 2018, it celebrated its 80th anniversary in Caracas, Venezuela. With the passing of time, the collection has been growing and still has great international scientific relevance. In 2005, an investigation was carried out on \"plumage differences in four subspecies of golden warbler Basileuterus culicivorus in Venezuela\".\nSee more\nSabana Grande (Caracas)\nBoulevard of Sabana Grande\nWilliam Phelps\nEl Recreo Shopping Mall\nReferences\n\nExternal links\nhttps://fundacionwhphelps.org/\n",
    "source": "wikipedia",
    "title": "William Phelps Ornithological Collection",
    "topic": "History_of_science"
  },
  {
    "id": "wiki_9127632",
    "text": "Biology is the scientific study of life and living organisms. It is a broad natural science that encompasses a wide range of fields and unifying principles that explain the structure, function, growth, origin, evolution, and distribution of life. Central to biology are five fundamental themes: the cell as the basic unit of life, genes and heredity as the basis of inheritance, evolution as the driver of biological diversity, energy transformation for sustaining life processes, and the maintenance of internal stability (homeostasis).\nBiology examines life across multiple levels of organization, from molecules and cells to organisms, populations, and ecosystems. Subdisciplines include molecular biology, physiology, ecology, evolutionary biology, developmental biology, and systematics, among others. Each of these fields applies a range of methods to investigate biological phenomena, including observation, experimentation, and mathematical modeling. Modern biology is grounded in the theory of evolution by natural selection, first articulated by Charles Darwin, and in the molecular understanding of genes encoded in DNA. The discovery of the structure of DNA and advances in molecular genetics have transformed many areas of biology, leading to applications in medicine, agriculture, biotechnology, and environmental science.\nLife on Earth is believed to have originated over 3.7 billion years ago. Today, it includes a vast diversity of organisms—from single-celled archaea and bacteria to complex multicellular plants, fungi, and animals. Biologists classify organisms based on shared characteristics and evolutionary relationships, using taxonomic and phylogenetic frameworks. These organisms interact with each other and with their environments in ecosystems, where they play roles in energy flow and nutrient cycling. As a constantly evolving field, biology incorporates new discoveries and technologies that enhance the understanding of life and its processes, while contributing to solutions for challenges such as disease, climate change, and biodiversity loss.\n\nEtymology\nFrom Greek βίος (bíos) 'life', (from Proto-Indo-European root *gwei-, to live) and λογία (logia) 'study of'. The compound appears in the title of Volume 3 of Michael Christoph Hanow's Philosophiae naturalis sive physicae dogmaticae: Geologia, biologia, phytologia generalis et dendrologia, published in 1766. The term biology in its modern sense appears to have been introduced independently by Thomas Beddoes (in 1799), Karl Friedrich Burdach (in 1800), Gottfried Reinhold Treviranus (Biologie oder Philosophie der lebenden Natur, 1802) and Jean-Baptiste Lamarck (Hydrogéologie, 1802).\n",
    "source": "wikipedia",
    "title": "Biology",
    "topic": "Biology"
  },
  {
    "id": "wiki_2537522",
    "text": "A biologist is a scientist who conducts research in biology. Biologists are interested in studying life on Earth, whether it is an individual cell, a multicellular organism, or a community of interacting populations. They usually specialize in a particular branch (e.g., molecular biology, zoology, and evolutionary biology) of biology and have a specific research focus (e.g., studying malaria or cancer).\nBiologists who are involved in basic research have the aim of advancing knowledge about the natural world. They conduct their research using the scientific method, which is an empirical method for testing hypotheses. Their discoveries may have applications for some specific purpose such as in biotechnology, which has the goal of developing medically useful products for humans.\nIn modern times, most biologists have one or more academic degrees such as a bachelor's degree, as well as an advanced degree such as a master's degree or a doctorate. Like other scientists, biologists can be found working in different sectors of the economy such as in academia, nonprofits, private industry, or government.\n\nHistory\nFrancesco Redi, the founder of experimental biology, is recognized to be one of the greatest biologists of all time. Robert Hooke, an English natural philosopher, coined the term cell, suggesting plant structure's resemblance to honeycomb cells.\nCharles Darwin and Alfred Wallace independently formulated the theory of evolution by natural selection, which was described in detail in Darwin's book On the Origin of Species, which was published in 1859. In it, Darwin proposed that the features of all living things, including humans, were shaped by natural processes of descent with accumulated modification leading to divergence over long periods of time. The theory of evolution in its current form affects almost all areas of biology. Separately, Gregor Mendel formulated the principles of inheritance in 1866, which became the basis of modern genetics.\nIn 1953, James D. Watson and Francis Crick described the basic structure of DNA, the genetic material for expressing life in all its forms, building on the work of Maurice Wilkins and Rosalind Franklin, suggested that the structure of DNA was a double helix.\nIan Wilmut led a research group that in 1996 first cloned a mammal from an adult somatic cell, a Finnish Dorset lamb named Dolly.\n",
    "source": "wikipedia",
    "title": "Biologist",
    "topic": "Biology"
  },
  {
    "id": "wiki_44707607",
    "text": "This is a list of encyclopedias as well as encyclopedic and biographical dictionaries published on the subject of biology in any language.\n\nEntries are in the English language unless specifically stated as otherwise.\n\nGeneral biology\nBecher, Anne, Joseph Richey. American environmental leaders: From colonial times to the present. Grey House, 2008. ISBN 9781592371198.\nButcher, Russell D., Stephen E. Adair, Lynn A. Greenwalt. America's national wildlife refuges: A complete guide. Roberts Rinehart Publishers in cooperation with Ducks Unlimited, 2003. ISBN 1570983798.\nCullen, Katherine E. (2009). Encyclopedia of Life Science. Infobase Publishing. ISBN 978-0-8160-7008-4.\nEcological Internet, Inc. EcoEarth.info: Environment portal and search engine. Ecological Internet, Inc. [1].\nEncyclopedia of Life Sciences. John Wiley & Sons. 25 May 2007. ISBN 978-0-470-06651-5.\nEncyclopedia of Life Sciences. Groves Dictionaries Incorporated. 1 November 2001. ISBN 978-1-56159-238-8.\nFriday, Adrian & Davis S. Ingram. The Cambridge Encyclopedia of Life Sciences. Cambridge, 1985.\nGaither, Carl C., Alma E. Cavazos-Gaither, Andrew Slocombe. Naturally speaking: A dictionary of quotations on biology, botany, nature and zoology. Institute of Physics, 2001. ISBN 0750306815.\nGibson, Daniel, National Audubon Society. Audubon guide to the national wildlife refuges. Southwest: Arizona, Nevada, New Mexico, Texas. St. Martin's Griffin, 2000. ISBN 0312207778.\nGoudie, Andrew, David J. Cuff. Encyclopedia of global change: Environmental change and human society. Oxford University Press, 2002. ISBN 0195108256.\nGove, Doris. Audubon guide to the national wildlife refuges. Southeast : Alabama, Florida, Georgia, Kentucky, Mississippi, North Carolina, Puerto Rico, South Carolina, Tennessee, U.S. Virgin Islands. St. Martin's Griffin, 2000. ISBN 0312241283.\nGrassy, John. Audubon guide to the national wildlife refuges: Northern Midwest: Illinois, Indiana, Iowa, Michigan, Minnesota, Nebraska, North Dakota, Ohio, South Dakota, Wisconsin. St. Martin's Griffin, c2000. ISBN 0312243154.\nGrassy, John. Audubon guide to the national wildlife refuges: Rocky Mountains: Colorado, Idaho, Montana, Utah, Wyoming. St. Martin's Griffin, 2000. ISBN 0312245742.\nGray, Peter. Encyclopedia of the Biological Sciences. Krieger, 1981.\nGrinstein, Louise S., Carol A. Biermann, Rose K. Rose. Women in the biological sciences: A biobibliographic sourcebook. Greenwood Press, 1997. ISBN 0313291802.\nHancock, John M., Marketa J. Zvelebil. Dictionary of bioinformatics and computational biology. Wiley-Liss, 2004. ISBN 0471436224.\nHosansky, David. The environment A to Z. CQ Press, 2001. ISBN 1568025831.\nLaubach, René. Audubon guide to the national wildlife refuges. New England : Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont. St. Martin's Griffin, 2000. ISBN 0312204507.\nMac Arthur, Loren, Debbie S. Miller. Audubon guide to the national wildlife refuges. Alaska and the Northwest: Alaska, Oregon, Washington.",
    "source": "wikipedia",
    "title": "Bibliography of encyclopedias: biology",
    "topic": "Biology"
  },
  {
    "id": "wiki_55931886",
    "text": "A bioactive terrarium (or vivarium) is a terrarium for housing one or more terrestrial animal species that includes live plants and populations of small invertebrates and microorganisms to consume and break down the waste products of the primary species. In a functional bioactive terrarium, the waste products will be broken down by these detritivores, reducing or eliminating the need for cage cleaning. Bioactive vivariums are used by zoos and hobbyists to house reptiles and amphibians in an aesthetically pleasing and enriched environment.\n\nEnclosure\nAny terrarium can be made bioactive by addition of the appropriate substrate, plants, and detritivores. Bioactive enclosures are often maintained as display terraria constructed of PVC, wood, glass and/or acrylic. Bioactive enclosures in laboratory \"rack\" style caging are uncommon.\nCleanup crew\nWaste products of the primary species are consumed by a variety of detritivores, referred to as the \"cleanup crew\" by hobbyists.  These can include woodlice, springtails, earthworms, millipedes, and various beetles, with different species being preferred in different habitats - the cleanup crew for a tropical rainforest bioactive terrarium may rely primarily on springtails, isopods, and earthworms, while a desert habitat might use beetles.  If the primary species is insectivorous, they may consume the cleanup crew, and thus the cleanup crew must have sufficient retreats to avoid being completely depopulated.\nAdditionally, bioactive terraria typically have a flourishing population of bacteria and other microorganisms which break down the wastes of the cleanup crew and primary species.  Fungi may occur as part of the terrarium cycle and will be consumed by the cleanup crew.\nSubstrate\nBioactive enclosures require some form of substrate to grow plants and to provide habitat for the cleanup crew.  The choice of substrate is typically determined by the habitat of the primary species (e.g. jungle vs desert), and created by mixing a variety of components such as organic topsoil (free of pesticides and non-biological fertilizers), peat, coco fiber, sand, long-fiber sphagnum moss, cypress mulch, and orchid bark in varying proportions.  \nIn wet habitats, there is typically a drainage layer beneath the substrate to allow water to pool without saturating the substrate. The drainage layer may be constructed via coarse gravel, stones, expanded clay aggregate, or may be wholly synthetic; the drainage layer is typically separated from the overlying substrate with a fine plastic mesh.  Additionally, some bioactive terraria include leaf-litter, which can serve as food and microhabitat for the cleanup crew.\n",
    "source": "wikipedia",
    "title": "Bioactive terrarium",
    "topic": "Biology"
  },
  {
    "id": "wiki_78359137",
    "text": "Bioliteracy is the ability to understand and engage with biological topics. The concept is used particularly in the contexts of biotechnology and biodiversity.\n\nDescription\nIn the biotechnology context, bioliteracy is considered important for promoting the biotechnology industry and the development of biological engineering products. It has also been defined as \"the concept of imbuing people, personnel, or teams with an understanding of and comfort with biology and biotechnology.\" The use in the context of biodiversity is somewhat distinct, focusing on improving awareness of different organisms with the goal of conservation.\nCitizen science initiatives, such as iNaturalist, are considered effective ways to increase bioliteracy, engaging students with the direct observation of nature.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Bioliteracy",
    "topic": "Biology"
  },
  {
    "id": "wiki_6920635",
    "text": "Biological constraints are factors which make populations resistant to evolutionary change. One proposed definition of constraint is \"A property of a trait that, although possibly adaptive in the environment in which it originally evolved, acts to place limits on the production of new phenotypic variants.\"  Constraint has played an important role in the development of such ideas as homology and body plans.\n\nTypes of constraint\nAny aspect of an organism that has not changed over a certain period of time could be considered to provide evidence for \"constraint\" of some sort. To make the concept more useful, it is therefore necessary to divide it into smaller units. First, one can consider the pattern of constraint as evidenced by phylogenetic analysis and the use of phylogenetic comparative methods; this is often termed phylogenetic inertia, or phylogenetic constraint. It refers to the tendency of related taxa sharing traits based on phylogeny.  Charles Darwin spoke of this concept in his 1859 book \"On the Origin of Species\", as being \"Unity of Type\" and went on to explain the phenomenon as existing because organisms do not start over from scratch, but have characteristics that are built upon already existing ones that were inherited from their ancestors; and these characteristics likely limit the amount of evolution seen in that new taxa due to these constraints. \nIf one sees particular features of organisms that have not changed over rather long periods of time (many generations), then this could suggest some constraint on their ability to change (evolve). However, it is not clear that mere documentation of lack of change in a particular character is good evidence for constraint in the sense of the character being unable to change. For example, long-term stabilizing selection related to stable environments might cause stasis.  It has often been considered more fruitful, to consider constraint in its causal sense: what are the causes of lack of change?\nRelationships of constraint classes\nAlthough they are separate, the types of constraints discussed are nevertheless relatable to each other. In particular, stabilizing selection, mechanical, and physical constraints might lead through time to developmental integration and canalisation. However, without any clear idea of any of these mechanisms, deducing them from mere patterns of stasis as deduced from phylogenetic patterns or the fossil record remains problematic.  In addition, the terminology used to describe constraints has led to confusion.\n",
    "source": "wikipedia",
    "title": "Biological constraints",
    "topic": "Biology"
  },
  {
    "id": "wiki_4318932",
    "text": "The biology of romantic love has been explored by such biological sciences as evolutionary psychology, evolutionary biology, anthropology and neuroscience. Neurochemicals and hormones such as dopamine and oxytocin are studied along with a variety of interrelated brain systems which produce the psychological experience and behaviors of romantic love.\nThe study of romantic love is still in its infancy. As of 2021, there were a total of 42 biological studies on romantic love.\n\nDefinition of romantic love\nThe meaning of the term \"romantic love\" has changed considerably throughout history, making it difficult to simply define. Initially it was coined to refer to certain attitudes and behaviors described in a body of literature now referred to as courtly love. However, academic psychology and especially biology also consider romantic love in a different sense, which refers to a brain system (or systems) related to pair bonding or mating with associated psychological properties.\nBode and Kushnick undertook a comprehensive review of romantic love from a biological perspective in 2021. They considered the psychology of romantic love, its mechanisms, development across the lifespan, functions, and evolutionary history. Based on the content of that review, they proposed a biological definition of romantic love:\n\nRomantic love is a motivational state typically associated with a desire for long-term mating with a particular individual. It occurs across the lifespan and is associated with distinctive cognitive, emotional, behavioral, social, genetic, neural, and endocrine activity in both sexes. Throughout much of the life course, it serves mate choice, courtship, sex, and pair-bonding functions. It is a suite of adaptations and by-products that arose sometime during the recent evolutionary history of humans.\nRomantic love in this sense is also not necessarily \"dyadic\", \"social\" or \"interpersonal\", despite being related to pair bonding. Romantic love can be experienced outside the context of a relationship, for example in the case of unrequited love where the feelings are not reciprocated. A person can develop romantic love feelings before any relationship has occurred, for only a potential partner. The potential partner can even be somebody they do not know well or are not acquainted with at all, as in cases of love at first sight and parasocial attachments.\nThe early stage of romantic love (which has obsessive and addictive features) might also be referred to as being \"in love\", passionate love, infatuation, limerence or obsessive love. Research has never settled on a unified terminology or set of methods. Distinctions are drawn between this early stage of romantic love and the \"attachment system\" theorized by the attachment theorists like John Bowlby. In the past, attachment theorists have argued that attachment theory and attachment styles can replace other theories of love, but academics on love have argued this is incorrect and that romantic love and att",
    "source": "wikipedia",
    "title": "Biology of romantic love",
    "topic": "Biology"
  },
  {
    "id": "wiki_43317198",
    "text": "Biospeleology, also known as cave biology, is a branch of biology dedicated to the study of organisms that live in caves and are collectively referred to as troglofauna.\n\nBiospeleology as a science\n\nNotable biospeleologists\nEmil Racoviță, co-founder of biospeleology, the first Romanian to go to the arctic, collected the type specimens of the flightless midge Belgica antarctica\nCarl Eigenmann, is credited with identifying and describing for the first time 195 genera containing nearly 600 species of fishes of North America and South America\nLouis Fage, was a founding member of the Commission de spéléologie\nRené Jeannel, co-founder of biospeleology\nCurt Kosswig, is known as the Father of Turkish Zoology.\nBoris Sket, Approximately 35 animal species are named sketi, and three genera.\nEndre Dudich, founder and committee member of the Hungarian Speleological Society in 1926.\nKarel Absolon, though more famous for his archaeological and speleological discoveries, Absolon started his career out as a biospeleologist.\nBibliography\nBernard Collignon, Caving, scientific approaches., Edisud 1988\nFabien Steak, Approach biospéologie, File EFS Instruction No. 116, 1st Edition, 1997\nC. Delamare-Debouteville, Life in caves, PUF, Que sais-je?, Paris 1971\nBernard Gèze, Scientific caving, Seuil, Paris, 1965, p. 137-167\nR. and V. Decou Ginet, Introduction to biology and groundwater ecology, University Publishing Delarge 1977\nRené Jeannel, Animal cave in France, Lechevalier, Paris, 1926\nRené Jeannel, Living fossils caves, Gallimard, Paris, 1943\nEdward Alfred Martel, Groundwater evolution, Flammarion, Paris, 1908, p. 242-289\nGeorges Émile Racovitza, Essay on biospéologiques problems, I Biospeologica 1907\nMichel Siffre, Animals sinkholes and caves, Hachette, 1979\nMichel Siffre, France The caves and caverns, ed. Privat, 1999, p. 136-153\nG. and R. Thines Tercafs, Atlas of the underground life: the cave animals, Boubée (Paris) and De Visscher (Brussels), 1972\nAlbert Vandel Biospéologie: the biology of cave animals, Gauthier-Villars, Paris, 1964\nArmand Vire, The subterranean fauna of France, Paris, 1900\n",
    "source": "wikipedia",
    "title": "Biospeleology",
    "topic": "Biology"
  },
  {
    "id": "wiki_78136945",
    "text": "The cancer exodus hypothesis establishes that circulating tumor cell clusters (CTC clusters) maintain their multicellular structure throughout the metastatic process. It was previously thought that these clusters must dissociate into single cells during metastasis. According to the hypothesis, CTC clusters intravasate (enter the bloodstream), travel through circulation as a cohesive unit, and extravasate (exit the bloodstream) at distant sites without disaggregating, significantly enhancing their metastatic potential. This concept is considered a key advancement in understanding of cancer biology and CTCs role in cancer metastasis.\n\nMechanism\nTraditionally, it was believed that CTC clusters needed to dissociate into individual cells during their journey through the bloodstream to seed secondary tumors. However, recent studies show that CTC clusters can travel through the bloodstream intact, enabling them to perform every step of metastasis while maintaining their group/cluster structure. \nThe cancer exodus hypothesis asserts that CTC clusters have several distinct advantages that increase their metastatic potential:\n\nHigher metastatic efficiency: CTC clusters have been shown to possess superior seeding capabilities at distant sites compared to single CTCs.\nSurvival and proliferation: The collective nature of CTC clusters allows them to share resources and offer intercellular support, improving their overall survival rates in the bloodstream.\nResistance to treatment: CTC clusters exhibit unique gene expression profiles that contribute to their ability to evade certain cancer therapies, making them more resistant than individual tumor cells.\nClinical relevance\nThe cancer exodus hypothesis offers important insights into how metastasis occurs and highlights the significance of CTC clusters in cancer progression. Detecting and analyzing CTC clusters through liquid biopsies could offer valuable information about the aggressiveness and metastatic potential of cancers. This information is particularly useful for identifying patients who may benefit from more aggressive treatment strategies.\n",
    "source": "wikipedia",
    "title": "Cancer exodus hypothesis",
    "topic": "Biology"
  },
  {
    "id": "wiki_38495892",
    "text": "Chlororespiration is a respiratory process that is thought to occur in plant chloroplasts, involving the electron transport chain (ETC) in the thylakoid membrane. It is thought to involve NAD(P)H dehydrogenase (NDH) and plastid terminal oxidase (PTOX/IMMUTANS), forming an ETC utilizing molecular oxygen as the electron acceptor. This process also interacts with the ETC in the mitochondrion where respiration takes place, as well as with photosynthesis. If photosynthesis is inhibited by environmental stressors like water deficiency, increased heat, and/or increased/decreased light exposure, or even chilling stress, then chlororespiration is one of the crucial ways that plants use to compensate for chemical energy synthesis.\n\nChlororespiration – the latest model\nInitially, the presence of chlororespiration as a legitimate respiratory process in plants was heavily doubted. However, experimentation on Chlamydomonas reinhardtii discovered plastoquinone (PQ) to be a redox carrier. The role of this redox carrier is to transport electrons from the NAD(P)H enzyme to an oxidase enzyme on the thylakoid membrane. Using this cyclic electron chain around photosystem I (PSI), chlororespiration compensates for the lack of light. This cyclic pathway also allows electrons to re-enter the PQ pool through NDH, which is then used to supply ATP to plant cells.\n\nIn the year 2002, the discovery of the molecules: plastid terminal oxidase (PTOX) and NDH complexes, have revolutionised the concept of chlororespiration. Using evidence from experimentation on a Meillandina rose, this latest model observes the role of PTOX to be an enzyme that prevents the PQ pool from over-reducing, by stimulating its reoxidation. Whereas, the NDH complexes are responsible for providing a gateway for electrons to form an ETC. The presence of such molecules are apparent in the non-appressed thylakoid membranes of higher plants like Meillandina roses.\nThe relation between chlororespiration, photosynthesis and respiration\nExperimentation with respiratory oxidase inhibitors (for instance, cyanide) on unicellular algae has revealed interactive pathways to be present between chloroplasts and mitochondria. Metabolic pathways responsible for photosynthesis are present in chloroplasts, whereas respiratory metabolic pathways are present in mitochondria. In these pathways, metabolic carriers (like phosphate) exchange NAD(P)H molecules between photosynthetic and respiratory ETCs. Evidence using mass spectrometry on algae and photosynthetic mutants of Chlamydomonas discovered that oxygen molecules were also being exchanged between photosynthetic and chlororespiratory ETCs. The mutant Chlamydomonas alga species lacks photosystems I and II (PSI and PSII), so when the alga underwent flash-induced PSI activity, it resulted in no effect on mitochondrial pathways of respiration. Instead, this flash-induced PSI activity caused an exchange between photosynthetic and chlororespiratory ETCs, which was observed using ",
    "source": "wikipedia",
    "title": "Chlororespiration",
    "topic": "Biology"
  },
  {
    "id": "wiki_76151489",
    "text": "A dermestarium (pl. dermestaria) is a room, container, or terrarium where taxidermists let invertebrates - typically beetle larvae - remove (eat) flesh and other soft parts from animal carcasses or parts of animal carcasses, such as skulls. The purpose is to expose and clean individual bones or entire skeletons for study and preservation. The word dermestarium derives from Dermestes, the genus name of the beetle most commonly used for the task. Other invertebrates, notably isopods and clothes moths, have also been used.\nDermestaria are used by museums and other research institutions. Dermestaria are sometimes located in separate, bespoke buildings, as is the case at the University of Wisconsin-Madison's zoological museum. Other dermestaria are of the container or terrarium type and can be kept inside regular research and museum facilities. The use of dermestaria in the USA goes back to at least the 1940s.\nPrior to dermestarium deposition, the animal carcass is skinned, and excess flesh and other soft parts are removed to minimize the dermestarium time needed. The carcass is left to dry somewhat, since Dermestes larvae prefer dry over moist meat. The carcass is then put on a tray and placed inside the dermestarium, which has been prepared with a colony of Dermestes larvae. The dermestarium time needed for the larvae to strip the bones clean of flesh and soft parts ranges from a few days for small animals to several years for very large animals; once the bones have been stripped clean of all soft parts, the tray with the carcass is removed and placed in a freezer for a period sufficient to kill off any larvae and adult beetles that happened to tag along.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Dermestarium",
    "topic": "Biology"
  },
  {
    "id": "wiki_790808",
    "text": "Endogeny, in biology, refers to the property of originating or developing from within an organism, tissue, or cell. \nFor example, endogenous substances, and endogenous processes are those that originate within a living system (e.g. an organism or a cell). For instance, estradiol is an endogenous estrogen hormone produced within the body, whereas ethinylestradiol is an exogenous synthetic estrogen, commonly used in birth control pills.\nIn contrast, exogenous substances and exogenous processes are those that originate from outside of an organism.\n\nReferences\n\nExternal links\n The dictionary definition of endogeny at Wiktionary\n",
    "source": "wikipedia",
    "title": "Endogeny (biology)",
    "topic": "Biology"
  },
  {
    "id": "wiki_56880369",
    "text": "Erinacin is a antihaemorrhagic protein first isolated in 1996 from the European Hedgehog, which is thought to convey (partial) immunity to snake venom.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Erinacin",
    "topic": "Biology"
  },
  {
    "id": "wiki_255468",
    "text": "Excretion is elimination of metabolic waste, which is an essential process in all organisms. In vertebrates, this is primarily carried out by the lungs, kidneys, and skin. This is in contrast with secretion, where the substance may have specific tasks after leaving the cell. For example, placental mammals expel urine from the bladder through the urethra, which is part of the excretory system. Unicellular organisms discharge waste products directly through the surface of the cell. Another example would be how mammals release solid waste (feces) through the anus during defecation.\nDuring activities such as cellular respiration, several chemical reactions take place in the body. These are known as metabolism. These chemical reactions produce waste products such as carbon dioxide, water, salts, urea and uric acid. Accumulation of these wastes beyond a level inside the body is harmful to the body. The excretory organs remove these wastes. This process of removal of metabolic waste from the body is known as excretion.\n\nProcesses across various types of life\n\nSee also\n\nReferences\n\nExternal links\n\nUAlberta.ca, Animation of excretion\nBrian J Ford on leaf fall in Nature\n",
    "source": "wikipedia",
    "title": "Excretion",
    "topic": "Biology"
  },
  {
    "id": "wiki_79707214",
    "text": "The concept of functional information is an attempt to rigorously define the information content of biological systems. The concept was originated by a group led by Jack W. Szostak in 2003.\n\nDefinition\nThey define functional information as follows:\n\nthe concept of degree of function is introduced, where the degree of function \n  \n    \n      \n        \n          E\n          \n            x\n          \n        \n      \n    \n    {\\displaystyle E_{x}}\n  \n is a non-negative objective measure of the capability of system \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n to do the physical function \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n.\nthe fraction of possible configurations of the system that can achieve at least a particular level of function \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n in regard to the physical function \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n is defined to be \n  \n    \n      \n        F\n        (\n        \n          E\n          \n            x\n          \n        \n        ≥\n        θ\n        )\n      \n    \n    {\\displaystyle F(E_{x}\\geq \\theta )}\n  \n\nthe functional information relative to a given level of function \n  \n    \n      \n        \n          E\n          \n            x\n          \n        \n        =\n        θ\n      \n    \n    {\\displaystyle E_{x}=\\theta }\n  \n is defined as \n  \n    \n      \n        I\n        (\n        \n          E\n          \n            x\n          \n        \n        ≥\n        θ\n        )\n        =\n        −\n        l\n        o\n        \n          g\n          \n            2\n          \n        \n        F\n        (\n        \n          E\n          \n            x\n          \n        \n        ≥\n        θ\n        )\n      \n    \n    {\\displaystyle I(E_{x}\\geq \\theta )=-log_{2}F(E_{x}\\geq \\theta )}\n  \n\nThis leads to two conclusions:\n\nbecause all possible configurations can achieve zero or more functionality, that is to say \n  \n    \n      \n        F\n        (\n        \n          E\n          \n            x\n          \n        \n        ≥\n        0\n        )\n        =\n        1\n      \n    \n    {\\displaystyle F(E_{x}\\geq 0)=1}\n  \n, the minimum possible functional information for a system is \n  \n    \n      \n        −\n        l\n        o\n        \n          g\n          \n            2\n          \n        \n        1\n      \n    \n    {\\displaystyle -log_{2}1}\n  \n, which is zero.\nfor the highest possible level of a degree of function of a system \n  \n    \n      \n        \n          E\n          \n            x\n          \n        \n        =\n        \n          θ\n          \n            m\n            a\n            x\n          \n        \n      \n    \n    {\\displaystyle E_{x}=\\theta _{max}}\n  \n, there will be a well defined \n  \n    \n      \n        I\n        (\n        \n          E\n          \n            x\n          \n        \n        =\n        \n          θ\n          \n            m\n            a\n            x\n          \n        \n        )\n        =\n        −\n        l\n        o\n  ",
    "source": "wikipedia",
    "title": "Functional information",
    "topic": "Biology"
  },
  {
    "id": "wiki_18101603",
    "text": "High throughput biology (or high throughput cell biology) is the use of automation equipment with classical cell biology techniques to address biological questions that are  otherwise unattainable using conventional methods. It may incorporate techniques from optics, chemistry, biology or image analysis to permit rapid, highly parallel research into how cells function, interact with each other and how pathogens exploit them in disease.\nHigh throughput cell biology has many definitions, but is most commonly defined by the search for active compounds in natural materials like in medicinal plants. This is also known as high throughput screening (HTS) and is how most drug discoveries are made today, many cancer drugs, antibiotics, or viral antagonists have been discovered using HTS. The process of HTS also tests substances for potentially harmful chemicals that could be potential human health risks. HTS generally involves hundreds of samples of cells with the model disease and hundreds of different compounds being tested from a specific source. Most often a computer is used to determine when a compound of interest has a desired or interesting effect on the cell samples.\nUsing this method has contributed to the discovery of the drug Sorafenib (Nexavar). Sorafenib is used as medication to treat multiple types of cancers, including renal cell carcinoma (RCC, cancer in the kidneys), hepatocellular carcinoma (liver cancer), and thyroid cancer. It helps stop cancer cells from reproducing by blocking the abnormal proteins present. In 1994, high throughput screening for this particular drug was completed. It was initially discovered by Bayer Pharmaceuticals in 2001. By using a RAF kinase biochemical assay, 200,000 compounds were screened from medicinal chemistry directed synthesis or combinatorial libraries to identify active molecules against activeRAF kinase. Following three trials of testing, it was found to have anti-angiogenic effects on the cancers, which stops the process of creating new blood vessels in the body.\nAnother discovery made using HTS is Maraviroc. It is an HIV entry inhibitor, and slows the process and prevents HIV from being able to enter human cells. It is used to treat a variety of cancers as well, reducing or blocking the metastasis of cancer cells, which is when cancer cells spread to a completely different part of the body from where it started. High throughput screening for Maraviroc was completed in 1997, and finalized in 2005 by Pfizer global research and development team.\nHigh-throughput biology serves as one facet of what has also been called \"omics research\" - the interface between large scale biology (genome, proteome, transcriptome), technology and researchers. High throughput cell biology has a definite focus on the cell, and methods accessing the cell such as imaging, gene expression microarrays, or genome wide screening. The basic idea is to take methods normally performed on their own and do a very large number of them w",
    "source": "wikipedia",
    "title": "High throughput biology",
    "topic": "Biology"
  },
  {
    "id": "wiki_75185459",
    "text": "Interdigitation is the interlinking of biological components that resembles the fingers of two hands being locked together. It can be a naturally occurring or man-made state.\n\nExamples\nNaturally occurring interdigitation includes skull sutures that develop during periods of brain growth, and which remain thin and straight, and later develop complex fractal interdigitations that provide interlocking strength. A layer of the retina where photoreception occurs is called the interdigitation zone. Adhesion or diffusive bonding occurs when sections of polymer chains from one surface interdigitate with those of an adjacent surface. In the dermis, dermal papillae (DP) (singular papilla, diminutive of Latin papula, 'pimple') are small, nipple-like extensions  of the dermis into the epidermis, also known as interdigitations. The distal convoluted tubule (DCT), a portion of kidney nephron, can be recognized by several distinct features, including lateral membrane interdigitations with neighboring cells.\nSome hypotheses contend that crown shyness, the interdigitation of canopy branches, leads to \"reciprocal pruning\" of adjacent trees.\nInterdigitation is also found in biological research. Interdigitation fusion is a method of preparing calcium- and phosphate-loaded liposomes. Drugs inserted in the bilayer biomembrane may influence the lateral organization of the lipid membrane, with interdigitation of the membrane to fill volume voids. A similar interdigitation process involves investigating dissipative particle dynamics (DPD) simulations by adding alcohol molecules to the bilayers of double-tail lipids. Pressure-induced interdigitation is used to study hydrostatic pressure of bicellular dispersions containing anionic lipids.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Interdigitation",
    "topic": "Biology"
  },
  {
    "id": "wiki_78141628",
    "text": "Plasmagene is a term used to describe genetic elements that exist outside of the nucleus, typically within the cytoplasm of a cell. These elements are usually associated with organelles like mitochondria and chloroplasts, which contain their own genetic material and replicate independently of the nuclear genome. Plasmagene theory as proposed by Tracy Sonneborn has significantly contributed to the understanding of non-Mendelian inheritance patterns, where traits are passed through cytoplasmic inheritance rather than through nuclear DNA.\n\nFunction and characteristics\nPlasmagenes play crucial roles in various cellular processes, especially those involved in energy production. For instance, mitochondrial plasmagenes are integral to oxidative phosphorylation, the process responsible for generating most of the cell's ATP, the main energy currency of cells. Though they can replicate independently, plasmagenes are often semi-autonomous, as they rely on nuclear genes for many essential proteins that support their functions.\nInheritance and implications\nOne of the most noteworthy aspects of plasmagenes is their involvement in non-Mendelian inheritance patterns. Unlike nuclear genes, which are inherited from both parents, plasmagenes are typically inherited maternally. This occurs because cytoplasmic organelles, like mitochondria, are transferred primarily through the egg cell during fertilization. Consequently, mutations or abnormalities in plasmagenes are linked to various inherited disorders, particularly those affecting muscular and neurological systems.\nHistorical context and research\nResearch on plasmagenes dates back to the mid-20th century, focusing on their role in extranuclear inheritance and its implications for genetic diseases. These studies have been instrumental in elucidating how genetic information can be passed outside the nuclear DNA, altering the understanding of inheritance patterns and disease transmission. The plasmagene theory was later disproved, and in 1976 Sonneborn affirmed this. But later researches after Sonneborn's death, provided validation allowing continued studies.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Plasmagene",
    "topic": "Biology"
  },
  {
    "id": "wiki_78237247",
    "text": "Poison exons (PEs); also called premature termination codon (PTC) exons or nonsense-mediated decay (NMD) exons] are a class of cassette exons that contain PTCs. Inclusion of a PE in a transcript targets the transcript for degradation via NMD. PEs are generally highly conserved elements of the genome and are thought to have important regulatory roles in biology. Targeting PE inclusion or exclusion in certain transcripts is being evaluated as a therapeutic strategy.\n\nDiscovery\nIn 2002, a model termed regulated unproductive splicing and translation (RUST) was proposed based on the finding that many (~one-third) alternatively spliced transcripts contain PEs. In this model, coupling alternative splicing to NMD (AS-NMD) is thought to tune transcript levels to regulate protein expression. Alternative splicing may also lead to NMD via other pathways besides PE inclusion, e.g., intron retention.\nPEs were initially characterized in RNA-binding proteins from the SR protein family. Genes for other RNA-binding proteins (RBPs) such as those for heterogenous nuclear ribonucleoprotein (hnRNP) also contain PEs. Numerous chromatin regulators also contain PEs, though these are less conserved than PEs within RBPs such as the SR proteins. Multiple spliceosomal components contain PEs. Certain PEs may occur only in specific tissues.\nPE-containing transcripts generally represent a minority of the overall transcript population, in part due to their active degradation via NMD, though this relative abundance can be elevated upon inhibition of NMD or certain biological states. Certain PE-containing transcripts are resistant to NMD and may be translated into truncated proteins.\nRegulation\nCis-regulatory elements neighboring PEs have been found to affect PE inclusion.\nMany proteins whose corresponding genes contain PEs autoregulate PE inclusion in their respective transcripts and thereby control their own levels via a feedback loop. Cross-regulation of PE inclusion has also been observed.\nDifferential splicing of PEs is implicated in biological processes such as differentiation, neurodevelopment, dispersal of nuclear speckles during hypoxia, tumorigenesis, organism growth, and T cell expansion.\nProtein kinases that regulate phosphorylation of splicing factors can affect splicing processes, thus kinase inhibitors may affect inclusion of PEs. For example, CMGC kinase inhibitors and CDK9 inhibitors have been found to induce PE inclusion in RBM39.\nSmall molecules that modulate chromatin accessibility can affect PE inclusion.\nMutations in splicing factors can lead to inclusion of PEs in certain transcripts.\nPE inclusion can be regulated by external variables such as temperature and electrical activity. For example, PE inclusion in RBM3 transcript is lowered during hypothermia. This is mediated by temperature-dependent binding of the splicing factor HNRNPH1 to the RBM3 transcript. The neuronal RBPs NOVA1/2 are translocated from the nucleus to the cytoplasm during pilocarpine-induce",
    "source": "wikipedia",
    "title": "Poison exon",
    "topic": "Biology"
  },
  {
    "id": "wiki_78277844",
    "text": "The term polylecty or generalist is used in pollination ecology to refer to bees that collect pollen from a range of unrelated plants.  Honey bees exemplify this behavior,  collecting nectar from a wide array of flowers. Other predominantly polylectic genera include Bombus, Ceratina, Heriades and Halictus. The opposite term is oligolecty, and refers to bees that exhibit a narrow, specialized preference for pollen sources, typically to a single family or genus of flowering plants. \nRoughly two-thirds of bee species in Europe are polylectic, relying on a diverse array of pollen sources. This broad foraging approach allows these generalist bees to thrive in various environments and quickly adjust to shifting conditions. However, despite their adaptability, they are less efficient pollen gatherers than oligolectic bees, whose  morphology is often specialized for more effective pollen collection from a narrower range of plants.\nA species that exhibits polylecty is known as polylectic. This term should not be confused with Polylectic as a grammatical term, which has a similar etymology to the biological definition but instead refers to the adjective of a multi-word term, as opposed to a monolectic which is the adjective for a one-word term.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Polylecty",
    "topic": "Biology"
  },
  {
    "id": "wiki_78142714",
    "text": "Spatial biology is the study of biomolecules and cells in their native three-dimensional context. Spatial biology encompasses different levels of cellular resolution including (1) subcellular localization of DNA, RNA, and proteins, (2) single-cell resolution and in situ communications like cell-cell interactions and cell signaling, (3) cellular neighborhoods, regions, or microenvironments, and (4) tissue architecture and organization in organs. Dysregulation of tissue organization is a common feature in human disease progression including tumorigenesis and neurodegeneration. Many fields within biology are studied for their individual contribution to spatial biology.\n\nSpatial transcriptomics\nSpatial transcriptomics measures mRNA transcript abundance and distribution in situ across a tissue. Spatial method for RNA in situ detection is first described in a 1969 landmark paper by Joseph G. Gall and Mary-Lou Pardue. Previous to spatial transcriptomics techniques, whole transcriptome profiling lacked spatial context because tissues were ground up in bulk RNA-seq or dissociated into single cells suspensions in single cell RNA-seq. Although some literature refers to \"spatial genomics\" for RNA, growing consensus has settled on usage of \"spatial transcriptomics\" or \"spatially resolved transcriptomics.\"\nSpatial proteomics\nSpatial proteomics measures the localization and abundance of proteins at the subcellular level across a tissue. Immunohistochemistry-based spatial proteomic methods include oligo barcoded antibodies, cyclic immunofluorescence (cycIF), co-detection by indexing (CODEX), iterative bleaching extends multiplicity (IBEX), multiplexed ion beam imaging (MIBI) and imaging mass cytometry (IMC). Other methods includes deep visual proteomics that profile protein expression in single cells by laser capture microdissection and mass spectroscopy. The term \"spatial medicine\" is recently coined by Eric Topol to refer to a study that used deep visual proteomics to find a therapeutic treatment for patients with a rare skin condition.\n",
    "source": "wikipedia",
    "title": "Spatial biology",
    "topic": "Biology"
  },
  {
    "id": "wiki_44057611",
    "text": "Tokogeny or tocogeny is the biological relationship between parent and offspring, or more generally between ancestors and descendants.  In contradistinction to phylogeny it applies to individual organisms as opposed to species.\nIn the tokogentic system shared characteristics are called traits.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Tokogeny",
    "topic": "Biology"
  },
  {
    "id": "wiki_20433613",
    "text": "The universality–diversity paradigm (UDP) is the analysis of biological materials based on the universality and diversity of its fundamental structural elements and functional mechanisms. The analysis of biological systems based on this classification has been a cornerstone of modern biology.\n\nExample: proteins\nFor example, proteins constitute the elementary building blocks of a vast variety of biological materials such as cells, spider silk or bone, where they create extremely robust, multi-functional materials by self-organization of structures over many length- and time scales, from nano to macro. Some of the structural features are commonly found in many different tissues, that is, they are highly conserved. Examples of such universal building blocks include alpha-helices, beta-sheets or tropocollagen molecules. In contrast, other features are highly specific to tissue types, such as particular filament assemblies, beta-sheet nanocrystals in spider silk or tendon fascicles. This coexistence of universality and diversity is an overarching feature in biological materials and a crucial component of materiomics. It might provide guidelines for bioinspired and biomimetic material development, where this concept is translated into the use of inorganic or hybrid organic-inorganic building blocks.\nSee also\nBionics\nMateriomics\nNanotechnology\nPhylogenetics\nReferences\nAckbarow, Theodor; Buehler, Markus J. (2008-07-01). \"Hierarchical Coexistence of Universality and Diversity Controls Robustness and Multi-Functionality in Protein Materials\". Journal of Computational and Theoretical Nanoscience. 5 (7). American Scientific Publishers: 1193–1204. Bibcode:2008JCTN....5.1193A. doi:10.1166/jctn.2008.2554. ISSN 1546-1955.\n[1] Going nature one better (MIT News Release, October 22, 2010).\n[2] S. Cranford, M. Buehler, Materiomics: biological protein materials, from nano to macro, Nanotechnology, Science and Applications, Vol. 3, pp. 127–148, 2010.\nBuehler, Markus J. (2010). \"Tu(r)ning weakness to strength\". Nano Today. 5 (5). Elsevier BV: 379–383. doi:10.1016/j.nantod.2010.08.001. ISSN 1748-0132.\n[3] S. Cranford, M.J. Buehler, Biomateriomics, 2012 (Springer, New York).\n",
    "source": "wikipedia",
    "title": "Universality–diversity paradigm",
    "topic": "Biology"
  },
  {
    "id": "wiki_22939",
    "text": "Physics is the scientific study of matter, its fundamental constituents, its motion and behavior through space and time, and the related entities of energy and force. It is one of the most fundamental scientific disciplines. A scientist who specializes in the field of physics is called a physicist.\nPhysics is one of the oldest academic disciplines. Over much of the past two millennia, physics, chemistry, biology, and certain branches of mathematics were a part of natural philosophy, but during the Scientific Revolution in the 17th century, these natural sciences branched into separate research endeavors. Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms studied by other sciences and suggest new avenues of research in these and other academic disciplines such as mathematics and philosophy.\nAdvances in physics often enable new technologies. For example, advances in the understanding of electromagnetism, solid-state physics, and nuclear physics led directly to the development of technologies that have transformed modern society, such as television, computers, domestic appliances, and nuclear weapons; advances in thermodynamics led to the development of industrialization; and advances in mechanics inspired the development of calculus.\n\nHistory\nThe word physics comes from the Latin physica ('study of nature'), which itself is a borrowing of the Greek φυσική (phusikḗ 'natural science'), a term derived from φύσις (phúsis 'origin, nature, property').\nCore theories\nPhysics deals with a wide variety of systems, although certain theories are used by all physicists. Each of these theories was experimentally tested numerous times and found to be an adequate approximation of nature.\nThese central theories are important tools for research into more specialized topics, and any physicist, regardless of their specialization, is expected to be literate in them. These include classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, and special relativity.\n",
    "source": "wikipedia",
    "title": "Physics",
    "topic": "Physics"
  },
  {
    "id": "wiki_80358385",
    "text": "The Atominstitute (German: Atominstitut) is an Austrian University research facility with its own inhouse nuclear reactor located in Vienna. The institute most known member is 2022 Nobel laureate Anton Zeilinger.\nAdditional to the academic activities, the inspectors of the International Atomic Energy Agency (IAEA), headquartered nearby in Vienna, use the reactor for training and education, before being sent to deployments sites worldwide. These nuclear facilities include those in warzones like the Nuclear program of Iran, and nuclear power stations in Ukraine, e.g. (Zaporizhia).\n\nHistory and location\nIn the 1950s, the institute was founded as common nuclear physics research facility of the Austrian Universities. With its  nuclear research reactor, which was officially commissioned in 1962, it is today the only facility remaining in Austria that has a running nuclear fission reactor. \nAs in 2025 the official name of the institute in English is \"Institute of Atomic and Subatomic Physics\". However internationally the easier to remember name of Atomic Institute, or Atominstitute is more widespread in use. Administratively it is part of Vienna University of Technology (TU Wien) and, together with the Institutes of Theoretical, Applied and Solid State Physics, forms the Faculty of Physics of this university.\nUnusual for a nuclear reactor, the institute is located within the densely populated 2nd district of Vienna, Leopoldstadt, less than 3km from the city center. It boards the most popular large recreational park of Vienna, the Prater, and the most western branch of the Danube stream, the Donaukanal.\nCurrent routine activity at nuclear reactor and cooperation with IAEA\nA central inhouse facility is the TRIGA Mark II research reactor. It is used for University teaching, together with other research infrastructure, at the institute. The instate allows for practical education in the handling and work with radioactive materials and ionizing radiation. In addition to research and teaching in the fields of reactor physics, radiation protection, radiopharmaceuticals, radiochemistry and archaeometry, the area of reactor operation management is covered also. This includes organisation and practice in radiation protection, security and nuclear safety.\nThe know-how in nuclear management methods is fruitful for the routine cooperation with the International Atomic Energy Agency (IAEA), headquartered at the UNO City located at the main branch of the Danube, only few kilometers away. The reactor allows for the theoretical and practical training of international IQEA experts in live radiation fields. These experts serve then as inspectors for nuclear programmes and facilities worldwide within the United Nations framework of the  non-proliferations of nuclear weapons (NPT).\nAdditionally, the reactor is used for education at the college level on nuclear physics also. Over 1.000 undergraduate students visit the facility in guided tours annually.\n",
    "source": "wikipedia",
    "title": "Atominstitute",
    "topic": "Physics"
  },
  {
    "id": "wiki_74985603",
    "text": "In solid state physics, edge states are the topologically protected electronic states that exist at the boundary of the material and cannot be removed without breaking the system's symmetry.\n\nBackground\n\nThe electronic band structure of materials is primarily studied based on the extent of the band gap, the gap between highest occupied valence bands and lowest unoccupied conduction bands. The possible energy level of the material that provides the discrete energy values of all possible states in the energy profile diagram can be represented by solving the Hamiltonian of the system. This solution provides the corresponding energy eigenvalues and eigenvectors. Based on the energy eigenvalues, conduction band are the high energy states (energy E > 0) while valence bands are the low energy states (E < 0). In some materials, for example, in graphene and zigzag graphene quantum dot, there exists the energy states having energy eigenvalues exactly equal to zero (E = 0) besides the conduction and valence bands. These states are called edge states which modifies the electronic and optical properties of the materials significantly.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Edge states",
    "topic": "Physics"
  },
  {
    "id": "wiki_78147827",
    "text": "In space physics, an electrostatic solitary wave (ESW) is a type of electromagnetic soliton occurring during short time scales (when compared to the general time scales of variations in the average electric field) in plasma. When a rapid change occurs in the electric field in a direction parallel to the orientation of the magnetic field, and this perturbation is caused by a unipolar or dipolar electric potential, it is classified as an ESW.\nSince the creation of ESWs is largely associated with turbulent fluid interactions, some experiments use them to compare how chaotic a measured plasma's mixing is. As such, many studies which involve ESWs are centered around turbulence, chaos, instabilities, and magnetic reconnection.\n\nHistory\nThe discovery of solitary waves in general is attributed to John Scott Russell in 1834, with their first mathematical conceptualization being finalized in 1871 by Joseph Boussinesq (and later refined and popularized by Lord Rayleigh in 1876). However, these observations and solutions were for oscillations of a physical medium (usually water), and not describing the behavior of non-particle waves (including electromagnetic waves). For solitary waves outside of media, which ESWs are classified asa, the first major framework was likely developed by Louis de Broglie in 1927, though his work on the subject was temporarily abandoned and was not completed until the 1950s.\nElectrostatic structures were first observed near Earth's polar cusp by Donald Gurnett and Louis A. Frank using data from the Hawkeye 1 satellite in 1978. However, it is Michael Temerin, William Lotko, Forrest Mozer, and Keith Cernyb who are credited with the first observation of electrostatic solitary waves in Earth's magnetosphere in 1982. Since then, a wide variety of magnetospheric satellites have observed and documented ESWs, allowing for analysis of them and the surrounding plasma conditions.\nDetection\nElectrostatic solitary waves, by their nature, are a phenomenon occurring in the electric field of a plasma. As such, ESWs are technically detectable by any instrument that can measure changes to the electric field during a sufficiently short time window. However, since a given plasma's electric field can vary widely depending on the properties of the plasma and since ESWs occur in short time windows, detection of ESWs can require additional screening of the data in addition to the measurement of the electric field itself. One solution to this obstacle for detecting ESWs, implemented by NASA's Magnetospheric Multiscale Mission (MMS), is to use a digital signal processor to analyze the electric field data and isolate short-duration spikes as a candidate for an ESW. Though the following detection algorithm is specific to MMS, other ESW-detecting algorithms function on similar principles.\nTo detect an ESW, the data from a device measuring the electric field is sent to the digital signal processor. This data is analyzed across a short time window (in the case ",
    "source": "wikipedia",
    "title": "Electrostatic solitary wave",
    "topic": "Physics"
  },
  {
    "id": "wiki_80064998",
    "text": "In statistical physics, frenesy is a measure of the dynamical activity of a system's microscopic trajectories under non-equilibrium conditions. Frenesy complements the notion of entropy production (which measures time-antisymmetric aspects associated with irreversibility), and represents how frequently states are visited or how many transitions occur over time, as well as how busy the system's trajectories are. It relates to reactivities, escape rates, and residence times of a physical state, as it quantifies the rate of microscopic configuration changes that accompany entropy production in nonequilibrium steady states.\n\nOrigin and context\nThe concept of frenesy was introduced in 2006 in the study of non-equilibrium processes. In systems described by trajectory ensembles or path-space measures (e.g. arising from Markov processes or Langevin dynamics), frenesy corresponds to the time-symmetric component of the action functional, which includes trajectory-dependent quantities such as escape rates, undirected traffic, and total configuration changes. As with many physical observables, the change in frenesy is often the relevant measurable quantity, especially in the context of non-equilibrium response theory.\nThe role of dynamical activity in trajectory ensembles was explored in the study of large deviations. The need to characterize the time-symmetric fluctuation sector was emphasized in another 2006 paper by Christian Maes et al. Earlier work had referred to this quantity as \"traffic\". A year later, the term \"frenetic\" was introduced in the framework of response theory.\nMathematically, for stochastic trajectories obeying local detailed balance, entropy production quantifies the asymmetry between forward and time-reversed paths, while frenesy quantifies the symmetric contribution invariant under time reversal. It therefore measures changes in dynamical activity or quiescence relative to a reference process and level of description.\nRole in fluctuation–response\nFrenesy contributes to the generalization of fluctuation–dissipation relations beyond equilibrium. In non-equilibrium steady states, the linear response of an observable depends on correlations with both entropy production and frenesy. This correction helps describe response phenomena in systems driven far from equilibrium.\nExtending the Kubo and Green–Kubo formalisms, non-equilibrium linear response theory decomposes the response into an \"entropic\" term and a \"frenetic\" term. The frenetic component is absent in equilibrium but becomes significant under external driving forces. This behavior appears in non-equilibrium versions of the Sutherland–Einstein relation, where mobility depends not only on the diffusion matrix of the unperturbed system but also on force–current correlations. The frenetic term can lead to negative responses, such as in differential mobility or non-equilibrium specific heats. This effect—where the response decreases despite stronger driving—has theoretical support in se",
    "source": "wikipedia",
    "title": "Frenesy (physics)",
    "topic": "Physics"
  },
  {
    "id": "wiki_79966709",
    "text": "Haloscopes are experimental devices designed to detect axions, hypothetical particles that are candidates for dark matter. These instruments typically use a resonant microwave cavity placed in a strong magnetic field to convert axions into detectable photons via the Primakoff effect.\nHaloscopes probe for axions in a specific mass range and operate by tuning the cavity to resonate at frequencies corresponding to those masses. They have provided the lowest limits to the axion-photon coupling constant in their mass region. They are a part of the current experimental effort in search for axions.\n\nThe most well-known haloscope experiment to date is ADMX (Axion Dark Matter eXperiment). Other axion experiments, like IAXO (International AXion Observatory), may incorporate haloscope techniques in its broader axion detection strategy. One of these techniques is RADES (Relic Axion Dark matter Exploratory Setup) which was operated in CAST.\nHaloscope techniques, have also been proposed for the detection of high-frequency gravitational waves. In these concepts, a resonant cavity placed in a strong magnetic field can convert gravitational wave energy into electromagnetic signals through axion-like couplings or other beyond-standard-model interactions. Such approaches aim to explore gravitational wave frequencies in the MHz to GHz range, which are not accessible to conventional interferometers like LIGO or Virgo.\n\nReferences\n\nBibliography\nCrescini, Nicolò (14 March 2022). \"Building instructions for a ferromagnetic axion haloscope\". The European Physical Journal Plus. 137 (3). arXiv:2201.04081. doi:10.1140/epjp/s13360-022-02533-w. Retrieved 16 July 2025.\n",
    "source": "wikipedia",
    "title": "Haloscope (physics)",
    "topic": "Physics"
  },
  {
    "id": "wiki_76868734",
    "text": "The HUN-REN Wigner Research Centre for Physics  is the largest Hungarian research institute studying physics. Formerly a research institute of the Hungarian Academy of Sciences, it became a member of the Eötvös Loránd Research Network  and after the ELKH's reorganisation it became part of the HUN-REN Hungarian Research Network. The Wigner Research Centre was established in 2012 by the merger of the MTA KFKI Institute for Particle and Nuclear Physics and the MTA Institute for Solid State Physics and Optics, and takes the name of the Nobel Prize winning physicist Eugene Wigner. The research centre has two institutes, the Wigner Institute for Particle and Nuclear Physics  and the Wigner Institute for Solid State Physics and Optics.\n\nHistory\nThe predecessor of the research centre was the Central Research Institute of Physics of the Hungarian Academy of Sciences, founded in 1950. Originally established with two departments, the institute was soon expanded by several departments under the leadership of researchers such as Károly Simonyi and Lajos Jánossy. According to the preparatory committee, its aim was \"to raise Hungarian physics research from its present state, which is far behind that of other disciplines, and to enable productive scientific research in all fields of physics which are of primary importance for the development and application of science\". From the very beginning, KFKI has been the home to a wide variety of research, and Wigner FK is no different. The direct or indirect exploitation of results has always been a feature. Physics was not the only field at the institute, but also various technical and even life sciences. After the change of regime, the KFKI was dissolved in 1992. and its scientific institutes were given full autonomy within the MTA.\nSubsequently, the MTA Wigner Research Centre for Physics  was established on January 1, 2012, by the merger of the former MTA KFKI Institute for Particle and Nuclear Physics and the former MTA Institute for Solid State Physics and Optics. Since 2013, the Wigner Data Centre has been part of the Research Centre. From September 1, 2019, the Wigner RCP has been under the management of the Eötvös Loránd Research Network, and continues to operate as an MTA Institute of excellence, today one of the largest research institutes for physics at the HUN-REN. The main research areas are: quantum technology, experimental and theoretical particle physics, nuclear physics, general relativity and gravity, fusion plasma physics, space physics, nuclear materials science, experimental and theoretical solid-state physics, statistical physics, atomic physics, optics and materials science. Wigner RCP is also part of many international collaborations, experiments and projects, such as ALICE experiment, CMS, Na61  or EuPRAXIA\nResearchers working in Wigner RCP or in its predecessors: Géza Györgyi, Lajos Jánossy, István Kovács, Vlagyimir Naumovics Gribov, Károly Simonyi, Dezső Kiss, Zoltán Perjés, József Zimányi, Pé",
    "source": "wikipedia",
    "title": "HUN-REN Wigner Research Centre for Physics",
    "topic": "Physics"
  },
  {
    "id": "wiki_79826334",
    "text": "Joaquim da Costa Ribeiro (Rio de Janeiro, July 8th 1906 - July 29th, 1960) was a Brazilian physicist and university professor in Brazil. He discovered the thermodielectric effect, also known as the Workman-Reynolds in the US. Ribeiro was a member of the Brazilian Academy of Sciences and was the first Scientific Director of CNPq.\nHe is the father of anthropologist Yvonne Maggie and grandfather of movie author Ana Costa Ribeiro, who directed \"Termodielétrico\", a memoir film about him and his legacy.\n\nBiography\nCosta Ribeiro was born at his family's house, on Barão de Itapejipe street, 82, in what was then the federal district of Brazil. His parents were Antonio Marques da Costa Ribeiro and Maria Constança Alburquerque da Costa Ribeiro. His father and grandfather, after whom Joaquim was named, were judges. \nCosta Ribeiro studied in a Jesuit school called Santo Inácio, until 1923, enrolling in the National School of Engineering the next year at the University of Brazil.\nTen years later, he got tenure at the same university. In 1940, Costa Ribeiro started researching new methods to measure radioactivity, and later studied the production of electret using several dielectric materials\nCosta Ribeiro observed that, during electret formation, electric current was unnecessary: the dielectric's natural freezing was enough to electrify the end material, provided that one of the cooling phases was solid.\nThe phenomenom was named \"thermodielectric effect\" by Costa Ribeiro and fully described by him in a 1944 article in the Annals of the Brazilian Academy of Sciences (ABC) that gathered significant attention at home and abroad. This marked the first physical phenomenom completely observed and described by a Brazilian researcher.\nTwo years later, he got a permanent position in general and experimental Physics at the National Philosophy College, at the same university.\nDeath\nCosta Ribeiro died on July 29, 1960 at 54 years of age, at the Casa de Saúde Santa Lúcia. He was survived by his nine children.\n",
    "source": "wikipedia",
    "title": "Joaquim da Costa Ribeiro",
    "topic": "Physics"
  },
  {
    "id": "wiki_78751748",
    "text": "Lofting, sometimes referred to as \"trajectory shaping\", is a trajectory optimization technique used in some missile systems to extend range and improve target engagement effectiveness, usually in beyond-visual range scenarios.\n\nMethod\nLofting involves a missile ascending to a higher altitude after launch, creating a parabolic arc similar to ballistic missiles, before descending toward its target. This elevated flight path allows the missile to capitalize on reduced air resistance at higher altitudes, increasing both the missile's potential energy and the kinetic energy during terminal guidance, thus enabling greater range and probability of kill.\nPeak altitiude of a lofted trajectory can be at altitudes ranging from 20,000–110,000 ft (6–34 km), with most air-to-air missiles peaking at around 80,000–100,000 ft (24–30 km), although the peaks of ballistic missiles' parabolic arcs can range from 50 km (164,042 ft) to 1,500 km (4,921,260 ft).\nAdvantages\nLofting offers several distinct advantages compared to sea-skimming and direct-intercept trajectories, particularly in beyond-visual-range engagements.\nUnlike sea-skimming, which prioritizes low-altitude flight to avoid radar detection but suffers from increased drag and limited range, lofting allows the missile to ascend to higher altitudes where air resistance is lower. This reduced drag enables greater range and energy efficiency, allowing the missile to retain more kinetic energy for terminal guidance and target interception.\nCompared to direct-intercept trajectories, lofting also improves engagement flexibility by providing a steeper attack angle, which is particularly effective against maneuvering or high-altitude targets.\nDisadvantages\nIn comparison to sea-skimming trajectories, lofting lacks radar-avoidance characteristics, making it susceptible to detection by its target and potential interceptors.\nLofting is also more mathematically and technologically complex in comparison to direct-interception, and is only viable in long-range engagements.\nAdditionally, the thinner air which lofting utilizes to reduce drag and increase range carries the downside of impeding the ability for control surfaces to maneuver the missile. This can reduce a missile's ability to adjust for fast-moving or maneuvering targets, however can be circumvented with the use of thrust vectoring - at the downside of added cost and complexity.\n",
    "source": "wikipedia",
    "title": "Missile lofting",
    "topic": "Physics"
  },
  {
    "id": "wiki_844186",
    "text": "Modern physics is a branch of physics that developed in the early 20th century and onward or branches greatly influenced by early 20th century physics. Notable branches of modern physics include quantum mechanics, special relativity, and general relativity.\nClassical physics is typically concerned with everyday conditions: speeds are much lower than the speed of light, sizes are much greater than that of atoms, and energies are relatively small. Modern physics, however, is concerned with more extreme conditions, such as high velocities that are comparable to the speed of light (special relativity), small distances comparable to the atomic radius (quantum mechanics), and very high energies (relativity). In general, quantum and relativistic effects are believed to exist across all scales, although these effects may be very small at human scale. While quantum mechanics is compatible with special relativity (See: Relativistic quantum mechanics), one of the unsolved problems in physics is the unification of quantum mechanics and general relativity, which the Standard Model of particle physics currently cannot account for.\nModern physics is an effort to understand the underlying processes of the interactions of matter using the tools of science and engineering. In a literal sense, the term modern physics means up-to-date physics. In this sense, a significant portion of so-called classical physics is modern. However, since roughly 1890, new discoveries have caused significant paradigm shifts: especially the advent of quantum mechanics (QM) and relativity (ER). Physics that incorporates elements of either QM or ER (or both) is said to be modern physics. It is in this latter sense that the term is generally used.\nModern physics is often encountered when dealing with extreme conditions. Quantum mechanical effects tend to appear when dealing with \"lows\" (low temperatures, small distances), while relativistic effects tend to appear when dealing with \"highs\" (high velocities, large distances), the \"middles\" being classical behavior. For example, when analyzing the behavior of a gas at room temperature, most phenomena will involve the (classical) Maxwell–Boltzmann distribution. However, near absolute zero, the Maxwell–Boltzmann distribution fails to account for the observed behavior of the gas, and the (modern) Fermi–Dirac or Bose–Einstein distributions have to be used instead.\n\nVery often, it is possible to find – or \"retrieve\" – the classical behavior from the modern description by analyzing the modern description at low speeds and large distances (by taking a limit, or by making an approximation). When doing so, the result is called the classical limit.\n\nHallmark experiments\nThese are generally considered to be experiments regarded leading to the foundation of modern physics:\n",
    "source": "wikipedia",
    "title": "Modern physics",
    "topic": "Physics"
  },
  {
    "id": "wiki_168907",
    "text": "Naïve physics or folk physics is the untrained human perception of basic physical phenomena. In the field of artificial intelligence the study of naïve physics is a part of the effort to formalize the common knowledge of human beings.\nMany ideas of folk physics are simplifications, misunderstandings, or misperceptions of well-understood phenomena, incapable of giving useful predictions of detailed experiments, or simply are contradicted by more thorough observations. They may sometimes be true, be true in certain limited cases, be true as a good first approximation to a more complex effect, or predict the same effect but misunderstand the underlying mechanism.\nNaïve physics is characterized by a mostly intuitive understanding humans have about objects in the physical world. Certain notions of the physical world may be innate.\n\nExamples\nSome examples of naïve physics include commonly understood, intuitive, or everyday-observed rules of nature:\n\nWhat goes up must come down\nA dropped object falls straight down\nA solid object cannot pass through another solid object\nA vacuum sucks things towards it\nAn object is either at rest or moving, in an absolute sense\nTwo events are either simultaneous or they are not\nMany of these and similar ideas formed the basis for the first works in formulating and systematizing physics by Aristotle and the medieval scholastics in Western civilization. In the modern science of physics, they were gradually contradicted by the work of Galileo, Newton, and others. The idea of absolute simultaneity survived until 1905, when the special theory of relativity and its supporting experiments discredited it.\nPsychological research\nThe increasing sophistication of technology makes possible more research on knowledge acquisition. Researchers measure physiological responses such as heart rate and eye movement in order to quantify the reaction to a particular stimulus. Concrete physiological data is helpful when observing infant behavior, because infants cannot use words to explain things (such as their reactions) the way most adults or older children can.\nResearch in naïve physics relies on technology to measure eye gaze and reaction time in particular. Through observation, researchers know that infants get bored looking at the same stimulus after a certain amount of time. That boredom is called habituation. When an infant is sufficiently habituated to a stimulus, he or she will typically look away, alerting the experimenter to his or her boredom. At this point, the experimenter will introduce another stimulus. The infant will then dishabituate by attending to the new stimulus. In each case, the experimenter measures the time it takes for the infant to habituate to each stimulus.\nAs an example of the use of this method, research by Susan Hespos and colleagues studied five-month-old infants' responses to the physics of liquids and solids. Infants in this research were shown liquid being poured from one glass to another until they were ",
    "source": "wikipedia",
    "title": "Naïve physics",
    "topic": "Physics"
  },
  {
    "id": "wiki_77326503",
    "text": "Negative air ions (NAI) are negatively charged single gas molecules or ion clusters in the air. They play a role in maintaining the charge balance of the atmosphere. The main components of air are molecular nitrogen and oxygen. Due to the strong electronegativity of oxygen and oxygen-containing molecules, they can easily capture electrons to form negatively charged air ions, most of which are superoxide radicals ·O2−, so NAI are mainly composed of negative oxygen ions, also called air negative oxygen ions.\nThe ions can be produced by natural means such as lightning, or artificially by methods such as a corona discharge. They can play a role in electrostatic removal of air particulates both for industrial applications and with indoor air, and there are claims that they have a beneficial health effect although the evidence for this is weak.\n\nGeneration mechanism\nVarious negative air ions are formed by combining active neutral molecules and electrons in the gas through a series of ion-molecule reactions.\nIn the air, due to the presence of many water molecules, the negative air ions typically combine with water to form hydrated negative air ions, such as O−·(H2O)n, O2−·(H2O)n, O3−·(H2O)n, OH−·(H2O)n, CO3−·(H2O)n, HCO3−·(H2O)n, CO4−·(H2O)n, NO2−·(H2O)n, NO3−·(H2O)n, etc. The ion clusters formed by the combination of small ions and water molecules have a longer survival period due to their large volume and the fact that the charge is protected by water molecules and is not easy to transfer. This is because in the molecular collision, the larger the molecular volume, the less energy is lost when encountering collisions with other molecules, thereby extending the survival time of negative air ions.\nGeneration methods\nNegative air ions can be produced by two methods: natural or artificial. The methods of producing negative air ions in nature include the waterfall effect, lightning ionization and from plants. Natural methods can produce a large number of negative air ions. The artificial means of producing negative air ions include corona discharge and water vapour. Compared with the negative air ions produced in nature, although artificial methods can produce high levels of negative air ions, there are differences in the types and concentrations of negative air ions.\n",
    "source": "wikipedia",
    "title": "Negative air ions",
    "topic": "Physics"
  },
  {
    "id": "wiki_78245824",
    "text": "In condensed matter physics, the Nottingham effect is a surface cooling and heating mechanism that occurs during field and thermionic electron emission. The effect is named after physicist Wayne B. Nottingham who explained it in a commentary to 1940 experiments by Gertrude M. Fleming and Joseph E. Henderson.\nThe temperature at which electron emission goes from heating to cooling is called the Nottingham inversion temperature.\n\nDescription\nNotably, the effect can be either heating or cooling of the surface emitting the electrons, depending upon the energy at which they are supplied. Above the Nottingham inversion temperature, the emission energy exceeds the Fermi energy of the electron supply and the emitted electron carries more energy away from the surface than is returned by the supply of a replacement electron, and the net heat flux from the Nottingham effect switches from heating to cooling the cathode.\nAlong with Joule heating, the Nottingham effect contributes to the thermal equilibrium of electron emission systems, typically becoming the dominant contributor at very high emission current densities. It comes into play in the operation of field emission array cathodes and other devices that rely upon stimulating Fowler-Nordheim electron emission, usually at the apex of a sharp tip used to create a field enhancement effect.  In extreme cases, the Nottingham effect can heat the emitter tips to temperatures exceeding the melting point of the tip material, causing the tip to deform and emit material that may cause a vacuum arc; this is a significant failure mode for tip-based cathodes.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Nottingham effect",
    "topic": "Physics"
  },
  {
    "id": "wiki_1996857",
    "text": "In thermodynamics, nucleation is the first step in the formation of either a new thermodynamic phase or structure via self-assembly or self-organization within a substance or mixture. Nucleation is typically defined to be the process that determines how long an observer has to wait before the new phase or self-organized structure appears. For example, if a volume of water is cooled (at atmospheric pressure) significantly below 0 °C, it will tend to freeze into ice, but volumes of water cooled only a few degrees below 0 °C often stay completely free of ice for long periods (supercooling). At these conditions, nucleation of ice is either slow or does not occur at all. However, at lower temperatures nucleation is fast, and ice crystals appear after little or no delay.\nNucleation is a common mechanism which generates first-order phase transitions, and it is the start of the process of forming a new thermodynamic phase. In contrast, new phases at continuous phase transitions start to form immediately.\nNucleation is often very sensitive to impurities in the system. These impurities may be too small to be seen by the naked eye, but still can control the rate of nucleation. Because of this, it is often important to distinguish between heterogeneous nucleation and homogeneous nucleation. Heterogeneous nucleation occurs at nucleation sites on surfaces in the system. Homogeneous nucleation occurs away from a surface.\n\nCharacteristics\nNucleation is usually a stochastic (random) process, so even in two identical systems nucleation will occur at different times. A common mechanism is illustrated in the animation to the right. This shows nucleation of a new phase (shown in red) in an existing phase (white). In the existing phase microscopic fluctuations of the red phase appear and decay continuously, until an unusually large fluctuation of the new red phase is so large it is more favourable for it to grow than to shrink back to nothing. This nucleus of the red phase then grows and converts the system to this phase. The standard theory that describes this behaviour for the nucleation of a new thermodynamic phase is called classical nucleation theory. However, the CNT fails in describing experimental results of vapour to liquid nucleation even for model substances like argon by several orders of magnitude.\nFor nucleation of a new thermodynamic phase, such as the formation of ice in water below 0 °C, if the system is not evolving with time and nucleation occurs in one step, then the probability that nucleation has not occurred should undergo exponential decay. This is seen for example in the nucleation of ice in supercooled small water droplets. The decay rate of the exponential gives the nucleation rate. Classical nucleation theory is a widely used approximate theory for estimating these rates, and how they vary with variables such as temperature. It correctly predicts that the time you have to wait for nucleation decreases extremely rapidly when supersaturated.\n",
    "source": "wikipedia",
    "title": "Nucleation",
    "topic": "Physics"
  },
  {
    "id": "wiki_2137509",
    "text": "In physics, a perfect fluid or ideal fluid  is a fluid that can be completely characterized by its rest frame mass density \n  \n    \n      \n        \n          ρ\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\rho _{m}}\n  \n and isotropic pressure ⁠\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n⁠. Real fluids are viscous (\"sticky\") and contain (and conduct) heat. Perfect fluids are idealized models in which these possibilities are ignored. Specifically, perfect fluids have no shear stresses, viscosity, or heat conduction. \nA quark–gluon plasma\nand graphene are examples of nearly perfect fluids that could be studied in a laboratory.\n\nNon-relativistic fluid mechanics\nIn classical mechanics, ideal fluids are described by Euler equations. Ideal fluids produce no drag according to d'Alembert's paradox. If a fluid produced drag, then work would be needed to move an object through the fluid and that work would produce heat or fluid motion. However, a perfect fluid can not dissipate energy and it can't transmit energy infinitely far from the object.\nA flock of birds in the medium of air is an example of a perfect fluid; an electron gas is also modeled as a perfect fluid.\nCosmology and astrophysics\nPerfect fluids are a fluid solution used in general relativity to model idealized distributions of matter, such as the interior of a star or an isotropic universe. In the latter case, the symmetry of the cosmological principle and the equation of state of the perfect fluid lead to Friedmann equation for the expansion of the universe.\nSee also\nEquation of state\nIdeal gas\nFluid solutions in general relativity\nPotential flow\nReferences\n\nFurther reading\nS.W. Hawking; G.F.R. Ellis (1973), The Large Scale Structure of Space-Time, Cambridge University Press ISBN 0-521-20016-4, ISBN 0-521-09906-4 (pbk.)\nJackiw, R; Nair, V P; Pi, S-Y; Polychronakos, A P (2004-10-22). \"Perfect fluid theory and its extensions\". Journal of Physics A: Mathematical and General. 37 (42): R327–R432. arXiv:hep-ph/0407101. doi:10.1088/0305-4470/37/42/R01. ISSN 0305-4470. Topical review.\n",
    "source": "wikipedia",
    "title": "Perfect fluid",
    "topic": "Physics"
  },
  {
    "id": "wiki_79820729",
    "text": "Physics of life is a branch of physics that studies the fundamental principles governing living systems. It applies methods from mechanics, thermodynamics, statistical physics, and information theory to biological phenomena ranging from molecular assemblies to ecosystems. The field seeks to understand how complex behaviors of life arise from interactions among physical components under conditions far from equilibrium. Biological physics has gained wider recognition as a distinct and essential area within physics research.\n\nOverview\nThe physics of life investigates how the familiar laws of physics apply to living matter, and how living systems sometimes require new physical principles for their understanding. Rather than viewing biology as an exception, researchers treat biological phenomena as fertile ground for discovering general laws of non-equilibrium matter and information processing.\nBiological physics has grown substantially and now constitutes one of the largest divisions of the American Physical Society (APS). It bridges traditional disciplines and introduces concepts such as stochasticity, phase transitions, and self-organization into the study of life.\nConceptual Foundations\nA 2022 decadal survey by the National Academies of Sciences, Engineering, and Medicine outlined five central questions guiding research in the physics of life:\nExperimental and Theoretical Approaches\nExperimental tools include optical tweezers, cryo-electron microscopy, and single-molecule tracking. Theoretical approaches combine statistical mechanics, continuum mechanics, machine learning, and non-equilibrium physics.\nConcepts such as phase transitions, self-organization, and stochastic fluctuations, traditionally studied in inanimate systems, have become central for understanding biological systems.\nRecognition and Growth\nBiological physics has evolved from an interdisciplinary curiosity into a central part of modern physics. Researchers emphasize that studying life offers opportunities to discover new organizing principles of matter and information. The National Academies report and commentary from the American Physical Society call for expanded funding, interdisciplinary training, and infrastructure to accelerate progress.\n",
    "source": "wikipedia",
    "title": "Physics of Life",
    "topic": "Physics"
  },
  {
    "id": "wiki_27481335",
    "text": "In physics, the plasmaron was proposed by Lundqvist in 1967 as a quasiparticle arising in a system that has strong plasmon-electron interactions. In the original work, the plasmaron was proposed to describe a secondary peak (or satellite) in the photoemission spectral function of the electron gas. More precisely it was defined as an additional zero of the quasi-particle equation \n  \n    \n      \n        (\n        ω\n        −\n        \n          ϵ\n          \n            H\n          \n        \n        −\n        R\n        e\n        [\n        Σ\n        (\n        ω\n        )\n        ]\n        =\n        0\n        )\n      \n    \n    {\\displaystyle (\\omega -\\epsilon _{H}-Re[\\Sigma (\\omega )]=0)}\n  \n. The same authors pointed out, in a subsequent work, that this extra solution might be an artifact of the used approximations:\n\nWe want to stress again that the discussion we have given of the one-electron spectrum is based on the assumption that vertex corrections are small. As discussed in the next section recent work by Langreth [29] shows that vertex corrections in the core electron problem can have a quite large effect on the form of satellite structures, while their effect on the quasi particle properties seems to be small. Preliminary investigations by one of us (L.H.) show similar strong vertex effects on the conduction band satellite. The details of the plasmaron structure should thus not be taken very seriously. A more mathematical discussion is provided.\nThe plasmaron was also studied in more recent works in the literature. It was shown, also with the support of the numerical simulations, that the plasmaron energy is an artifact of the approximation used to numerically compute the spectral function, e.g. solution of the dyson equation for the many body green function with a frequency dependent GW self-energy. This approach give rise to a wrong plasmaron peak instead of the plasmon satellite which can be measured experimentally.\nDespite this fact, experimental observation of a plasmaron was reported in 2010 for graphene.\nAlso supported by earlier theoretical work. However subsequent works discussed that the theoretical interpretation of the experimental measure was not correct, in agreement with the fact that the plasmaron is only an artifact of the GW self-energy used with the Dyson equation. The artificial nature of the plasmaron peak was also proven via the comparison of experimental and numerical simulations for the photo-emission spectrum of bulk silicon. Other works on plasmaron have been published in the literature.\nObservation of plasmaron peaks have also been reported in optical measurements of elemental bismuth and in other optical measurements.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Plasmaron",
    "topic": "Physics"
  },
  {
    "id": "wiki_75463818",
    "text": "A quasi-isodynamic (QI) stellarator is a type of stellarator (a magnetic confinement fusion reactor) that satisfies the property of omnigeneity, avoids the potentially hazardous toroidal bootstrap current, and has minimal neoclassical transport in the collisionless regime.\nWendelstein 7-X, the largest stellarator in the world, was designed to be roughly quasi-isodynamic (QI).\nIn contrast to quasi-symmetric fields, exactly QI fields on flux surfaces cannot be expressed analytically. However, it has been shown that nearly-exact QI can be extremely well approximated through mathematical optimization, and that the resulting fields enjoy the aforementioned properties.\nIn a QI field, level curves of the magnetic field strength \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n on a flux surface close poloidally (the short way around the torus), and not toroidally (the long way around), causing the stellarator to resemble a series of linked magnetic mirrors.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Quasi-isodynamic stellarator",
    "topic": "Physics"
  },
  {
    "id": "wiki_81547074",
    "text": "SDSS J120136.02+300305.5 is an optically inactive, quiescent galaxy that possibly contains a  milliparsec supermassive black hole binary.\n\nDiscovery\nX-ray fluxes in the galaxy were first detected in June 2010 with a flux 56 times higher than an upper limit from ROSAT, corresponding to LX ~ 3 × 1044 erg s−1. It had the rough optical spectrum of quiescent galaxy. All in all, the flux evolved in fair consistency with the normal t−5/3 model, something that would be normal for returning stellar debris, before fading by a factor of roughly 300 after 300 days. However, the source was incredibly volatile, becoming invisible between 27–48 days after discovery. It may have matched a Bremsstrahlung or double-power-law model and usually softens as time passes.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "SDSS J120136.02+300305.5",
    "topic": "Physics"
  },
  {
    "id": "wiki_76197486",
    "text": "Shockwave cosmology is a non-standard cosmology proposed by Joel Smoller and Blake Temple in 2003. In this model, the “big bang” is an explosion inside a black hole, producing the expanding volume of space and matter that includes the observable universe.\n\nIntegration with general relativity\nSmoller and Temple integrate shock waves into Einstein's general relativity. This produces a universe that \"looks essentially identical to the aftermath of the big bang\" according to cosmologists Barnes and Lewis. They explain that Smoller and Temple's version is distinguished from the big bang only by there being a shockwave at the leading edge of an explosion – one that, for Smoller and Temple's model, must be beyond the observable universe. However, Barnes and Lewis do not support shockwave cosmology because they see it as not testable; they point out that there is no explosion in the standard theory of the Big Bang.\nCurrent and future state of the universe\nFrom Smoller and Temple's calculations, we are still inside an expanding black hole. The configuration of 'flat' spacetime (see Minkowski space) inside a black hole, also occurs during the moments of the formation of a black hole from a collapsing star.\nEventually, according to shockwave cosmology, the mass of our expanding volume of space and matter will fall in density as it expands. At some point, the event horizon of the black hole will cease to be. An outside observer will then see it appear as a white hole. The matter would then continue to expand.\nAlternative to dark energy\nIn related work, Smoller, Temple, and Vogler propose that this shockwave may have resulted in our part of the universe having a lower density than that surrounding it, causing the accelerated expansion normally attributed to dark energy.  \nThey also propose that this related theory could be tested: a universe with dark energy should give a figure for the cubic correction to redshift versus luminosity C = −0.180 at a = a whereas for Smoller, Temple, and Vogler's alternative C should be positive rather than negative. They give a more precise calculation for their wave model alternative as: the cubic correction to redshift versus luminosity at a = a is C = 0.359.\n",
    "source": "wikipedia",
    "title": "Shockwave cosmology",
    "topic": "Physics"
  },
  {
    "id": "wiki_21276538",
    "text": "Surface stress was first defined by Josiah Willard Gibbs (1839–1903) as the amount of the reversible work per unit area needed to elastically stretch a pre-existing surface. Depending upon the convention used, the area is either the original, unstretched one which represents a constant number of atoms, or sometimes is the final area; these are atomistic versus continuum definitions. Some care is needed to ensure that the definition used is also consistent with the elastic strain energy, and misinterpretations and disagreements have occurred in the literature.\nA similar term called \"surface free energy\", the excess free energy per unit area needed to create a new surface, is sometimes confused with \"surface stress\". Although surface stress and surface free energy of liquid–gas or liquid–liquid interface are the same, they are very different in solid–gas or solid–solid interface. Both terms represent an energy per unit area, equivalent to a  force per unit length, so are sometimes referred to as \"surface tension\", which contributes further to the confusion in the literature.\n\nThermodynamics of surface stress\nThe continuum definition of surface free energy is the amount of reversible work \n  \n    \n      \n        d\n        w\n      \n    \n    {\\displaystyle dw}\n  \n performed to create new area \n  \n    \n      \n        d\n        A\n      \n    \n    {\\displaystyle dA}\n  \n of surface, expressed as:\n\n  \n    \n      \n        d\n        w\n        =\n        γ\n        d\n        A\n      \n    \n    {\\displaystyle dw=\\gamma dA}\n  \n\nIn this definition the number of atoms at the surface is proportional to the area. Gibbs was the first to define another surface quantity, different from the surface free energy \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n, that is associated with the reversible work per unit area needed to elastically stretch a pre-existing surface. In a continuum approach one can define a surface stress tensor \n  \n    \n      \n        \n          f\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle f_{ij}}\n  \n that relates the work associated with the variation in \n  \n    \n      \n        γ\n        A\n      \n    \n    {\\displaystyle \\gamma A}\n  \n, the total excess free energy of the surface due to a strain tensor \n  \n    \n      \n        \n          e\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle e_{ij}}\n  \n\n  \n    \n      \n        A\n        \n          f\n          \n            i\n            j\n          \n        \n        =\n        d\n        (\n        γ\n        A\n        )\n        \n          /\n        \n        d\n        \n          e\n          \n            i\n            j\n          \n        \n        =\n        A\n        d\n        γ\n        \n          /\n        \n        d\n        \n          e\n          \n            i\n            j\n          \n        \n        +\n        γ\n        d\n        A\n        \n          /\n        \n        d\n        \n          e\n          \n  ",
    "source": "wikipedia",
    "title": "Surface stress",
    "topic": "Physics"
  },
  {
    "id": "wiki_79850130",
    "text": "Synchronous lateral excitation is a dynamic phenomenon where pedestrians walking on a footbridge subconsciously synchronize their lateral footsteps with the bridge’s natural swaying motion, amplifying lateral vibrations. First widely recognized during the 2000 opening of the London Millennium Bridge, synchronous lateral excitation has since become a critical consideration in the design of lightweight pedestrian structures.\n\nMechanism\nSynchronous lateral excitation arises from two interrelated synchronization processes. The first is the pedestrian-structure synchronization, where slight lateral bridge movements (e.g., from wind or random pedestrian steps) prompt walkers to adjust their gait to match the bridge’s oscillation frequency, increasing lateral forces.\nThe second is pedestrian-pedestrian synchronization, where individuals unconsciously align their stepping patterns, further reinforcing the resonant force.\nKey cases\nThe London Millennium Bridge experienced lateral vibrations up to 70 mm due to synchronous lateral excitation, requiring a £5M retrofit with dampers.\nThe Auckland Harbour Bridge experienced a lateral frequency of 0.67 Hz during a 1975 demonstration.\nThe Birmingham NEC Link bridge experienced a lateral frequency of 0.7 Hz.\nThe Toda Park Bridge in Japan is an early documented case (1990s) studied by Fujino et al., informing later synchronous lateral excitation models.\nMitigation strategies\nSome ways to avoid synchronous lateral excitation are the implementation of tuned mass dampers, which were used in the Millennium Bridge to increase damping from 0.5% to 20% critical. Other strategies involve designing bridges with lateral frequencies outside the 0.5–1.1 Hz range as well as managing crows by limiting pedestrian density during events.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Synchronous lateral excitation",
    "topic": "Physics"
  },
  {
    "id": "wiki_467047",
    "text": "The term \"thermal energy\" is often used ambiguously in physics and engineering. It can denote several different physical concepts, including:\n\nInternal energy: The energy contained within a body of matter or radiation, excluding the potential energy of the whole system.\nHeat: Energy in transfer between a system and its surroundings by mechanisms other than thermodynamic work and transfer of matter.\nThe characteristic energy kBT, where T denotes temperature and kB denotes the Boltzmann constant; it is twice that associated with each degree of freedom.\nMark Zemansky (1970) has argued that the term \"thermal energy\" is best avoided due to its ambiguity. He suggests using more precise terms such as \"internal energy\" and \"heat\" to avoid confusion. The term is, however, used in some textbooks.\n\nRelation between heat and internal energy\nIn thermodynamics, heat is energy in transfer to or from a thermodynamic system by mechanisms other than thermodynamic work or transfer of matter, such as conduction, radiation, and friction. Heat refers to a quantity in transfer between systems, not to a property of any one system, or \"contained\" within it; on the other hand, internal energy and enthalpy are properties of a single system. Heat and work depend on the way in which an energy transfer occurs. In contrast, internal energy is a property of the state of a system and can thus be understood without knowing how the energy got there.\nMacroscopic thermal energy\nIn addition to the microscopic kinetic energies of its molecules, the internal energy of a body includes chemical energy belonging to distinct molecules, and the global joint potential energy involved in the interactions between molecules and suchlike. Thermal energy may be viewed as contributing to internal energy or to enthalpy.\nMicroscopic thermal energy\nIn a statistical mechanical account of an ideal gas, in which the molecules move independently between instantaneous collisions, the internal energy is just the sum total of the gas's independent particles' kinetic energies, and it is this kinetic motion that is the source and the effect of the transfer of heat across a system's boundary. For a gas that does not have particle interactions except for instantaneous collisions, the term \"thermal energy\" is effectively synonymous with \"internal energy\".\nIn many statistical physics texts, \"thermal energy\" refers to \n  \n    \n      \n        k\n        T\n      \n    \n    {\\displaystyle kT}\n  \n, the product of the Boltzmann constant and the absolute temperature, also written as ⁠\n  \n    \n      \n        \n          k\n          \n            B\n          \n        \n        T\n      \n    \n    {\\displaystyle k_{\\text{B}}T}\n  \n⁠.\n",
    "source": "wikipedia",
    "title": "Thermal energy",
    "topic": "Physics"
  },
  {
    "id": "wiki_74170779",
    "text": "The toroidal solenoid was an early 1946 design for a fusion power device designed by George Paget Thomson and Moses Blackman of Imperial College London. It proposed to confine a deuterium fuel plasma to a toroidal (donut-shaped) chamber using magnets, and then heating it to fusion temperatures using radio frequency energy in the fashion of a microwave oven. It is notable for being the first such design to be patented, filing a secret patent on 8 May 1946 and receiving it in 1948.\nA critique by Rudolf Peierls noted several problems with the concept. Over the next few years, Thomson continued to suggest starting an experimental effort to study these issues, but was repeatedly denied as the underlying theory of plasma diffusion was not well developed. When similar concepts were suggested by Peter Thonemann that included a more practical heating arrangement, John Cockcroft began to take the concept more seriously, establishing small study groups at Harwell. Thomson adopted Thonemann's concept, abandoning the radio frequency system.\nWhen the patent had still not been granted in early 1948, the Ministry of Supply inquired about Thomson's intentions. Thomson explained the problems he had getting a program started and that he did not want to hand off the rights until that was clarified. As the directors of the UK nuclear program, the Ministry quickly forced Harwell's hand to provide funding for Thomson's program. Thomson then released his rights the patent, which was granted late that year. Cockcroft also funded Thonemann's work, and with that, the UK fusion program began in earnest. After the news furor over the Huemul Project in February 1951, significant funding was released and led to rapid growth of the program in the early 1950s, and ultimately to the ZETA reactor of 1958.\n\nConceptual development\nThe basic understanding of nuclear fusion was developed during the 1920s as physicists explored the new science of quantum mechanics. George Gamow's 1928 work on quantum tunnelling demonstrated that nuclear reactions could take place at lower energies than classical theory predicted. Using this theory, in 1929 Fritz Houtermans and Robert Atkinson demonstrated that expected reaction rates in the core of the Sun supported Arthur Eddington's 1920 suggestion that the Sun is powered by fusion.\nIn 1934, Mark Oliphant, Paul Harteck and Ernest Rutherford were the first to achieve fusion on Earth, using a particle accelerator to shoot deuterium nuclei into a metal foil containing deuterium, lithium or other elements. This allowed them to measure the nuclear cross section of various fusion reactions, and determined that the deuterium-deuterium reaction occurred at a lower energy than other reactions, peaking at about 100,000 electronvolts (100 keV).\nThis energy corresponds to the average energy of particles in a gas heated to a billion Kelvin. Materials heated beyond a few tens of thousand Kelvin dissociate into their electrons and nuclei, producing a gas-like state",
    "source": "wikipedia",
    "title": "Toroidal solenoid",
    "topic": "Physics"
  },
  {
    "id": "wiki_79026168",
    "text": "The Wohlfarth Memorial Lecture, and the Wohlfarth Lectureship, is a lecture and prize given at the UK-based Institute of Physics' annual Magnetism Conference. It is named after Professor Erich Peter Wohlfarth, in honour of his \"outstanding contribution [...] to the field of magnetism\". It has been awarded since 1989.\n\nRecipients\n\nSee also\nInstitute of Physics\nInstitute of Physics Awards\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Wohlfarth Lectureship",
    "topic": "Physics"
  },
  {
    "id": "wiki_5180",
    "text": "Chemistry is the scientific study of the properties and behavior of matter. It is a physical science within the natural sciences that studies the chemical elements that make up matter and compounds made of atoms, molecules and ions: their composition, structure, properties, behavior and the changes they undergo during reactions with other substances. Chemistry also addresses the nature of chemical bonds in chemical compounds.\nIn the scope of its subject, chemistry occupies an intermediate position between physics and biology. It is sometimes called the central science because it provides a foundation for understanding both basic and applied scientific disciplines at a fundamental level. For example, chemistry explains aspects of plant growth (botany), the formation of igneous rocks (geology), how atmospheric ozone is formed and how environmental pollutants are degraded (ecology), the properties of the soil on the Moon (cosmochemistry), how medications work (pharmacology), and how to collect DNA evidence at a crime scene (forensics).\nChemistry has existed under various names since ancient times. It has evolved, and now chemistry encompasses various areas of specialisation, or subdisciplines, that continue to increase in number and interrelate to create further interdisciplinary fields of study. The applications of various fields of chemistry are used frequently for economic purposes in the chemical industry.\n\nEtymology\nThe word chemistry comes from a modification during the Renaissance of the word alchemy, which referred to an earlier set of practices that encompassed elements of chemistry, metallurgy, philosophy, astrology, astronomy, mysticism, and medicine. Alchemy is often associated with the quest to turn lead or other base metals into gold, though alchemists were also interested in many of the questions of modern chemistry.\nThe modern word alchemy in turn is derived from the Arabic word al-kīmīā (الكیمیاء). This may have Egyptian origins since al-kīmīā is derived from the Ancient Greek χημία, which is in turn derived from the word Kemet, which is the ancient name of Egypt in the Egyptian language. Alternately, al-kīmīā may derive from χημεία 'cast together'.\n",
    "source": "wikipedia",
    "title": "Chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_43180070",
    "text": "Actinide chemistry (or actinoid chemistry) is one of the main branches of nuclear chemistry that investigates the processes and molecular systems of the actinides. The actinides derive their name from the group 3 element actinium. The informal chemical symbol An is used in general discussions of actinide chemistry to refer to any actinide. All but one of the actinides are f-block elements, corresponding to the filling of the 5f electron shell; lawrencium, a d-block element, is also generally considered an actinide. In comparison with the lanthanides, also mostly f-block elements, the actinides show much more variable valence. The actinide series encompasses the 15 metallic chemical elements with atomic numbers from 89 to 103, actinium through lawrencium.\n\nMain branches\n\nNuclear reactions\nSome early evidence for nuclear fission was the formation of a short-lived radioisotope of barium which was isolated from neutron irradiated uranium (139Ba, with a half-life of 83 minutes and 140Ba, with a half-life of 12.8 days, are major fission products of uranium). At the time, it was thought that this was a new radium isotope, as it was then standard radiochemical practice to use a barium sulfate carrier precipitate to assist in the isolation of radium.\nPUREX\nThe PUREX process is a liquid–liquid extraction ion-exchange method used to reprocess spent nuclear fuel, in order to extract primarily uranium and plutonium, independent of each other, from the other constituents. The current method of choice is to use the PUREX liquid–liquid extraction process which uses a tributyl phosphate/hydrocarbon mixture to extract both uranium and plutonium from nitric acid. This extraction is of the nitrate salts and is classed as being of a solvation mechanism. For example, the extraction of plutonium by an extraction agent (S) in a nitrate medium occurs by the following reaction.\n\nPu4+(aq) + 4 NO−3(aq) + 2 S(organic) → [Pu(NO3)4S2](organic)\nA complex bond is formed between the metal cation, the nitrates and the tributyl phosphate, and a model compound of a dioxouranium(VI) complex with two nitrates and two triethyl phosphates has been characterised by X-ray crystallography. After the dissolution step it is normal to remove the fine insoluble solids, because otherwise they will disturb the solvent extraction process by altering the liquid-liquid interface. It is known that the presence of a fine solid can stabilize an emulsion. Emulsions are often referred to as third phases in the solvent extraction community.\nAn organic solvent composed of 30% tributyl phosphate (TBP) in a hydrocarbon solvent, such as kerosene, is used to extract the uranium as UO2(NO3)2·2TBP complexes, and plutonium as similar complexes, from other fission products, which remain in the aqueous phase. The transuranium elements americium and curium also remain in the aqueous phase. The nature of the organic soluble uranium complex has been the subject of some research. A series of complexes of uranium with ",
    "source": "wikipedia",
    "title": "Actinide chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_1839",
    "text": "Allotropy or allotropism (from Ancient Greek  ἄλλος (allos) 'other' and  τρόπος (tropos) 'manner, form') is the property of some chemical elements to exist in two or more different forms, in the same physical state, known as allotropes of the elements. Allotropes are different structural modifications of an element: the atoms of the element are bonded together in different manners.\nFor example, the allotropes of carbon include diamond (the carbon atoms are bonded together to form a cubic lattice of tetrahedra), graphite (the carbon atoms are bonded together in sheets of a hexagonal lattice), graphene (single sheets of graphite), and fullerenes (the carbon atoms are bonded together in spherical, tubular, or ellipsoidal formations).\nThe term allotropy is used for elements only, not for compounds. The more general term, used for any compound, is polymorphism, although its use is usually restricted to solid materials such as crystals. Allotropy refers only to different forms of an element within the same physical phase (the state of matter, i.e. plasmas, gases, liquids, or solids). The differences between these states of matter would not alone constitute examples of allotropy. Allotropes of chemical elements are frequently referred to as polymorphs or as phases of the element.\nFor some elements, allotropes have different molecular formulae or different crystalline structures, as well as a difference in physical phase; for example, two allotropes of oxygen (dioxygen, O2, and ozone, O3) can both exist in the solid, liquid and gaseous states. Other elements do not maintain distinct allotropes in different physical phases; for example, phosphorus has numerous solid allotropes, which all revert to the same P4 form when melted to the liquid state.\n\nHistory\nThe concept of allotropy was originally proposed in 1840 by the Swedish scientist Baron Jöns Jakob Berzelius (1779–1848). The term is derived from Greek  άλλοτροπἱα (allotropia) 'variability, changeableness'. After the acceptance of Avogadro's hypothesis in 1860, it was understood that elements could exist as polyatomic molecules, and two allotropes of oxygen were recognized as O2 and O3. In the early 20th century, it was recognized that other cases such as carbon were due to differences in crystal structure.\nBy 1912, Ostwald noted that the allotropy of elements is just a special case of the phenomenon of polymorphism known for compounds, and proposed that the terms allotrope and allotropy be abandoned and replaced by polymorph and polymorphism. Although many other chemists have repeated this advice, IUPAC and most chemistry texts still favour the usage of allotrope and allotropy for elements only.\n",
    "source": "wikipedia",
    "title": "Allotropy",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_1187",
    "text": "An alloy is a mixture of chemical elements of which in most cases at least one is a metallic element, although it is also sometimes used for mixtures of elements; herein only metallic alloys are described. Metallic alloys often have properties that differ from those of the pure elements from which they are made. The vast majority of metals used for commercial purposes are alloyed to improve their properties or behavior, such as increased strength, hardness or corrosion resistance. Metals may also be alloyed to reduce their overall cost, for instance alloys of gold and copper.\nIn an alloy, the atoms are joined by metallic bonding rather than by covalent bonds typically found in chemical compounds. The alloy constituents are usually measured by mass percentage for practical applications, and in atomic fraction for basic science studies. Alloys are usually classified as substitutional or interstitial alloys, depending on the atomic arrangement that forms the alloy. They can be further classified as homogeneous (consisting of a single phase), or heterogeneous (consisting of two or more phases) or intermetallic. An alloy may be a solid solution of metal elements (a single phase, where all metallic grains (crystals) are of the same composition) or a mixture of metallic phases (two or more solutions, forming a microstructure of different crystals within the metal).\nExamples of alloys include red gold (gold and copper), white gold (gold and silver), sterling silver (silver and copper), steel or silicon steel (iron with non-metallic carbon or silicon respectively), solder, brass, pewter, duralumin, bronze, and amalgams. Alloys are used in a wide variety of applications, from the steel alloys, used in everything from buildings to automobiles to surgical tools, to exotic titanium alloys used in the aerospace industry, to beryllium-copper alloys for non-sparking tools.\n\nCharacteristics\nAn alloy is a mixture of chemical elements which forms an impure substance (admixture) that retains the characteristics of a metal. Alloys are made by mixing two or more elements, at least one of which is a metal. This is usually called the primary metal or the base metal, and the name of this metal may also be the name of the alloy. The other constituents may or may not be metals but, when mixed with the molten base, they will be soluble and dissolve into the mixture.\nThe mechanical properties of alloys will often be quite different from those of its individual constituents. A metal that is normally very soft (malleable), such as aluminium, can be altered by alloying it with another soft metal, such as copper. Although both metals are very soft and ductile, the resulting aluminium–copper alloy will have much greater strength. Adding a small amount of non-metallic carbon to iron trades its great ductility for the greater strength of an alloy called steel. Due to its very-high strength, but still substantial toughness, and its ability to be greatly altered by heat treatment, st",
    "source": "wikipedia",
    "title": "Alloy",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_18504176",
    "text": "Amateur chemistry or home chemistry is the pursuit of chemistry as a private hobby. Amateur chemistry is usually done with whatever chemicals are available at disposal at the privacy of one's home. It should not be confused with clandestine chemistry, which involves the illicit production of controlled drugs.[a] Notable amateur chemists include Oliver Sacks and Sir Edward Elgar.\n\nHistory\n\nNotable amateur chemists\nInternet pioneer Vint Cerf, Intel co-founder Gordon Moore, and Hewlett Packard co-founder David Packard all used to practice amateur chemistry.\nBritish neurologist Oliver Sacks was a keen amateur chemist in his youth, as described in his memoir Uncle Tungsten: Memories of a Chemical Boyhood.\nNobel Prize winning chemist Linus Pauling practised amateur chemistry in his youth.\nWolfram Research co-founder Theodore Gray is a keen amateur chemist and element collector.  His exploits (most notably the construction of a wooden table in the shape of the periodic table, having compartments holding real samples of each element) earned him the 2002 Ig Nobel prize for chemistry, which he accepted as a great honor.  He writes a column for Popular Science magazine, featuring his home experiments.\nAmateur rocketeer (and later NASA engineer) Homer Hickham, together with his fellow Rocket Boys, experimented with a range of home-made rocket propellants.  These included \"Rocket Candy\" made from potassium nitrate and sugar, and \"Zincoshine\" made from zinc and sulfur held together with moonshine alcohol.\nComposer Sir Edward Elgar practised amateur chemistry from a laboratory erected in his back garden. The original manuscript of the prelude to The Kingdom is stained with chemicals.\nRobert Boyle is largely regarded today as the first modern chemist, and therefore one of the founders of modern chemistry, and one of the pioneers of modern experimental scientific method.\nMaurice Ward, a hairdresser and amateur chemist who invented the thermal insulating material called Starlite.\nRobert Cornelius, inventor, businessman and lamp manufacturer credited for creating the first photographic self-portrait in 1839.\n",
    "source": "wikipedia",
    "title": "Amateur chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_39488712",
    "text": "The Arens–van Dorp synthesis is a name reaction in organic chemistry. It describes the addition of lithiated ethoxyacetylenes to ketones to give propargyl alcohols, which can undergo further reaction to form α,β-unsaturated aldehydes, or esters. There is also a variation of this reaction called the Isler modification, where the acetylide anion is generated in situ from β-chlorovinyl ether using lithium amide.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Arens–van Dorp synthesis",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_71268",
    "text": "Astrochemistry is the interdisciplinary scientific study of the abundance and reactions of molecules in space and their interaction with radiation. The discipline overlaps with astronomy and chemistry. The term may refer to studies within both the Solar System and the interstellar medium. The investigation of elemental abundances and isotope ratios in Solar System materials, such as meteorites, is known as cosmochemistry, while the study of interstellar atoms and molecules and their interaction with radiation is sometimes called molecular astrophysics. The formation, composition, evolution and fate of molecular clouds is of particular interest, as these clouds are the birthplaces of planetary systems.\n\nHistory\nAs an offshoot of astronomy and chemistry, the history of astrochemistry follows the development of both fields. Advances in observational and experimental spectroscopy enabled the detection of an ever‑growing range of molecules within planetary systems and the surrounding interstellar medium. The expanding inventory of detected species, made possible by improvements in spectroscopy and related technologies, has in turn broadened the chemical space accessible to astrochemical research.\nSpectroscopy\nOne of the most important experimental tools in astrochemistry is spectroscopy, which uses telescopes to measure the absorption and emission of light from atoms and molecules in different astrophysical environments. By comparing astronomical observations with laboratory spectra, astrochemists can infer the elemental abundances, chemical composition and temperatures of stars and interstellar clouds. This is possible because ions, atoms and molecules have characteristic spectra: that is, they absorb and emit light at specific wavelengths, many of which are not visible to the human eye. Different regions of the electromagnetic spectrum (radio, infrared, visible, ultraviolet and others) probe different types of transitions and are therefore sensitive to different kinds of species.\n",
    "source": "wikipedia",
    "title": "Astrochemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_706999",
    "text": "Atmospheric chemistry is a branch of atmospheric science that studies the chemistry of the Earth's atmosphere and that of other planets. This multidisciplinary approach of research draws on environmental chemistry, physics, meteorology, computer modeling, oceanography, geology and volcanology, climatology and other disciplines to understand both natural and human-induced changes in atmospheric composition. Key areas of research include the behavior of trace gasses, the formation of pollutants, and the role of aerosols and greenhouse gasses. Through a combination of observations, laboratory experiments, and computer modeling, atmospheric chemists investigate the causes and consequences of atmospheric changes.\n\nAtmospheric composition\nThe composition and chemistry of the Earth's atmosphere is important for several reasons, but primarily because of the interactions between the atmosphere and living organisms. Natural processes such as volcano emissions, lightning and bombardment by solar particles from corona changes the composition of the Earth's atmosphere. It has also been changed by human activity and some of these changes are harmful to human health, crops and ecosystems.\nHistory\nThe first scientific studies of atmospheric composition began in the 18th century when chemists such as Joseph Priestley, Antoine Lavoisier and Henry Cavendish made the first measurements of the composition of the atmosphere.\nIn the late 19th and early 20th centuries, researchers shifted their interest towards trace constituents with very low concentrations. An important finding from this era was the discovery of ozone by Christian Friedrich Schönbein in 1840.\nIn the 20th century atmospheric science moved from studying the composition of air to consider how the concentrations of trace gasses in the atmosphere have changed over time and the chemical processes which create and destroy compounds in the air. Two important outcomes were the explanation by Sydney Chapman and Gordon Dobson of how the ozone layer is created and maintained, and Arie Jan Haagen-Smit’s explanation of photochemical smog. Further studies on ozone issues led to the 1995 Nobel Prize in Chemistry award shared between Paul Crutzen, Mario Molina and Frank Sherwood Rowland. \n\nIn the 21st century the focus is now shifting again. Instead of concentrating on atmospheric chemistry in isolation, it is now seen as one part of the Earth system with the rest of the atmosphere, biosphere and geosphere. A driving force for this link is the relationship between chemistry and climate. The changing climate and the recovery of the ozone hole and the interaction of the composition of the atmosphere with the oceans and terrestrial ecosystems are examples of the interdependent relationships between Earth's systems. A new field of extraterrestrial atmospheric chemistry has also recently emerged. Astrochemists analyze the atmospheric compositions of the Solar System and exoplanets to determine the formation of astronomical",
    "source": "wikipedia",
    "title": "Atmospheric chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_902",
    "text": "Atoms are the basic particles of the chemical elements and the fundamental building blocks of matter. An atom consists of a nucleus of protons and generally neutrons, surrounded by an electromagnetically bound swarm of electrons.  The chemical elements are distinguished from each other by the number of protons that are in their atoms. For example, any atom that contains 11 protons is sodium, and any atom that contains 29 protons is copper. Atoms with the same number of protons but a different number of neutrons are called isotopes of the same element.\nAtoms are extremely small, typically around 100 picometers across. A human hair is about a million carbon atoms wide. Atoms are smaller than the shortest wavelength of visible light, which means humans cannot see atoms with conventional microscopes. They are so small that accurately predicting their behavior using classical physics is not possible due to quantum effects.\nMore than 99.94% of an atom's mass is in the nucleus. Protons have a positive electric charge and neutrons have no charge, so the nucleus is positively charged. The electrons are negatively charged, and this opposing charge is what binds them to the nucleus. If the numbers of protons and electrons are equal, as they normally are, then the atom is electrically neutral as a whole. A charged atom is called an ion. If an atom has more electrons than protons, then it has an overall negative charge and is called a negative ion (or anion). Conversely, if it has more protons than electrons, it has a positive charge and is called a positive ion (or cation).\nThe electrons of an atom are attracted to the protons in an atomic nucleus by the electromagnetic force. The protons and neutrons in the nucleus are attracted to each other by the nuclear force. This force is usually stronger than the electromagnetic force that repels the positively charged protons from one another. Under certain circumstances, the repelling electromagnetic force becomes stronger than the nuclear force. In this case, the nucleus splits and leaves behind different elements. This is a form of nuclear decay.\nAtoms can attach to one or more other atoms by chemical bonds to form chemical compounds such as molecules or crystals. The ability of atoms to attach and detach from each other is responsible for most of the physical changes observed in nature. Chemistry is the science that studies these changes.\n\nHistory of atomic theory\n\n",
    "source": "wikipedia",
    "title": "Atom",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_62904359",
    "text": "Biliproteins are pigment protein compounds that are located in photosynthesising organisms such as algae, and sometimes also in certain insects. They refer to any protein that contains a bilin chromophore. In plants and algae, the main function of biliproteins is to make the process of light accumulation required for photosynthesis more efficient; while in insects they play a role in growth and development. Some of their properties: including light-receptivity, light-harvesting and fluorescence have made them suitable for applications in bioimaging and as indicators; while other properties such as anti-oxidation, anti-aging and anti-inflammation in phycobiliproteins have given them potential for use in medicine, cosmetics and food technology. While research on biliproteins dates back as far as 1950, it was hindered due to issues regarding biliprotein structure, lack of methods available for isolating individual biliprotein components, as well as limited information on lyase reactions (which are needed to join proteins with their chromophores). Research on biliproteins has also been primarily focused on phycobiliproteins; but advances in technology and methodology, along with the discovery of different types of lyases, has renewed interest in biliprotein research, allowing new opportunities for investigating biliprotein processes such as assembly/disassembly and protein folding.\n\nFunctions\n\nStructure\nThe structure of biliproteins is typically characterised by bilin chromophores arranged in linear tetrapyrrolic formation, and the bilins are covalently bound to apoproteins via thioether bonds. Each type of biliprotein has a unique bilin that belongs to it (e.g. phycoerythrobilin is the chromophore of phycoerythrin and phycocyanobilin is the chromophore of phycocyanin).  The bilin chromophores are formed by the oxidative cleavage of a haem ring and catalysed by haem oxygenases at one of four methine bridges, allowing four possible bilin isomers to occur.  In all organisms known to have biliproteins, cleavage usually occurs at the α-bridge, generating biliverdin IXα.\nPhycobiliproteins are grouped together in separate clusters, approximately 40nm in diameter, known as phycobilisomes.  The structural changes involved in deriving bilins from their biliverdin IXα isomer determine the spectral range of light absorption.\nThe structure of biliproteins in insects differ slightly than those in plants and algae; they have a crystal structure and their chromophores are not covalently bound to the apoproteins. Unlike phycobiliproteins whose chromophores are held in an extended arrangement by specific interactions between chromophores and proteins, the chromophore in insect biliproteins has a cyclic helical crystal structure in the protein-bound state, as found in studies of the biliprotein extracted from the large white butterfly.\n",
    "source": "wikipedia",
    "title": "Biliprotein",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_35741423",
    "text": "In aquatic toxicology, bioconcentration is the accumulation of a water-borne chemical substance in an organism exposed to the water.\nThere are several ways in which to measure and assess bioaccumulation and bioconcentration.  These include: octanol-water partition coefficients (KOW), bioconcentration factors (BCF), bioaccumulation factors (BAF) and biota-sediment accumulation factor (BSAF).  Each of these can be calculated using either empirical data or measurements, as well as from mathematical models. One of these mathematical models is a fugacity-based BCF model developed by Don Mackay.\nBioconcentration factor can also be expressed as the ratio of the concentration of a chemical in an organism to the concentration of the chemical in the surrounding environment. The BCF is a measure of the extent of chemical sharing between an organism and the surrounding environment.\nIn surface water, the BCF is the ratio of a chemical's concentration in an organism to the chemical's aqueous concentration.  BCF is often expressed in units of liter per kilogram (ratio of mg of chemical per kg of organism to mg of chemical per liter of water). BCF can simply be an observed ratio, or it can be the prediction of a partitioning model. A partitioning model is based on assumptions that chemicals partition between water and aquatic organisms as well as the idea that chemical equilibrium exists between the organisms and the aquatic environment in which it is found\n\nCalculation\nBioconcentration can be described by a bioconcentration factor (BCF), which is the ratio of the chemical concentration in an organism or biota to the concentration in water:\n\n  \n    \n      \n        B\n        C\n        F\n        =\n        \n          \n            \n              C\n              o\n              n\n              c\n              e\n              n\n              t\n              r\n              a\n              t\n              i\n              o\n              \n                n\n                \n                  B\n                  i\n                  o\n                  t\n                  a\n                \n              \n            \n            \n              C\n              o\n              n\n              c\n              e\n              n\n              t\n              r\n              a\n              t\n              i\n              o\n              \n                n\n                \n                  W\n                  a\n                  t\n                  e\n                  r\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle BCF={\\frac {Concentration_{Biota}}{Concentration_{Water}}}}\n  \n\nBioconcentration factors can also be related to the octanol-water partition coefficient, Kow. The octanol-water partition coefficient (Kow) is correlated with the potential for a chemical to bioaccumulate in organisms; the BCF can be predicted from log Kow, via computer programs based on structure activity relationship (SAR) or through the linear equatio",
    "source": "wikipedia",
    "title": "Bioconcentration",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_24568072",
    "text": "Biophysical chemistry is a physical science that uses the concepts of physics and physical chemistry for the study of biological systems. The most common feature of the research in this subject is to seek an explanation of the various phenomena in biological systems in terms of either the molecules that make up the system or the supra-molecular structure of these systems. Apart from the biological applications, recent research showed progress in the medical field as well.\n\nHistory\n\nTechniques\nBiophysical chemists employ various techniques used in physical chemistry to probe the structure of biological systems. These techniques include spectroscopic methods such as nuclear magnetic resonance (NMR) and other techniques like X-ray diffraction and cryo-electron microscopy. An example of research in biophysical chemistry includes the work for which the 2009 Nobel Prize in Chemistry was awarded. The prize was based on X-ray crystallographic studies of the ribosome that helped to unravel the physical basis of its biological function as a molecular machine that translates mRNA into polypeptides. Other areas in which biophysical chemists engage themselves are protein structure and the functional structure of cell membranes. For example, enzyme action can be explained in terms of the shape of a pocket in the protein molecule that matches the shape of the substrate molecule or its modification due to binding of a metal ion. The structures of many large protein assemblies, such as ATP synthase, also exhibit machine-like dynamics as they act on their substrates. Similarly, the structure and function of the biomembranes may be understood through the study of model supramolecular structures as liposomes or phospholipid vesicles of different compositions and sizes.\nApplications\nThere are several biological and medical applications that apply the knowledge of biophysical chemistry to benefit humankind.\nInstitutes\nThe oldest reputed institute for biophysical chemistry is the Max Planck Institute for Biophysical Chemistry in Göttingen.\n",
    "source": "wikipedia",
    "title": "Biophysical chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_13464959",
    "text": "Bittern (pl. bitterns), or nigari, is the salt solution formed when halite (table salt) precipitates from seawater or brines. Bitterns contain magnesium, calcium, and potassium ions as well as chloride, sulfate, iodide, and other ions.\nBittern is commonly formed in salt ponds where the evaporation of water prompts the precipitation of halite. These salt ponds can be part of a salt-producing industrial facility, or they can be used as a waste storage location for brines produced in desalination processes.\nBittern is a source of many useful salts. It is used as a natural source of Mg2+, and it can be used as a coagulant both in the production of tofu and in the treatment of industrial wastewater.\n\nHistory\nBittern has been extracted for a long time, at least several centuries. The Dutch chemist Petrus Jacobus Kipp (1808–1864) experimented with saturated solutions of bittern. The term for the solution is a modification of \"bitter\".\nUses\n\nEnvironmental impact\nIn some jurisdictions, most bitterns are used for other production instead of being directly discarded.  In other jurisdictions each tonne of salt produced can create 3+ tonnes of waste bitterns.\nAlthough bittern generally contains the same compounds as seawater, it is much more concentrated than seawater. If bittern is released directly into seawater, the ensuing salinity increase may harm marine life around the point of release. Even small increases in salinity can disrupt marine species' osmotic balances, which may result in the death of the organism in some cases.\nIn December 1997, 94 corpses of green sea turtles, Chelonia mydas, were found at the Ojo de Liebre Lagoon (OLL) in Mexico, adjacent to the industrial operation of Exportadora de Sal S.A. (ESSA), the largest saltworks in the world. The fluoride ion F− content in bitterns was 60.5-fold more than that in seawater. The bitterns osmolality was 11,000 mosm/kg of water, whereas the turtle's plasma osmolality was about 400 mosm/kg of water. Researchers concluded that the dumping of bitterns into the ocean should be avoided.\n\nThe lack of adequate disposal methods for bitterns and concerns of local commercial and recreational fishing associations about bitterns’ deleterious impacts upon local fish and prawn hatchery areas led the Western Australian EPA in 2008 to recommend against the proposed 4.2 million tonne per annum Straits Salt project in The Pilbara region of WA.  The EPA concluded that: ...the proposed solar salt farm is located in an area that presents unacceptably high risks of environmental harm to wetland values and unacceptable levels of uncertainty in relation to long term management of bitterns. [...] A high level of uncertainty in relation to the proponent’s ability to manage the ongoing production of over 1 million cubic metres per annum of bitterns C, which is toxic to marine biota and therefore likely to degrade wetland and biodiversity values should bitterns discharge occur either accidentally or be required to maintain sa",
    "source": "wikipedia",
    "title": "Bittern (salt)",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_52050131",
    "text": "Building block is a term in chemistry which is used to describe a virtual molecular fragment or a real chemical compound the molecules of which possess reactive functional groups. Building blocks are used for bottom-up modular assembly of molecular architectures: nano-particles, metal-organic frameworks, organic molecular constructs, supra-molecular complexes. Using building blocks ensures strict control of what a final compound or a (supra)molecular construct will be.\n\nBuilding blocks for medicinal chemistry\nIn medicinal chemistry, the term defines either imaginable, virtual molecular fragments or chemical reagents from which drugs or drug candidates might be constructed or synthetically prepared.\nExamples\nTypical examples of building block collections for medicinal chemistry are libraries of fluorine-containing building blocks. Introduction of the fluorine into a molecule has been shown to be beneficial for its pharmacokinetic and pharmacodynamic properties, therefore, the fluorine-substituted building blocks in drug design increase the probability of finding drug leads. Other examples include natural and unnatural amino acid libraries, collections of conformationally constrained bifunctionalized compounds and diversity-oriented  building block collections.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Building block (chemistry)",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_58076521",
    "text": "C1 chemistry is the chemistry of one-carbon molecules.  Although many compounds and ions contain only one carbon, stable and abundant C-1 feedstocks are the focus of research.  Four compounds are of major industrial importance: methane, carbon monoxide, carbon dioxide, and methanol.  Technologies that interconvert these species are often used massively to match supply to demand.\n\nIndustrial processes\nCarbon monoxide and methanol are important chemical feedstocks.  CO is utilized by myriad carbonylation reactions.  Together with hydrogen, it is the feed for the Fischer–Tropsch process, which affords liquid fuels.  Methanol is the precursor to acetic acid, dimethyl ether, formaldehyde, and many methyl compounds (esters, amines, halides). A larger-scale application is methanol to olefins, which produces ethylene and propylene.\nIn contrast to carbon monoxide and methanol, methane and carbon dioxide have limited uses as feedstocks for chemicals and fuels.  This disparity contrasts with the relative abundance of methane and carbon dioxide.  Methane is often partially converted to carbon monoxide for utilization in Fischer-Tropsch processes. Of interest for upgrading methane is its oxidative coupling:\n\n2CH4 + O2 → C2H4 + 2H2O\nConversion of carbon dioxide to unsaturated hydrocarbons via electrochemical reduction is a hopeful avenue of research. Still, no stable and economic technology has yet been developed.\nBiochemistry\n\nMethane, carbon monoxide, carbon dioxide, and methanol are substrates and products of enzymatic processes.  \nIn methanogenesis, carbon monoxide, carbon dioxide, and methanol are converted to methane. Methanogenesis by methanogenic archaea is reversible.\nIn photosynthesis, carbon dioxide and water are converted to sugars (and O2), the energy for this (thermally) uphill reaction being provided by sunlight.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "C1 chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_63498286",
    "text": "Calconcarboxylic acid (IUPAC name 3-hydroxy-4-[(2-hydroxy-4-sulfonaphthalen-1-yl)diazenyl]naphthalene-2-carboxylic acid; commonly called Patton and Reeder's Indicator) is an azo dye that is used as an indicator for complexometric titrations of calcium with ethylenediaminetetraacetic acid (EDTA) in the presence of magnesium. Structurally, it is similar to eriochrome blue black R, which is obtained from calconcarboxylic acid  by decarboxylation and reaction with sodium hydroxide.\n\nProperties\nCalconcarboxlic acid is soluble in water and a variety of other solvents, including sodium hydroxide, ethanol and methanol. It has a violet colour in dissolved form in ethanol. The melting point of calconcarboxylic acid is at approximately 300 °C, where it undergoes thermal decomposition.\nBackground\nThough the determination of calcium and magnesium by complexometric titration with standard solutions of disodium dihydrogen tetraacetate, utilising Eriochrome Black T as indicator is widely accepted and quite adequately understood, it, like other complexometric titration methods, suffers from the limitations of having an indistinct endpoint (where a photometric titrator is needed to provide acceptable accuracy) and/or having to separate the metals before titration can occur. Calconcarboxylic acid was thus adopted as a superior alternative due to its ability to give a good and visual endpoint and its rapid performance even with the presence of magnesium.\nSynthesis\nAs described by James Patton and Wendell Reeder in 1956, calconcarboxylic acid can be synthesised by coupling diazotized 1-amino-2-naphthol-4-sulfonic acid with 2-hydroxy-3-napthoic acid.\nApplications\nCalconcarboxylic acid is used for the determination of calcium ion concentration by complexometric titration. Free calconcarboxylic acid is blue colour, but changes to pink/red when it forms a complex with calcium ions. EDTA forms a more stable complex with calcium than calconcarboxylic acid does, so addition of EDTA to the Ca–calconcarboxylic acid complex causes formation of Ca-EDTA instead, leading to reversion to the blue colour of free calconcarboxylic acid.\nFor the complexometric titration, the indicator is first added to the titrant containing the calcium ions to form the calcium ion-indicator complex (Ca-PR) with a pink/red colour. This is then titrated against a standard solution of EDTA. The endpoint can be observed when the indicator produces a sharp, stable colour change from wine red to pure blue, which occurs at pH values between 12 and 14, this indicates the endpoint of the titration, as the Ca-PR complexes have been completely replaced by the Ca-EDTA complexes and hence the PR indicator reverts to its blue colour.\nThe reaction can be given by:\n\nCa-PR + EDTA4- → PR + [Ca-EDTA]2-\nThe Patton-Reeder Indicator is often used here in the form of a triturate.\nThis method of complexometric titration is dependent on the pH of the solution being sufficiently high to ensure that magnesium ions precipitate ",
    "source": "wikipedia",
    "title": "Calconcarboxylic acid",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_81646131",
    "text": "Carbodiphosphoranes are a class of organophosphorus compounds and a subtype of carbones with general formula C(PR3)2, consisting of a central carbon atom bound to two phosphine ligands by dative bonds. The central carbon atom has formal oxidation state zero and two high-energy lone pairs with σ and π-symmetry, making carbodiphosphoranes highly nucleophilic and strongly σ- and π-donating through the carbon atom. Carbodiphosphoranes have gained interest for their unique double-donating properties, and have been used as ligands in a number of main group and transition metal complexes with applications in catalysis. Carbodiphosphoranes generally have a bent molecular geometry, but observed P–C–P bond angles range from 100° to 180°.\nThe term “carbodiphosphorane” can also refer to hexaphenylcarbodiphosphorane, the most common carbodiphosphorane used in chemistry and the first species synthesized.\n\nStructure and Bonding\nThe electronic structure of carbodiphosphoranes was initially proposed alongside their original synthesis in 1961 to be a resonance hybrid of a double-bonded species isoelectronic to carbodiimides and a bisylide with the central carbon atom having a formal charge of -2. Kaska et al. in 1971 proposed an additional coordinatively unsaturated structure with two dative bonds from the phosphorus lone pairs and all four valence electrons of carbon unengaged in bonding, which has since been confirmed as the major resonance contributor to carbodiphosphoranes by computational studies. In 1976, the earliest synthesis of a geminal dimetallated carbodiphosphorane species was reported by Schmidbaur et al., demonstrating for the first time their ability to bind two metals at the central carbon, which provided experimental evidence consistent with the bisylide and zero-valent resonance structures. \n\nThe electronic structure of the central carbon atom can be compared to the central carbon of N-heterocyclic carbenes, which have a σ-symmetric lone pair as the HOMO and an unoccupied π-symmetric orbital as the LUMO, making them good σ-donors and π-acceptors. In contrast, carbodiphosphoranes have two lone pairs, the π-symmetric HOMO and the σ-symmetric HOMO-1, and no carbon-centered electrons engaged in bonding. For this reason, carbodiphosphoranes are more strongly Lewis basic than carbenes and have stronger electron-donating ability, which has been evaluated by the Tolman Electronic Parameter (TEP). TEP can also be used to compare relative donor strengths between carbodiphosphoranes, which differ based on the nature of the phosphine. Because of the presence of the lone pairs, the molecular geometry is typically bent, with common P–C–P bond angles being around 120°–145°. However, a bond angle of 180° has been observed in hexaphenylcarbodiphosphorane under some crystallization conditions, instead of its typical 136.9° bond angle. This linear structure is unique among carbones, and the authors attribute this structure to the low energy difference of 3.1 kcal/",
    "source": "wikipedia",
    "title": "Carbodiphosphoranes",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_12502695",
    "text": "The carryover effect is a term used in clinical chemistry to describe the transfer of unwanted material from one container or mixture to another. It describes the influence of one sample upon the following one. It may be from a specimen, or a reagent, or even the washing medium. The significance of carry over is that even a small amount can  lead to erroneous results.\n\nCarryover effect in clinical laboratory\nCarryover experiments are widely used for clinical chemistry and immunochemistry analyzers to evaluate and validate carryover effects.  The pipetting and washing systems in an automated analyzer are designed to continuously cycle between the aspiration of patient specimens and cleaning. An obvious concern is a potential for carryover of analyte from one patient specimen into one or more following patient specimens, which can falsely increase or decrease the measured analyte concentration. Specimen carryover is typically addressed by judicious choice of probe material, probe design, and an efficient probe washing system to flush the probe of residual patient specimens or reagents retained in their bores or clinging to the probe exterior surface before they are introduced into the next patient sample, reagent container, or cuvette/reaction vessel.\nSignificance in carryover assessment\nThe pathological range of measurement could be of several order to reference interval(e.g., Sex hormone, Tumor marker, Troponin...etc.).  A small portion of carryover could lead to erroneous results.\nCarryover assessment\n\nIUPAC made a recommendation in 1991 for the description and measurement of carryover effects in clinical chemistry.  The carryover ratio is the percentage of H3 carry to L1 constituting the carryover portion \"h\".  In a design of 3 high samples followed by 3 low samples, h can be calculated as (L1 - mean of L2&L3) / (H3 - mean of L2&L3)\nThe carry-over ratio's acceptance criteria depend on the measurement and the laboratory concerned.  For example, 1% carryover of plasma albumin would generally lead to a clinically insignificant effect, while 1% carryover of cardiac High sensitivity Troponin assay would be catastrophic.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Carryover effect",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_11092324",
    "text": "Chemistry is often called the central science because of its role in connecting the physical sciences, which include chemistry, with the life sciences, pharmaceutical sciences and applied sciences such as medicine and engineering. The nature of this relationship is one of the main topics in the philosophy of chemistry and in scientometrics. The phrase was popularized by its use in a textbook by Theodore L. Brown and H. Eugene LeMay, titled Chemistry: The Central Science, which was first published in 1977, with a fifteenth edition published in 2021.\nThe central role of chemistry can be seen in the systematic and hierarchical classification of the sciences by Auguste Comte. Each discipline provides a more general framework for the area it precedes (mathematics → astronomy → physics → chemistry → biology → social sciences). Balaban and Klein have more recently proposed a diagram showing the partial ordering of sciences in which chemistry may be argued is \"the central science\" since it provides a significant degree of branching. In forming these connections the lower field cannot be fully reduced to the higher ones. It is recognized that the lower fields possess emergent ideas and concepts that do not exist in the higher fields of science.\nThus chemistry is built on an understanding of laws of physics that govern particles such as atoms, protons, neutrons, electrons, thermodynamics, etc. although it has been shown that it has not been \"fully 'reduced' to quantum mechanics\". Concepts such as the periodicity of the elements and chemical bonds in chemistry are emergent in that they are more than the underlying forces defined by physics.\nIn the same way, biology cannot be fully reduced to chemistry, although the machinery that is responsible for life is composed of molecules. For instance, the machinery of evolution may be described in terms of chemistry by the understanding that it is a mutation in the order of genetic base pairs in the DNA of an organism. However, chemistry cannot fully describe the process since it does not contain concepts such as natural selection that are responsible for driving evolution. Chemistry is fundamental to biology since it provides a methodology for studying and understanding the molecules that compose cells.\nConnections made by chemistry are formed through various sub-disciplines that utilize concepts from multiple scientific disciplines. Chemistry and physics are both needed in the areas of physical chemistry, nuclear chemistry, and theoretical chemistry. Chemistry and biology intersect in the areas of biochemistry, medicinal chemistry, molecular biology, chemical biology, molecular genetics, and immunochemistry. Chemistry and the earth sciences intersect in areas like geochemistry and hydrology.\n\nSee also\nFundamental science\nHard and soft science\nPhilosophy of chemistry\nSpecial sciences\nUnity of science\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "The central science",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_47856613",
    "text": "The charged aerosol detector (CAD) is a detector used in conjunction with high-performance liquid chromatography (HPLC) and ultra high-performance liquid chromatography (UHPLC) to measure the amount of chemicals in a sample by creating charged aerosol particles which are detected using an electrometer. It is commonly used for the analysis of compounds that cannot be detected using traditional UV/Vis approaches due to their lack of a chromophore. The CAD can measure all non-volatile and many semi-volatile analytes including, but not limited to, antibiotics, excipients, ions, lipids, natural products, biofuels, sugars and surfactants. The CAD, like other aerosol detectors (e.g., evaporative light scattering detectors (ELSD) and condensation nucleation light scattering detectors (CNLSD)), falls under the category of destructive general-purpose detectors (see Chromatography detectors).\n\nHistory\nThe predecessor to the CAD, termed an evaporative electrical detector, was first described by Kaufman in 2002 at TSI Inc in US patent 6,568,245 and was based on the coupling of liquid chromatographic approaches to TSI's electrical aerosol measurement (EAM) technology.  At around the same time Dixon and Peterson at California State University were investigating the coupling of liquid chromatography to an earlier version of TSI's EAM technology, which they called an aerosol charge detector. Subsequent collaboration between TSI and ESA Biosciences Inc. (now part of Thermo Fisher Scientific), led to the first commercial instrument, the Corona CAD, which received both the Pittsburgh Conference Silver Pittcon Editor's Award (2005) and R&D 100 award (2005). Continued research and engineering improvements in product design resulted in CADs with ever increasing capabilities. The newest iterations of the CAD are the Thermo Scientific Corona Veo Charged Aerosol Detector, Corona Veo RS Charged Aerosol Detector and Thermo Scientific Vanquish Charged Aerosol Detectors.\nPrinciples of operation\nThe general detection scheme  involves:\n\nPneumatic nebulization of mobile phase from the analytical column forming an aerosol.\nAerosol conditioning to remove large droplets.\nEvaporation of solvent from the droplets to form dried particles.\nParticle charging using an ion jet formed via corona discharge.\nParticle selection – an ion trap is used to excess ions and high mobility charged particles.\nMeasurement of the aggregate charge of aerosol particles using a filter/electrometer.\nThe CAD like other aerosol detectors, can only be used with volatile mobile phases. For an analyte to be detected it must be less volatile than the mobile phase.\nMore detailed information on how CAD works can be found on the Charged Aerosol Detection for Liquid Chromatography Resource Center.\n",
    "source": "wikipedia",
    "title": "Charged aerosol detector",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_28254396",
    "text": "Chemical bath deposition, also called chemical solution deposition and CBD, is a method of thin-film deposition (solids forming from a solution or gas), using an aqueous precursor solution. Chemical bath deposition typically forms films using heterogeneous nucleation (deposition or adsorption of aqueous ions onto a solid substrate), to form homogeneous thin films of metal chalcogenides (mostly oxides, sulfides, and selenides) and many less common ionic compounds. Chemical bath deposition produces films reliably, using a simple process with little infrastructure, at low temperature (<100 ˚C), and at low cost. Furthermore, chemical bath deposition can be employed for large-area batch processing or continuous deposition. Films produced by CBD are often used in semiconductors, photovoltaic cells, and supercapacitors, and there is increasing interest in using chemical bath deposition to create nanomaterials.\n\nUses\nChemical bath deposition is useful in industrial applications because it is extremely cheap, simple, and reliable compared to other methods of thin-film deposition, requiring only aqueous solution at (relatively) low temperatures and minimal infrastructure. The chemical bath deposition process can easily be scaled up to large-area batch processing or continuous deposition. \nChemical bath deposition forms small crystals, which are less useful for semiconductors than the larger crystals created by other methods of thin-film deposition but are more useful for nano materials. However, films formed by chemical bath deposition often have better photovoltaic properties (band electron gap) than films of the same substance formed by other methods.\nProcess\nChemical bath deposition relies on creating a solution such that deposition (changing from an aqueous to a solid substance) will only occur on the substrate, using the method below:\n\nMetal salts and (usually) chalcogenide precursors are added to water to form an aqueous solution containing the metal ions and chalcogenide ions which will form the compound to be deposited.\nTemperature, pH, and concentration of salts are adjusted until the solution is in metastable supersaturation, that is until the ions are ready to deposit but can’t overcome the thermodynamic barrier to nucleation (forming solid crystals and precipitating out of the solution).\nA substrate is introduced, which acts as a catalyst to nucleation, and the precursor ions adhere to onto the substrate forming a thin crystalline film by one of the two methods described below.\nThat is, the solution is in a state where the precursor ions or colloidal particles are ‘sticky’, but can’t 'stick' to each other. When the substrate is introduced, the precursor ions or particles stick to it and aqueous ions stick to solid ions, forming a solid compound—depositing to form crystalline films. \nThe pH, temperature, and composition of the film affect crystal size, and can be used to control the rate of formation and the structure of the film. Other factors ",
    "source": "wikipedia",
    "title": "Chemical bath deposition",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_5736171",
    "text": "The chemical state of a chemical element is due to its electronic, chemical and physical properties as it exists in combination with itself or a group of one or more other elements. A chemical state is often defined as an \"oxidation state\" when referring to metal cations. When referring to organic materials, a chemical state is usually defined as a chemical group, which is a group of several elements bonded together. Material scientists, solid state physicists, analytical chemists, surface scientists and spectroscopists describe or characterize the chemical, physical and/or electronic nature of the surface or the bulk regions of a material as having or existing as one or more chemical states.\n\nOverview\nThe chemical state set comprises and encompasses these subordinate groups and entities: chemical species, functional group, anion, cation, oxidation state, chemical compound and elemental forms of an element.\nThis term or phrase is commonly used when interpreting data from analytical techniques such as: \n\nAuger electron spectroscopy (AES)\nEnergy-dispersive X-ray spectroscopy (EDS, EDX)\nInfrared spectroscopy (IR, FT-IR, ATR)\nLiquid chromatography (LC, HPLC)\nMass spectrometry (MS, ToF-SIMS, D-SIMS)\nNuclear magnetic resonance (NMR, H-NMR, C-NMR, X-NMR)\nPhotoemission spectroscopy (PES, UPS)\nRaman spectroscopy (FT-Raman)\nUltraviolet-visible spectroscopy (UV-Vis)\nX-ray photoelectron spectroscopy (XPS, ESCA)\nWavelength dispersive X-ray spectroscopy (WDX, WDS)\nSignificance\nThe chemical state of a group of elements, can be similar to, but not identical to, the chemical state of another similar group of elements because the two groups have different ratios of the same elements and exhibit different chemical, electronic, and physical properties that can be detected by various spectroscopic techniques.\nA chemical state can exist on or inside the surface of a solid state material and can often, but not always, be isolated or separated from the other chemical species found on the surface of that material. Surface scientists, spectroscopists, chemical analysts, and material scientists frequently describe the chemical nature of the chemical species, functional group, anion, or cation detected on the surface and near the surface of a solid state material as its chemical state.\nTo understand how a chemical state differs from an oxidation state, anion, or cation, compare sodium fluoride (NaF) to polytetrafluoroethylene (PTFE, Teflon). Both contain fluorine, the most electronegative element, but only NaF dissolves in water to form separate ions, Na+ and F−. The electronegativity of the fluorine strongly polarizes the electron density that exists between the carbon and the fluorine, but not enough to produce ions which would allow it to dissolve in the water. The carbon and fluorine in Teflon (PTFE) both have an electronic charge of zero since they form a covalent bond, but few scientists describe those elements as having an oxidation state of zero. On the other hand, ",
    "source": "wikipedia",
    "title": "Chemical state",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_85029",
    "text": "Chemical synthesis (chemical combination) is the artificial execution of chemical reactions to obtain one or more products. This occurs by physical and chemical manipulations, usually involving one or more reactions. In modern laboratory uses, the process is reproducible and reliable.\nA chemical synthesis involves one or more compounds (known as reagents or reactants) that will experience a transformation under certain conditions. Various reaction types can be applied to formulate a desired product. Many reactions require some form of processing (\"work-up\") or purification procedure to isolate the final product.\nThe amount produced by chemical synthesis is known as the reaction yield. Typically, yields are expressed as a mass in grams (in a laboratory setting) or as a percentage of the total theoretical quantity that could be produced based on the limiting reagent. A side reaction is an unwanted chemical reaction that can reduce the desired yield. The word synthesis was used first in a chemical context by the chemist Hermann Kolbe.\n\nStrategies\nChemical synthesis employs various strategies to achieve efficient and precise molecular transformations that are more complex than simply converting a reactant A to a reaction product B directly. These strategies can be grouped into approaches for managing reaction sequences.\nOrganic synthesis\nOrganic synthesis is a special type of chemical synthesis dealing with the synthesis of organic compounds. For the total synthesis of a complex product, multiple procedures in sequence may be required to synthesize the product of interest, needing a lot of time. A purely synthetic chemical synthesis begins with basic lab compounds. A semisynthetic process starts with natural products from plants or animals and then modifies them into new compounds.\nInorganic synthesis\nInorganic synthesis and organometallic synthesis are used to prepare compounds with significant non-organic content. An illustrative example is the preparation of the anti-cancer drug cisplatin from potassium tetrachloroplatinate.\n",
    "source": "wikipedia",
    "title": "Chemical synthesis",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_7746249",
    "text": "Chemical technologists and technicians (abbr. chem techs) are workers who provide technical support or services in chemical-related fields.  They may work under direct supervision or may work independently, depending on their specific position and duties.  Their work environments differ widely and include, but are not limited to, laboratories and industrial settings.  As such, it is nearly impossible to generalize the duties of chem techs as their individual jobs vary greatly.  Biochemical techs often do similar work in biochemistry.\n\nTechnologists\nChemical technologists are more likely than technicians to participate in the actual design of experiments, and may be involved in the interpretation of experimental data.  They may also be responsible for the operation of chemical processes in large plants, and may even assist chemical engineers in the design of the same.\nSome post-secondary education is generally required to be either a chemical technician or technologist.  Occasionally, a company may be willing to provide a high school graduate with training to become a chemical technician, but more often, a two-year degree will be required. Chemical technologists generally require completion of a specific college program—either two year or four year— in chemical, biochemical, or chemical engineering technology or a closely related discipline. \nThey usually work under or with a scientist such as a chemist or biochemist.\nTechnicians\nChemical or biochemical technicians often work in clinical (medical) laboratories conducting routine analyses of medical samples such as blood and urine.  Industries which employ chem techs include chemical, petrochemical, and pharmaceutical industries. Companies within these industries can be concerned with manufacturing, research and development (R&D), consulting, quality control, and a variety of other areas.  Also, chem techs working for these companies may be used to conduct quality control and other routine analyses, or assist in chemical and biochemical research including analyses, industrial chemistry, environmental protection, and even chemical engineering.\n",
    "source": "wikipedia",
    "title": "Chemical technologist",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_11960085",
    "text": "Chemophobia (or chemphobia or chemonoia) is an aversion to or prejudice against chemicals or chemistry. The phenomenon has been ascribed both to a reasonable concern over the potential adverse effects of synthetic chemicals, and to an irrational fear of these substances because of misconceptions about their potential for harm, particularly the possibility of certain exposures to some synthetic chemicals elevating an individual's risk of cancer.  Consumer products with labels such as \"natural\" and \"chemical free\" (the latter being impossible if taken literally, since all consumer products consist of chemical substances) appeal to chemophobic sentiments by offering consumers what appears to be a safer alternative (see appeal to nature).\n\nDefinition and uses\nThere are differing opinions on the proper usage of the word chemophobia.  The International Union of Pure and Applied Chemistry (IUPAC) defines chemophobia as an \"irrational fear of chemicals\".  According to the American Council on Science and Health, chemophobia is a fear of synthetic substances arising from \"scare stories\" and exaggerated claims about their dangers prevalent in the media.\nDespite containing the suffix -phobia, the majority of written work focusing on addressing chemophobia describes it as a non-clinical aversion or prejudice, and not as a phobia in the standard medical definition. Chemophobia is generally addressed by chemical education and public outreach despite the fact that much of chemophobia is economic or political in nature.\nMichelle Francl has written: \"We are a chemophobic culture. Chemical has become a synonym for something artificial, adulterated, hazardous, or toxic.\" She characterizes chemophobia as \"more like color blindness than a true phobia\" because chemophobics are \"blind\" to most of the chemicals that they encounter; every substance in the universe is a chemical. Francl proposes that such misconceptions are not innocuous, as demonstrated in one case by local statutes opposing the fluoridation of public water despite documented cases of tooth loss and nutritional deficit. In terms of risk perception, naturally occurring chemicals feel safer than synthetic ones to most people because of the involvement of humans.  Consequently, people fear man-made or \"unnatural\" chemicals, while accepting natural chemicals that are known to be dangerous or poisonous.\n\nThe Carcinogenic Potency Project, which is a part of the US EPA's Distributed Structure-Searchable Toxicity (DSSTox) Database Network, has been systemically testing the carcinogenicity of chemicals, both natural and synthetic, and building a publicly available database of the results since the 1980s.  Their work attempts to fill in the gaps in our scientific knowledge of the carcinogenicity of all chemicals, both natural and synthetic, as the scientists conducting the Project described in the journal, Science, in 1992: Toxicological examination of synthetic chemicals, without similar examination of chemicals t",
    "source": "wikipedia",
    "title": "Chemophobia",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_48999251",
    "text": "Chemoproteomics (also known as chemical proteomics) entails a broad array of techniques used to identify and interrogate protein-small molecule interactions. Chemoproteomics complements phenotypic drug discovery, a paradigm that aims to discover lead compounds on the basis of alleviating a disease phenotype, as opposed to target-based drug discovery (reverse pharmacology), in which lead compounds are designed to interact with predetermined disease-driving biological targets. As phenotypic drug discovery assays do not provide confirmation of a compound's mechanism of action, chemoproteomics provides valuable follow-up strategies to narrow down potential targets and eventually validate a molecule's mechanism of action. Chemoproteomics also attempts to address the inherent challenge of drug promiscuity in small molecule drug discovery by analyzing protein-small molecule interactions on a proteome-wide scale. A major goal of chemoproteomics is to characterize the interactome of drug candidates to gain insight into mechanisms of off-target toxicity and polypharmacology.\nChemoproteomics assays can be stratified into three basic types. Solution-based approaches involve the use of drug analogs that chemically modify target proteins in solution, tagging them for identification. Immobilization-based approaches seek to isolate potential targets or ligands by anchoring their binding partners to an immobile support. Derivatization-free approaches aim to infer drug-target interactions by observing changes in protein stability or drug chromatography upon binding. Computational techniques complement the chemoproteomic toolkit as parallel lines of evidence supporting potential drug-target pairs, and are used to generate structural models that inform lead optimization. Several targets of high profile drugs have been identified using chemoproteomics, and the continued improvement of mass spectrometer sensitivity and chemical probe technology indicates that chemoproteomics will play a large role in future drug discovery.\n\nBackground\n\n",
    "source": "wikipedia",
    "title": "Chemoproteomics",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_59274182",
    "text": "In modern valence bond (VB) theory calculations, Chirgwin–Coulson weights (also called Mulliken weights) are the relative weights of a set of possible VB structures of a molecule. Related methods of finding the relative weights of valence bond structures are the Löwdin and the inverse weights.\n\nBackground\nFor a wave function \n  \n    \n      \n        Ψ\n        =\n        \n          ∑\n          \n            i\n          \n        \n        \n          C\n          \n            i\n          \n        \n        \n          Φ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\Psi =\\sum \\limits _{i}C_{i}\\Phi _{i}}\n  \n where \n  \n    \n      \n        \n          Φ\n          \n            1\n          \n        \n        ,\n        \n          Φ\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          Φ\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\Phi _{1},\\Phi _{2},\\dots ,\\Phi _{n}}\n  \n are a linearly independent, orthogonal set of basis orbitals, the weight of a constituent orbital \n  \n    \n      \n        \n          Ψ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\Psi _{i}}\n  \n would be \n  \n    \n      \n        \n          C\n          \n            i\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle C_{i}^{2}}\n  \n since the overlap integral, \n  \n    \n      \n        \n          S\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle S_{ij}}\n  \n , between two wave functions \n  \n    \n      \n        \n          Ψ\n          \n            i\n          \n        \n        ,\n        \n          Ψ\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\Psi _{i},\\Psi _{j}}\n  \n would be 1 for \n  \n    \n      \n        i\n        =\n        j\n      \n    \n    {\\displaystyle i=j}\n  \n and 0 for \n  \n    \n      \n        i\n        ≠\n        j\n      \n    \n    {\\displaystyle i\\neq j}\n  \n . In valence bond theory, however, the generated structures are not necessarily orthogonal with each other, and oftentimes have substantial overlap between the two structures. As such, when considering non-orthogonal constituent orbitals (i.e. orbitals with non-zero overlap) the non-diagonal terms in the overlap matrix would be non-zero, and must be included in determining the weight of a constituent orbital. A method of computing the weight of a constituent orbital, \n  \n    \n      \n        \n          Φ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\Phi _{i}}\n  \n, proposed by Chirgwin and Coulson would be:\n\nApplication of the Chirgwin-Coulson formula to a molecular orbital yields the Mulliken population of the molecular orbital.\n",
    "source": "wikipedia",
    "title": "Chirgwin–Coulson weights",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_48248326",
    "text": "In chemistry, the term chromogen refers to a colourless (or faintly coloured) chemical compound that can be converted by chemical reaction into a compound which can be described as \"coloured\" (a chromophore). There is no universally agreed definition of the term. Various dictionaries give the following definitions:\n\nA substance capable of conversion into a pigment or dye.\nAny substance that can become a pigment or coloring matter, a substance in organic fluids that forms colored compounds when oxidized, or a compound, not itself a dye, that can become a dye.\nAny substance, itself without color, giving origin to a coloring matter.\nIn biochemistry the term has a rather different meaning. The following are found in various dictionaries.\n\nA precursor of a biochemical pigment\nA pigment-producing microorganism\nAny of certain bacteria that produce a pigment\nA strongly pigmented or pigment-generating organelle, organ, or microorganism.\n\nApplications in chemistry\nIn chromogenic photography, film or paper contains one or many layers of silver halide (AgX) emulsion, along with dye couplers that, in combination with processing chemistry, form visible dyes.\nApplications in biochemistry and medicine\nThe Runyon classification classifies mycobacteria by chromogenic properties.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Chromogen",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_610329",
    "text": "Clandestine chemistry is chemistry carried out in secret, and particularly in illegal drug laboratories. Larger labs are usually run by gangs or organized crime intending to produce for distribution on the black market. Smaller labs can be run by individual chemists working clandestinely in order to synthesize smaller amounts of controlled substances or simply out of a hobbyist interest in chemistry, often because of the difficulty in ascertaining the purity of other, illegally synthesized drugs obtained on the black market. The term clandestine lab is generally used in any situation involving the production of illicit compounds, regardless of whether the facilities being used qualify as a true laboratory.\n\nHistory\nAncient forms of clandestine chemistry included the manufacturing of explosives.\nFrom 1919 to 1933, the United States prohibited the sale, manufacture, or transportation of alcoholic beverages. This opened a door for brewers to supply their own town with alcohol. Just like modern-day drug labs, distilleries were placed in rural areas. The term moonshine generally referred to \"corn whiskey\", that is, a whiskey-like liquor made from corn. Today, American-made corn whiskey can be labeled or sold under that name, or as bourbon or Tennessee whiskey, depending on the details of the production process.\nPsychoactive substances\n\nExplosives\nClandestine chemistry is not limited to drugs; it is also associated with explosives, and other illegal chemicals. Of the explosives manufactured illegally, nitroglycerin and acetone peroxide are easiest to produce due to the ease with which the precursors can be acquired.\nUncle Fester is a writer who commonly writes about different aspects of clandestine chemistry. Secrets of Methamphetamine Manufacture is among his most popular books, and is considered required reading for DEA agents. More of his books deal with other aspects of clandestine chemistry, including explosives, and poisons. Fester is, however, considered by many to be a faulty and unreliable source for information in regard to the clandestine manufacture of chemicals.\n",
    "source": "wikipedia",
    "title": "Clandestine chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_49059554",
    "text": "Clay chemistry is an applied subdiscipline of chemistry which studies the chemical structures, properties and reactions of or involving clays and clay minerals. It is a multidisciplinary field, involving concepts and knowledge from inorganic and structural chemistry, physical chemistry, materials chemistry, analytical chemistry, organic chemistry, mineralogy, geology and others.\nThe study of the chemistry (and physics) of clays and clay minerals is of great academic and industrial relevance as they are among the most widely used industrial minerals, being employed as raw materials (ceramics, pottery, etc.), adsorbents, catalysts, additives, mineral charges, medicines, building materials and others.\nThe unique properties of clay minerals including: nanometric scale layered construction, presence of fixed and interchangeable charges, possibility of adsorbing and hosting (intercalating) molecules, ability of forming stable colloidal dispersions, possibility of tailored surface and interlayer chemical modification and others, make the study of clay chemistry a very important and extremely varied field of research.\nMany distinct fields and knowledge areas are impacted by the physico-chemical behavior of clay minerals, from environmental sciences to chemical process engineering, from pottery to nuclear waste management.\nTheir cation exchange capacity (CEC) is of great importance in the balance of the most common cations in soil (Na+, K+, NH4+, Ca2+, Mg2+) and pH control, with direct impact on the soil fertility. It also plays an important role in the fate of most Ca2+ arriving from land (river water) into the seas.\nThe ability to change and control the CEC of clay minerals offers a valuable tool in the development of selective adsorbents with applications as varied as chemical sensors or pollution cleaning substances for contaminated water, for example.\nThe understanding of the reactions of clay minerals with water (intercalation, adsorption, colloidal dispersion, etc.) are indispensable for the ceramic industry (plasticity and flow control of ceramic raw mixtures, for example). Those interactions also influence a great number of mechanical properties of soils, being carefully studied by building and construction engineering specialists.\nThe interactions of clay minerals with organic substances in the soil also plays a vital role in the fixation of nutrients and fertility, as well as in the fixation or leaching of pesticides and other contaminants. Some clay minerals (kaolinite) are used as carrier material for fungicides and insecticides.\nThe weathering of many rock types produce clay minerals as one of its last products. The understanding of these geochemical processes is also important for the understanding of geological evolution of landscapes and macroscopic properties of rocks and sediments. Presence of clay minerals in Mars, detected by the Mars Reconnaissance Orbiter in 2009 was another strong evidence of the existence of water on the planet in",
    "source": "wikipedia",
    "title": "Clay chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_37605500",
    "text": "The colloidal probe technique is commonly used to measure interaction forces acting between colloidal particles and/or planar surfaces in air or in solution. This technique relies on the use of an atomic force microscope (AFM). However, instead of a cantilever with a sharp AFM tip, one uses the colloidal probe. The colloidal probe consists of a colloidal particle of few micrometers in diameter that is attached to an AFM cantilever. The colloidal probe technique can be used in the sphere-plane or sphere-sphere geometries (see figure). One typically achieves a force resolution between 1 and 100 pN and a distance resolution between 0.5 and 2 nm.\nThe colloidal probe technique has been developed in 1991 independently by Ducker and Butt. Since its development this tool has gained wide popularity in numerous research laboratories, and numerous reviews are available in the scientific literature.\nAlternative techniques to measure force between surfaces involve the surface forces apparatus, total internal reflection microscopy, and optical tweezers techniques to with video microscopy.\n\nPurpose\nThe possibility to measure forces involving particles and surfaces directly is essential since such forces are relevant in a variety of processes involving colloidal and polymeric systems. Examples include particle aggregation, suspension rheology, particle deposition, and adhesion processes. One can equally study similar biological phenomena, such as deposition of bacteria or the infection of cells by viruses. Forces are equally most informative to investigate the mechanical properties of interfaces, bubbles, capsules, membranes, or cell walls. Such measurements permit to make conclusions about the elastic or plastic deformation or eventual rupture in such systems.\nThe colloidal probe technique provides a versatile tool to measure such forces between a colloidal particle and a planar substrate or between two colloidal particles (see figure above). The particles used in such experiments have typically a diameter between 1–10 μm. Typical applications involve measurements of electrical double layer forces and the corresponding surface potentials or surface charge, van der Waals forces, or forces induced by adsorbed polymers.\n",
    "source": "wikipedia",
    "title": "Colloidal probe technique",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_69164293",
    "text": "Compliance constants are the elements of an inverted Hessian matrix. The calculation of compliance constants provides an alternative description of chemical bonds in comparison with the widely used force constants explicitly ruling out the dependency on the coordinate system. They provide the unique description of the mechanical strength for covalent and non-covalent bonding. While force constants (as energy second derivatives) are usually given in aJ/Å2 or N/cm, compliance constants are given in Å2/aJ or Å / mdyn.\n\nHistory\nHitherto, recent publications that broke the wall of putative chemical understanding and presented detection/isolation of novel compounds with intriguing bonding characters can still be provocative at times. The stir in such discoveries arose partly from the lack of a universally accepted bond descriptor. While bond dissociation energies (BDE) and rigid force constants have been generally regarded as primary tools for such interpretation, they are prone to flawed definition of chemical bonds in certain scenarios whether simple or controversial.\nSuch reasons prompted the necessity to seek an alternative approach to describe covalent and non-covalent interactions more rigorously. Jörg Grunenberg, a German chemist at the TU Braunschweig and his Ph.D. student at the time, Kai Brandhorst, developed a program COMPLIANCE (freely available to the public), which harnesses compliance constants for tackling the aforementioned tasks. The authors use an inverted matrix of force constants, i.e., inverted Hessian matrix, originally introduced by W. T. Taylor and K. S. Pitzer. The insight in choosing the inverted matrix is from the realization that not all elements in the Hessian matrix are necessary—and thus redundant—for describing covalent and non-covalent interactions. Such redundancy is common for many molecules, and more importantly, it ushers in the dependence of the elements of the Hessian matrix on the choice of coordinate system. Therefore, the author claimed that force constants albeit more widely used are not an appropriate bond descriptor whereas non-redundant and coordinate system-independent compliance constants are.\n",
    "source": "wikipedia",
    "title": "Compliance constants",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_52625201",
    "text": "Compound Interest is a website launched in 2013 by Andy Brunning with infographics on everyday chemistry. The infographics describe, for example, how chemicals found in food and nature give them smell, taste, and color. The website has a monthly collaboration with the American Chemical Society. Content of the website is used as information source by various newspapers and media, including the Washington Post, Time, The Conversation, and Forbes.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Compound Interest (website)",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_38403118",
    "text": "In chemistry, congeners are chemical substances \"related to each other by origin, structure, or function\".\n\nCommon origin and structure\nAny significant quantity of a polyhalogenated compound is by default a blend of multiple molecule types because each molecule forms independently, and chlorine and bromine do not strongly select which site(s) they bond to.\n\nPolychlorinated biphenyls (PCBs) are a family of 209 congeners.\nPolybrominated biphenyls and polychlorinated diphenyl ethers are also families of 209 congeners.\nSimilarly polychlorinated dibenzodioxins, polychlorinated dibenzofurans, polychlorinated terphenyls, polychlorinated naphthalene, polychloro phenoxy phenol, and polybrominated diphenyl ethers (PBDEs) (pentabromodiphenyl ether, octabromodiphenyl ether, decabromodiphenyl ether), etc. are also groups of congeners.\nCommon origin\nCongener (alcohol), substances other than alcohol (desirable or undesirable) also produced during fermentation.\nCongeners of oleic acids can modify cell membrane behavior, protecting against tumors or having effects on blood pressure.\nCommon structure\nCongeners can refer to similar compounds that substitute other elements with similar valences, yielding molecules having similar structures. Examples:\n\npotassium chloride and sodium chloride may be considered congeners; also potassium chloride and potassium fluoride.\nhydrogen peroxide (HOOH), hydrogen thioperoxide (HSOH), and hydrogen disulfide (HSSH).\nStructural analogs are often isoelectronic.\nOther\nCongeners refer to the various oxidation states of a given element in a compound. For example, titanium(II) chloride (titanium dichloride), titanium(III) chloride (titanium trichloride), and titanium(IV) chloride (titanium tetrachloride) may be considered congeners.\nCongeners can refer to other elements in the same group in the periodic table. For example, congeners of the Group 11 element copper are silver and gold, sometimes found together in the same ores (porphyry copper deposit) due to their chemical similarity.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Congener (chemistry)",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_70333668",
    "text": "Cononsolvency is a phenomenon where two solvents that can typically readily dissolve a polymer, when mixed, at certain ratios of these two solvents, are no longer able to dissolve the polymer. This phenomenon is in contrast to cosolvency where two solvents that are both poor at dissolving a material, but when the two poor solvents admixed, can form a mixed solvent capable of dissolving the material.\nThe first works of both experimental and theoretical about the cononsolvency effect were published in the late 1970s. Since then, numerous studies focused on a manifold of different polymers that featured the cononsolvency effect in water and various organic cosolvents such as methanol, ethanol, and acetone. Typically poly(acrylamide)s such as poly(N-isopropylacrylamide) show the cononsolvency effect, while this effect is also known for other homopolymers and for more complex systems e.g., diblock copolymer, polyelectrolytes, crosslinked microgels, micelles, and grafted polymer brushes. Recently, it was also shown that thermo-responsive thin films exhibit the cononsolvency effect in a mixed solvent vapor phase, which can be explained by a decreased volume phase transition temperature, the thin-film analogy of a lower critical solution temperature. These experimental studies are supported by a growing number of simulation studies.\nAfter 45 years of research, the origin of the molecular mechanism behind the cononsolvency effect in a mixture of solvents remains not fully resolved yet. To date, researchers have considered various interactions between polymer and solvent/cosolvent as possible factors leading to the cononsolvency effect, such as competitive hydrogen bonding of the solvent and cosolvent with the polymer, hydrophobic hydration of particular functional groups of the polymer,  cosolvent induced geometric frustration, excluded-volume interactions due to the surfactant-like behavior of amphiphilic cosolvents, as well as the three body effects, i.e., temporary bridging of one or more individual polymer chains by the cosolvent.\nIn literature, cononsolvency was reported almost exclusively for polymers in aqueous solution. This, however, does not mean that cononsolvency cannot happen in non-aqueous solutions. For example, poly(methyl methacrylate) shows the cononsolvency effect in the binary mixtures of two organic solvents (chlorobutane and amyl acetate).  \n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Cononsolvency",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_33177861",
    "text": "Core–shell semiconducting nanocrystals (CSSNCs) are a class of materials which have properties intermediate between those of small, individual molecules and those of bulk, crystalline semiconductors. They are unique because of their easily modular properties, which are a result of their size. These nanocrystals are composed of a quantum dot semiconducting core material and a shell of a distinct semiconducting material.  The core and the shell are typically composed of type II–VI, IV–VI, I-III-VI, and III–V semiconductors, with configurations such as CdS/ZnS, CdSe/ZnS, CuInZnSe/ZnS, CdSe/CdS, and InAs/CdSe (typical notation is: core/shell) Organically passivated quantum dots have low fluorescence quantum yield due to surface related trap states. CSSNCs address this problem because the shell increases quantum yield by passivating the surface trap states.  In addition, the shell provides protection against environmental changes, photo-oxidative degradation, and provides another route for modularity. Precise control of the size, shape, and composition of both the core and the shell enable the emission wavelength to be tuned over a wider range of wavelengths than with either individual semiconductor.  These materials have found applications in biological systems and optics.\n\nBackground\nColloidal semiconductor nanocrystals, which are also called quantum dots (QDs), consist of ~1–10 nm diameter semiconductor nanoparticles that have organic ligands bound to their surface.  These nanomaterials have found applications in nanoscale photonic, photovoltaic, and light-emitting diode (LED) devices due to their size-dependent optical and electronic properties.  Quantum dots are popular alternatives to organic dyes as fluorescent labels for biological imaging and sensing due to their small size, tuneable emission, and photostability.\nThe luminescent properties of quantum dots arise from exciton decay (recombination of electron hole pairs) which can proceed through a radiative or nonradiative pathway.  The radiative pathway involves electrons relaxing from the conduction band to the valence band by emitting photons with wavelengths corresponding to the semiconductor's bandgap. Nonradiative recombination can occur through energy release via phonon emission or Auger recombination. In this size regime, quantum confinement effects lead to a size dependent increasing bandgap with observable, quantized energy levels.  The quantized energy levels observed in quantum dots lead to electronic structures that are intermediate between single molecules which have a single HOMO-LUMO gap and bulk semiconductors which have continuous energy levels within bands \n\nSemiconductor nanocrystals generally adopt the same crystal structure as their extended solids. At the surface of the crystal, the periodicity abruptly stops, resulting in surface atoms having a lower coordination number than the interior atoms. This incomplete bonding (relative to the interior crystal structure) results ",
    "source": "wikipedia",
    "title": "Core–shell semiconductor nanocrystal",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_74173478",
    "text": "Corrosion inhibitors are substances used in the oil industry to protect equipment and pipes against corrosion. Corrosion is a common problem in the oil industry due to the presence of water, gases, and other corrosive contaminants in the production environment.\nAnodic inhibitors and cathodic inhibitors are the two main categories of corrosion inhibitors. While cathodic inhibitors act as catalysts to slow down corrosion, anodic inhibitors protect metal surfaces by acting as physical barriers. They can also be divided into organic and inorganic corrosion inhibitors based on their chemical composition.\nCorrosion inhibitors are used in the petroleum industry in several steps, including drilling, production, transportation, and storage of oil and gas. They can mitigate different types of corrosion in the petroleum industry, such as generalized corrosion, pitting corrosion, erosion corrosion, stress corrosion, galvanic corrosion, cavitation corrosion, and hydrogen blister.\n\nCorrosion Inhibitor Families\nThere are different chemical families of corrosion inhibitors used in the oil industry, among them are the following:\nFatty Imidazolines: These are imidazole-based compounds, usually with a long unsaturated chain length, derived mainly from oleic acid. They are very effective in preventing acid corrosion of carbon steel (Figure 1).\n\nFatty amines: These corrosion inhibitors are organic compounds that contain an amino group and an alkyl group. They act as cathodic inhibitors and form a protective layer on the metal surface.They work efficiently against corrosion brought about by carbon dioxide (CO2) and hydrogen sulfide (H2S). Also, ethoxylated amines are widely applied for the same purpose (Figure 2).\n\nOrganic Acids: Organic acids such as acetic acid, formic acid and citric acid are used as corrosion inhibitors. These acids react with metal ions to form insoluble compounds that protect the metal surface. These inhibitors are often used in combination with other corrosion inhibitors and techniques, such as cathodic protection and coatings, to provide comprehensive corrosion protection. CO2 and H2S are regularly seen in oilfields and are notorious for causing corrosion of metal sections. Fortunately, they can be kept under control with measures that have been found to be effective (Figure 3).\n\nPyridines: Some studies have shown that certain pyridines can inhibit corrosion caused by the presence of acid gases, such as carbon dioxide and hydrogen sulfide, which are common in the oil industry. Pyridine and its derivatives have been shown to be effective inhibitors for a wide range of metals, such as carbon steel, stainless steel, and copper alloys. They act by adsorbing to the metal surface and forming a protective film, which can be physical or chemical in nature. Pyridine and its derivatives are also effective in inhibiting localized corrosion, such as pitting and crevice corrosion (Figure 4).\n\nAzoles: Azoles, such as triazole and benzotriazole, oxazole and ",
    "source": "wikipedia",
    "title": "Corrosion inhibitors for the petroleum industry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_40913419",
    "text": "In chemistry, a crossover experiment is a method used to study the mechanism of a chemical reaction. In a crossover experiment, two similar but distinguishable reactants simultaneously undergo a reaction as part of the same reaction mixture. The products formed will either correspond directly to one of the two reactants (non-crossover products) or will include components of both reactants (crossover products). The aim of a crossover experiment is to determine whether or not a reaction process involves a stage where the components of each reactant have an opportunity to exchange with each other.\nThe results of crossover experiments are often straightforward to analyze, making them one of the most useful and most frequently applied methods of mechanistic study. In organic chemistry, crossover experiments are most often used to distinguish between intramolecular and intermolecular reactions.\nInorganic and organometallic chemists rely heavily on crossover experiments, and in particular isotopic labeling experiments, for support or contradiction of proposed mechanisms.  When the mechanism being investigated is more complicated than an intra- or intermolecular substitution or rearrangement, crossover experiment design can itself become a challenging question. A well-designed crossover experiment can lead to conclusions about a mechanism that would otherwise be impossible to make. Many mechanistic studies include both crossover experiments and measurements of rate and kinetic isotope effects.\n\nPurpose\nCrossover experiments allow for experimental study of a reaction mechanism. Mechanistic studies are of interest to theoretical and experimental chemists for a variety of reasons including prediction of stereochemical outcomes, optimization of reaction conditions for rate and selectivity, and design of improved catalysts for better turnover number, robustness, etc. Since a mechanism cannot be directly observed or determined solely based on the reactants or products, mechanisms are challenging to study experimentally. Only a handful of experimental methods are capable of providing information about the mechanism of a reaction, including crossover experiments, studies of the kinetic isotope effect, and rate variations by substituent. The crossover experiment has the advantage of being conceptually straightforward and relatively easy to design, carry out, and interpret. In modern mechanistic studies, crossover experiments and KIE studies are commonly used in conjunction with computational methods.\n",
    "source": "wikipedia",
    "title": "Crossover experiment (chemistry)",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_18388283",
    "text": "Crystal chemistry is the study of the principles of chemistry behind crystals and their use in describing structure-property relations in solids, as well as the chemical properties of periodic structures. The principles that govern the assembly of crystal and glass structures are described, models of many of the technologically important crystal structures (alumina, quartz, perovskite) are studied, and the effect of crystal structure on the various fundamental mechanisms responsible for many physical properties are discussed.\nThe objectives of the field include:\n\nidentifying important raw materials and minerals as well as their names and chemical formulae.\ndescribing the crystal structure of important materials and determining their atomic details\nlearning the systematics of crystal and glass chemistry.\nunderstanding how physical and chemical properties are related to crystal structure and microstructure.\nstudying the engineering significance of these ideas and how they relate to foreign products: past, present, and future.\nTopics studied are:\n\nChemical bonding, Electronegativity\nFundamentals of crystallography: crystal systems, Miller Indices, symmetry elements, bond lengths and radii, theoretical density\nCrystal and glass structure prediction: Pauling's and Zachariasen’s rules\nPhase diagrams and crystal chemistry (including solid solutions)\nImperfections (including defect chemistry and line defects)\nPhase transitions\nStructure – property relations: Neumann's law, melting point, mechanical properties (hardness, slip, cleavage, elastic moduli), wetting, thermal properties (thermal expansion, specific heat, thermal conductivity), diffusion, ionic conductivity, refractive index, absorption, color, Dielectrics and Ferroelectrics, and Magnetism\nCrystal structures of representative metals, semiconductors, polymers, and ceramics\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Crystal chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_7794",
    "text": "Crystallography is the branch of science devoted to the study of molecular and crystalline structure and properties. The word crystallography is derived from the Ancient Greek word κρύσταλλος (krústallos; \"clear ice, rock-crystal\"), and γράφειν (gráphein; \"to write\"). In July 2012, the United Nations recognised the importance of the science of crystallography by proclaiming 2014 the International Year of Crystallography.\nCrystallography is a broad topic, and many of its subareas, such as X-ray crystallography, are themselves important scientific topics. Crystallography ranges from the fundamentals of crystal structure to the mathematics of crystal geometry, including those that are not periodic or quasicrystals. At the atomic scale it can involve the use of X-ray diffraction to produce experimental data that the tools of X-ray crystallography can convert into detailed positions of atoms, and sometimes electron density. At larger scales it includes experimental tools such as orientational imaging to examine the relative orientations at the grain boundary in materials. Crystallography plays a key role in many areas of biology, chemistry, and physics, as well as in emerging developments in these fields.\n\nHistory and timeline\nBefore the 20th century, the study of crystals was based on physical measurements of their geometry using a goniometer. This involved measuring the angles of crystal faces relative to each other and to theoretical reference axes (crystallographic axes), and establishing the symmetry of the crystal in question. The position in 3D space of each crystal face is plotted on a stereographic net such as a Wulff net or Lambert net. The pole to each face is plotted on the net. Each point is labelled with its Miller index. The final plot allows the symmetry of the crystal to be established.\nThe discovery of X-rays and electrons in the last decade of the 19th century enabled the determination of crystal structures on the atomic scale, which brought about the modern era of crystallography. The first X-ray diffraction experiment was conducted in 1912 by Max von Laue, while electron diffraction was first realized in 1927 in the Davisson–Germer experiment and parallel work by George Paget Thomson and Alexander Reid. These developed into the two main branches of crystallography, X-ray crystallography and electron diffraction. The quality and throughput of solving crystal structures greatly improved in the second half of the 20th century, with the developments of customized instruments and phasing algorithms. Nowadays, crystallography is an interdisciplinary field, supporting theoretical and experimental discoveries in various domains. Modern-day scientific instruments for crystallography vary from laboratory-sized equipment, such as diffractometers and electron microscopes, to dedicated large facilities, such as photoinjectors, synchrotron light sources and free-electron lasers.\n",
    "source": "wikipedia",
    "title": "Crystallography",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_68615793",
    "text": "Cyclosiloxanes are a class of silicone material. They are volatile and often used as a solvent. The three main commercial varies are octamethylcyclotetrasiloxane (D4), decamethylcyclopentasiloxane (D5) and dodecamethylcyclohexasiloxane (D6). They evaporate and degrade in air under sunlight.\n\nOctamethylcyclotetrasiloxane (D4)\nThe octamethylcyclotetrasiloxane silicone liquid has no odor and consists of four repeating units of silicon (Si) and oxygen (O) atoms in a closed loop giving it a circular structure. Each silicon atom has two methyl groups attached (CH3).\nDecamethylcyclopentasiloxane (D5)\nDecamethylcyclopentasiloxane silicone liquid has no odor and consists of five repeating units of silicon (Si) and oxygen (O) atoms in a closed loop giving it a circular structure. Each silicon atom has two methyl groups attached (CH3). Typically it is used as an ingredient in antiperspirant, skin cream, sun protection lotion and make-up. With a low surface tension of 18 mN/m this material has good spreading properties.\nDodecamethylcyclohexasiloxane (D6)\nThe dodecamethylcyclohexasiloxane silicone liquid has no odor and consists of six repeating units of silicon (Si) and oxygen (O) atoms in a closed loop giving it a circular structure. Each silicon atom has two methyl groups attached (CH3).\nCASRN: 540-97-6. D6 is also contained under the CAS No. (69430-24-6 ) which is associated with the names cyclopolydimethylsiloxane, cyclopolydimethylsiloxane (DX), cyclosiloxanes di-Me, dimethylcyclopolysiloxane, polydimethyl siloxy cyclics, polydimethylcyclosiloxane, cyclomethicone and mixed cyclosiloxane.\nSee also\nPolydimethylsiloxane\nCyclomethicone\nSiloxane and other organosilicon compounds\nLiterature\nCyclosiloxanes (pdf-file), Materials for the December 4-5, 2008 Meeting of the California Environmental Contaminant Biomonitoring Program (CECBP) Scientific Guidance Panel (SGP)\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Cyclosiloxane",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_77419359",
    "text": "Dark oxygen production refers to the generation of molecular oxygen (O2) through processes that do not involve light-dependent oxygenic photosynthesis. The name therefore uses a different sense of 'dark' than that used in the phrase \"biological dark matter\" (for example) which indicates obscurity to scientific assessment rather than the photometric meaning. While the majority of Earth's oxygen is produced by plants and photosynthetically active microorganisms via photosynthesis, dark oxygen production occurs via a variety of abiotic and biotic processes and may support aerobic metabolism in dark, anoxic environments.\nThe metallic nodule theory for dark oxygen production in particular is controversial, with scientists disagreeing about their validity.\n\nAbiotic production\nAbiotic production of dark oxygen can occur through several mechanisms, such as:\n\nWater radiolysis: This process typically takes place in dark geological ecosystems, such as aquifers, where the decay of radioactive elements in surrounding rock leads to the breakdown of water molecules, producing O2.\nOxidation of surface-bound radicals: On silicon-bearing minerals like quartz, surface-bound radicals can undergo oxidation, contributing to O2 production.\nIn addition to direct O2 formation, these processes often produce reactive oxygen species (ROS), such as hydroxyl radicals (OH•), superoxide (O2•-), and hydrogen peroxide (H2O2). These ROS can be converted into O2 and water either biotically, through enzymes like superoxide dismutase and catalase, or abiotically, via reactions with ferrous iron and other reduced metals.\nBiotic production\nBiotic production of dark oxygen is performed by microorganisms through distinct microbial processes, including:\n\nChlorite dismutation: This involves the dismutation of chlorite (ClO2−) into O2 and chloride ions.\nNitric oxide dismutation: This involves the dismutation of nitric oxide (NO) into O2 and dinitrogen gas (N2) or nitrous oxide (N2O).\nWater lysis via methanobactins: Methanobactins can lyse water molecules to produce O2.\nThese processes enable microbial communities to sustain aerobic metabolism in environments that lack oxygen.\n",
    "source": "wikipedia",
    "title": "Dark oxygen",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_60575040",
    "text": "In chemistry, the decay technique is a method to generate chemical species such as radicals, carbocations, and other potentially unstable covalent structures by radioactive decay of other compounds. For example, decay of a tritium-labeled molecule yields an ionized helium atom, which might then break off to leave a cationic molecular fragment.\nThe technique was developed in 1963 by the Italian chemist Fulvio Cacace at the University of Rome.  It has allowed the study of a vast number of otherwise inaccessible compounds and reactions.  It has also provided much of our current knowledge about the chemistry of the helium hydride ion [HeH]+.\n\nCarbocation generation\nIn the basic method, a molecule (R,R′,R″)C−T is prepared where the vacant bond of the desired radical or ion is satisfied by an atom of tritium 3H, the radioactive isotope of hydrogen with mass number 3.  As the tritium undergoes beta decay (with a half-life of 12.32 years), it is transformed into an ion of helium-3, creating the cation (R,R′,R″)C−[3He]+.\nIn the decay, an electron and an antineutrino are ejected at great speed from the tritium nucleus, changing one of the neutrons into a proton with the release of 18,600 electronvolts (eV) of energy.  The neutrino escapes the system; the electron is generally captured within a short distance, but far enough away from the site of the decay that it can be considered lost from the molecule.  Those two particles carry away most of the released energy, but their departure causes the nucleus to recoil, with about 1.6 eV of energy. This recoil energy is larger than the bond strength of the carbon–helium bond (about 1 eV), so this bond breaks.  The helium atom almost always leaves as a neutral 3He, leaving behind the carbocation [(R,R′,R″)C]+.\nThese events happen very quickly compared to typical molecular relaxation times, so the carbocation is usually created in the same conformation and electronic configuration as the original neutral molecule.  For example, decay of tritiated methane, CH3T (R = R′ = R″ = H) produces the carbenium ion H3C+ in a tetrahedral conformation, with one of the orbitals having a single unpaired electron and the other three forming a trigonal pyramid.  The ion then relaxes to its more favorable trigonal planar form, with release of about 30 kcal/mol of energy—that goes into vibrations and rotation of the ion.\nThe carbocation then can interact with surrounding molecules in many reactions that cannot be achieved by other means.  When formed within a rarefied gas, the carbocation and its reactions can be studied by mass spectrometry techniques.  However the technique can be used also in condensed matter (liquids and solids).  In liquid phase, the carbocation is initially formed in the same solvation state as the parent molecule, and some reactions may happen before the solvent shells around it have time to rearrange.  In a crystalline solid, the cation is formed in the same crystalline site; and the nature, position, and ori",
    "source": "wikipedia",
    "title": "Decay technique",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_48623453",
    "text": "DePriester Charts provide an efficient method to find the vapor-liquid equilibrium ratios for different substances at different conditions of pressure and temperature. The original chart was put forth by C.L. DePriester in an article in Chemical Engineering Progress in 1953. These nomograms have two vertical coordinates, one for pressure, and another for temperature. \"K\" values, representing the tendency of a given chemical species to partition itself preferentially between liquid and vapor phases, are plotted in between. Many DePriester charts have been printed for simple hydrocarbons.\n\nExample\nFor example, to find the K value of methane at 100 psia and 60 °F.\n\nOn the left-hand vertical axis, locate and mark the point containing the pressure 100 psia.\nOn the right-hand vertical axis, locate and mark the point containing the temperature 60°F.\nConnect the points with a straight line.\nNote where the line crosses the methane axis. Read this K-value off the chart (approximately 21.3).\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "DePriester chart",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_74926555",
    "text": "In the iron and steel industry, direct reduction is a set of processes for obtaining iron from iron ore, by reducing iron oxides without melting the metal. The resulting product is pre-reduced iron ore.\nHistorically, direct reduction was used to obtain a mix of iron and slag called a bloom in a bloomery. At the beginning of the 20th century, this process was abandoned in favor of the blast furnace, which produces iron in two stages (reduction-melting to produce cast iron, followed by refining in a converter).\nHowever, various processes were developed in the course of the 20th century and, since the 1970s, the production of pre-reduced iron ore has undergone remarkable industrial development, notably with the rise of the Midrex process. Designed to replace the blast furnace, these processes have so far only proved profitable in certain economic contexts, which still limits this sector to less than 5% of world steel production.\n\nHistory\n\nChemical reactions\n\nProcedures\nPlants for the production of pre-reduced iron ore are known as direct reduction plants. The principle involves exposing iron ore to the reducing action of a high-temperature gas (around 1000 °C). This gas is composed of carbon monoxide and dihydrogen, the proportions of which depend on the production process.\nGenerally speaking, there are two main types of processes:\n\nprocesses where the reducing gas is obtained from natural gas. In this case, the ore is reduced in tanks;\nprocesses where the reducing gas is obtained from coal. The reactor is generally an inclined rotary kiln, similar to those used in cement plants, in which coal is mixed with limestone and ore, then heated.\nAnother way of classifying processes is to distinguish between those where the reducing gases are produced in specific facilities separate from the reduction reactor - which characterizes most processes using natural gas - and those where the gases are produced inside the fusion reactor: coal-fired processes generally fall into this category. However, many \"gas-fired\" processes can be fed by gasification units producing a reducing gas from coal.\nIn addition, since the melting stage is necessary to obtain alloys, reduction-melting processes have been developed which, like blast furnaces, produce a more or less carburized liquid metal. Finally, many more or less experimental processes have been developed.\n",
    "source": "wikipedia",
    "title": "Direct reduction",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_37732235",
    "text": "Double layer forces occur between charged objects across liquids, typically water. This force acts over distances that are comparable to the Debye length, which is on the order of one to a few tenths of nanometers. The strength of these forces increases with the magnitude of the surface charge density (or the electrical surface potential). For two similarly charged objects, this force is repulsive and decays exponentially at larger distances, see figure. For unequally charged objects and eventually at shorted distances, these forces may also be attractive. The theory due to Derjaguin, Landau, Verwey, and Overbeek (DLVO) combines such double layer forces together with Van der Waals forces in order to estimate the actual interaction potential between colloidal particles.\nAn electrical double layer develops near charged surfaces (or another charged objects) in aqueous solutions. Within this double layer, the first layer corresponds to the charged surface. These charges may originate from tightly adsorbed ions, dissociated surface groups, or substituted ions within the crystal lattice. The second layer corresponds to the diffuse layer, which contains the neutralizing charge consisting of accumulated counterions and depleted coions. The resulting potential profile between these two objects leads to differences in the ionic concentrations within the gap between these objects with respect to the bulk solution. These differences generate an osmotic pressure, which generates a force between these objects.\nThese forces are easily experienced when hands are washed with soap. Adsorbing soap molecules make the skin negatively charged, and the slippery feeling is caused by the strongly repulsive double layer forces. These forces are further relevant in many colloidal or biological systems, and may be responsible for their stability, formation of colloidal crystals, or their rheological properties.\n\nPoisson–Boltzmann model\nThe most popular model to describe the electrical double layer is the Poisson-Boltzmann (PB) model. This model can be equally used to evaluate double layer forces. Let us discuss this model in the case of planar geometry as shown in the figure on the right. In this case, the electrical potential profile ψ(z) near a charged interface will only depend on the position z. The corresponding Poisson's equation reads in SI units\n\n  \n    \n      \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              ψ\n            \n            \n              d\n              \n                z\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        −\n        \n          \n            ρ\n            \n              \n                ϵ\n                \n                  0\n                \n              \n              ϵ\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {d^{2}\\psi }{dz^{2}}}=-{\\frac {\\",
    "source": "wikipedia",
    "title": "Double layer forces",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_59891440",
    "text": "Made up of primary carbon, carbon black is spherical in shape and arranged into aggregates and agglomerates. It differs from other carbon forms (diamond, graphite, coke) in its complex configuration, colloid dimensions and quasi-graphitic structure. Carbon black's purity and composition are practically free of inorganic pollutants and extractable organic substances.\nA distinction is made between these two terms:\n\nCarbon black – a specially produced type of carbon using the process of incomplete combustion with restricted oxygen access. The article addresses this type of carbon.\nSoot – auxiliary fuel (coal, hydrocarbons, crude oil) combustion product, which is considered to be a hazardous substance with carcinogenic properties.\nCarbon black can be characterized as a substance with over 97% amorphous carbon content. It is used extensively in many areas of industrial chemistry. It is often used in the plastic and rubber manufacturing industries, where it improves electrical conductivity and electromagnetic or thermo-conductive characteristics of plastic materials and rubbers. By virtue of its pigmentation capabilities, it is also used for the production of special printing inks, paints and varnishes. Thanks to its advanced porous structure, it is also used as a catalyst carrier, and its notable sorption attributes are used for, in example, catching gaseous pollutants at waste incinerator plants.\nCarbon black predominantly includes a conductive type of carbon, which combines an extremely high specific surface and extensively developed structure – microporosity. At the same time, it consists of primary carbon particles and boasts a high degree of aggregation. Carbon black's grouping facilitates the formation of a conductive structure in plastics, rubbers and other composites. These characteristics predetermine electroconductive carbon black's primary area of application, i.e. electrical conductivity modification of nearly all types of plastic materials by adding a relatively low volume of carbon black. Such modifications can be used for numerous purposes, from establishing antistatic properties to adjusting polymer conductivity. Another valuable property of electroconductive carbon black is its excellent ability to absorb UV radiation on the visible spectrum, i.e. as a UV stabilizer for plastic materials, pigment in printer inks, paints and varnishes, or for coloring plastics, rubbers and sealants.\n\nProduction\nCarbon black begins as a byproduct of what is referred to as partial oxidation, a process during which crude oil residues, such as vacuum residues from crude oil distillation or residues from the thermic cracking process, split due to the effects of the mixture of oxygen and water steam under high temperatures around 1,300 °C.\nPartial oxidation of various raw materials always creates a gaseous mixture containing CO, CO2, H2O, H2, CH4 and H2S and COS formed from sulfurous compounds. Carbon black is formed as an undesired byproduct. The amount of ",
    "source": "wikipedia",
    "title": "Electroconductive carbon black",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_8339883",
    "text": "Electrolysed water (also electrolyzed water, EOW, electrolyzed oxidizing water, electro-activated water, super-oxidized solution or electro-chemically activated water solution) is produced by the electrolysis of water containing dissolved sodium chloride. The electrolysis of salt solutions produces a solution of hypochlorous acid and sodium hydroxide. The hypochlorous acid and sodium hydroxide (essentially, bleach) generated by electrolysis can be used as a disinfectant, if the solution is used immediately before the solution degrades.\n\nCreation\nThe electrolysis occurs in a vessel with separation of the cathodic and anodic solutions. \nAt the cathode, hydrogen gas and hydroxide ions are produced, leading to an alkaline solution that consists essentially of sodium hydroxide. \nAt the anode, chloride ions can be oxidized to elemental chlorine, which is present in acidic solution and can be corrosive to metals. If the solution near the anode is acidic then it will contain elemental chlorine.  \nThe key to delivering a powerful sanitizing agent is to form hypochlorous acid without elemental chlorine. This occurs at around neutral pH .  Hypochlorous is a weak acid and an oxidizing agent. This \"acidic electrolyzed water\" can be raised in pH by mixing in the desired amount of hydroxide ion solution from the cathode compartment, yielding a solution of Hypochlorous acid (HOCl) and sodium hydroxide (NaOH).  A solution at pH 7.3 will contain equal concentrations of hypochlorous acid and hypochlorite ion; reducing the pH will shift the balance toward the hypochlorous acid.  At a pH between 5.5 and 6.0 approximately 90% of the ions are in the form of hypochlorous acid.\nProposed use as a disinfectant\nBoth sodium hydroxide and hypochlorous acid can be disinfecting agents; The key to effective sanitation is to have a high proportion of hypochlorous acid present, this happens between acidic and neutral pH conditions. \nUnder some controlled circumstances, EOW can kill bacteria and inactivate viruses. Freshly made EOW (used within 2 minutes of creation) was shown to achieve a 5-log reduction in pathogens.\nThe disinfectant claims of EOW are based on a formulation containing a mixed oxidant with a  corrosive pH of 2.53.\n",
    "source": "wikipedia",
    "title": "Electrolysed water",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_70995834",
    "text": "In chemistry and particularly biochemistry, an energy-rich species (usually energy-rich molecule) or high-energy species (usually high-energy molecule) is a chemical species which reacts, potentially with other species found in the environment, to release chemical energy.\nIn particular, the term is often used for:\n\nadenosine triphosphate (ATP) and similar molecules called high-energy phosphates, which release inorganic phosphate into the environment in an exothermic reaction with water:\nATP + H2O → ADP + Pi   ΔG°' = −30.5 kJ/mol (−7.3 kcal/mol)\nfuels such as hydrocarbons, carbohydrates, lipids, proteins, and other organic molecules which react with oxygen in the environment to ultimately form carbon dioxide, water, and sometimes nitrogen, sulfates, and phosphates\nmolecular hydrogen\nmonatomic oxygen, ozone, hydrogen peroxide, singlet oxygen and other metastable or unstable species which spontaneously react without further reactants\nin particular, the vast majority of free radicals\nexplosives such as nitroglycerin and other substances which react exothermically without requiring a second reactant\nmetals or metal ions which can be oxidized to release energy\nThis is contrasted to species that are either part of the environment (this sometimes includes diatomic triplet oxygen) or do not react with the environment (such as many metal oxides or calcium carbonate); those species are not considered energy-rich or high-energy species.\n\nAlternative definitions\nThe term is often used without a definition. Some authors define the term \"high-energy\" to be equivalent to \"chemically unstable\", while others reserve the term for high-energy phosphates, such as the Great Soviet Encyclopedia which defines the term \"high-energy compounds\" to refer exclusively to those.\nThe IUPAC glossary of terms used in ecotoxicology defines a primary producer as an \"organism capable of using the energy derived from light or a chemical substance in order to manufacture energy-rich organic compounds\". However, IUPAC does not formally define the meaning of \"energy-rich\".\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Energy-rich species",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_656979",
    "text": "Environmental chemistry is the scientific study of the chemical and biochemical phenomena that occur in natural places. It should not be confused with green chemistry, which seeks to reduce potential pollution at its source. It can be defined as the study of the sources, reactions, transport, effects, and fates of chemical species in the air, soil, and water environments; and the effect of human activity and biological activity on these. Environmental chemistry is an interdisciplinary science that includes atmospheric, aquatic and soil chemistry, as well as heavily relying on analytical chemistry and being related to environmental and other areas of science.\nEnvironmental chemistry involves first understanding how the uncontaminated environment works, which chemicals in what concentrations are present naturally, and with what effects. Without this it would be impossible to accurately study the effects humans have on the environment through the release of chemicals.\nEnvironmental chemists draw traditional chemical concepts as well as sampling and analytical techniques.\n\nContaminant\nA contaminant is a substance present in nature at a level higher than fixed levels or that would not otherwise be there.   This may be due to human activity and bioactivity. The term contaminant is often used interchangeably with pollutant, which is a substance that detrimentally impacts the surrounding environment.  While a contaminant is sometimes a substance in the environment as a result of human activity, but without harmful effects, it sometimes the case that toxic or harmful effects from contamination only become apparent at a later date.\nThe \"medium\" such as  soil or organism such as  fish affected by the pollutant or contaminant is called a receptor, whilst a sink is a chemical medium or species that retains and interacts with the pollutant such as carbon sink and its effects by microbes.\nEnvironmental indicators\nChemical measures of water quality include dissolved oxygen (DO), chemical oxygen demand (COD), biochemical oxygen demand (BOD), total dissolved solids (TDS), pH, nutrients (nitrates and phosphorus), heavy metals, soil chemicals (including copper, zinc, cadmium, lead and mercury), and pesticides.\n",
    "source": "wikipedia",
    "title": "Environmental chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_1631889",
    "text": "In chemistry, equivalent weight (more precisely, equivalent mass) is the mass of one equivalent, that is the mass of a given substance which will combine with or displace a fixed quantity of another substance. The equivalent weight of an element is the mass which combines with or displaces 1.008 gram of hydrogen or 8.0 grams of oxygen or 35.5 grams of chlorine. The corresponding unit of measurement is sometimes expressed as \"gram equivalent\".\nThe equivalent weight of an element is the mass of a mole of the element divided by the element's valence. That is, in grams, the atomic weight of the element divided by the usual valence.  For example, the equivalent weight of oxygen is 16.0/2 = 8.0 grams.\nFor acid–base reactions, the equivalent weight of an acid or base is the mass which supplies or reacts with one mole of hydrogen cations (H+). For redox reactions, the equivalent weight of each reactant supplies or reacts with one mole of electrons (e−) in a redox reaction.\nEquivalent weight has the units of mass, unlike atomic weight, which is now used as a synonym for relative atomic mass and is dimensionless. Equivalent weights were originally determined by experiment, but (insofar as they are still used) are now derived from molar masses. The equivalent weight of a compound can also be calculated by dividing the molecular mass by the number of positive or negative electrical charges that result from the dissolution of the compound.\n\nIn history\nThe first equivalent weights were published for acids and bases by Carl Friedrich Wenzel in 1777. A larger set of tables was prepared, possibly independently, by Jeremias Benjamin Richter, starting in 1792. However, neither Wenzel nor Richter had a single reference point for their tables, and so had to publish separate tables for each pair of acid and base.\nJohn Dalton's first table of atomic weights (1808) suggested a reference point, at least for the elements: taking the equivalent weight of hydrogen to be one unit of mass. However, Dalton's atomic theory was far from universally accepted in the early 19th century. One of the greatest problems was the reaction of hydrogen with oxygen to produce water. One gram of hydrogen reacts with eight grams of oxygen to produce nine grams of water, so the equivalent weight of oxygen was defined as eight grams. Since Dalton supposed (incorrectly) that a water molecule consisted of one hydrogen and one oxygen atom, this would imply an atomic weight of oxygen equal to eight. However, expressing the reaction in terms of gas volumes following Gay-Lussac's law of combining gas volumes, two volumes of hydrogen react with one volume of oxygen to produce two volumes of water, suggesting (correctly) that the atomic weight of oxygen is sixteen. The work of Charles Frédéric Gerhardt (1816–56), Henri Victor Regnault (1810–78) and Stanislao Cannizzaro (1826–1910) helped to rationalise this and many similar paradoxes, but the problem was still the subject of debate at the Karlsruhe Cong",
    "source": "wikipedia",
    "title": "Equivalent weight",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_37125569",
    "text": "Estimated maximum possible concentration (EMPC) is a term used in dioxin concentration determination for a concentration between limit of quantification and limit of detection.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Estimated maximum possible concentration",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_152969",
    "text": "A eutectic system or eutectic mixture ( yoo-TEK-tik) is a type of a homogeneous mixture that has a melting point lower than those of the constituents. The lowest possible melting point over all of the mixing ratios of the constituents is called the eutectic temperature. On a phase diagram, the eutectic temperature is seen as the eutectic point (see plot).\nNon-eutectic mixture ratios have different melting temperatures for their different constituents, since one component's lattice will melt at a lower temperature than the other's. Conversely, as a non-eutectic mixture cools down, each of its components solidifies into a lattice at a different temperature, until the entire mass is solid.  A non-eutectic mixture thus does not have a single melting/freezing point temperature at which it changes phase, but rather a temperature at which it changes between liquid and slush (known as the liquidus) and a lower temperature at which it changes between slush and solid (the solidus).\nIn the real world, eutectic properties can be used to advantage in such processes as eutectic bonding, where silicon chips are bonded to gold-plated substrates with ultrasound, and eutectic alloys prove valuable in such diverse applications as soldering, brazing, metal casting, electrical protection, fire sprinkler systems, and nontoxic mercury substitutes.\nThe term eutectic was coined in 1884 by the British physicist and chemist Frederick Guthrie (1833–1886). The word originates from Greek  εὐ- (eû) 'well' and  τῆξῐς (têxis) 'melting'. Before his studies, chemists assumed \"that the alloy of minimum fusing point must have its constituents in some simple atomic proportions\", but he showed that that is not always true.\n\nEutectic phase transition\nThe eutectic solidification is defined as follows:\n\n  \n    \n      \n        \n          Liquid\n        \n        \n        \n          \n            →\n            \n              \n                cooling\n              \n            \n            \n              \n                \n                  eutectic\n                \n                \n                  temperature\n                \n              \n            \n          \n        \n        \n        α\n        \n           solid solution\n        \n         \n        +\n         \n        β\n        \n           solid solution\n        \n      \n    \n    {\\displaystyle {\\text{Liquid}}\\quad {\\xrightarrow[{\\text{cooling}}]{{\\text{eutectic}} \\atop {\\text{temperature}}}}\\quad \\alpha {\\text{ solid solution}}\\ +\\ \\beta {\\text{ solid solution}}}\n  \n\nThis type of reaction is an invariant reaction, because it is in thermal equilibrium; another way to define this is the change in Gibbs free energy equals zero. Tangibly, this means the liquid and two solid solutions all coexist at the same time and are in chemical equilibrium. There is also a thermal arrest for the duration of the phase change during which the temperature of the system does not change.\nThe resulting solid macrostructure from a eutectic reaction depends on ",
    "source": "wikipedia",
    "title": "Eutectic system",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_56824574",
    "text": "A field effect is the polarization of a molecule through space. The effect is a result of an electric field produced by charge localization in a molecule. This field, which is substituent and conformation dependent, can influence structure and reactivity by manipulating the location of electron density in bonds and/or the overall molecule. The polarization of a molecule through its bonds is a separate phenomenon known as induction. Field effects are relatively weak, and diminish rapidly with distance, but have still been found to alter molecular properties such as acidity.\n\nField sources\nField effects can arise from the electric dipole field of a bond containing an electronegative atom or electron-withdrawing substituent, as well as from an atom or substituent bearing a formal charge. The directionality of a dipole, and concentration of charge, can both define the shape of a molecule's electric field which will manipulate the localization of electron density toward or away from sites of interest, such as an acidic hydrogen. Field effects are typically associated with the alignment of a dipole field with respect to a reaction center. Since these are through space effects, the 3D structure of a molecule is an important consideration. A field may be interrupted by other bonds or atoms before propagating to a reactive site of interest. Atoms of differing electronegativities can move closer together resulting in bond polarization through space that mimics the inductive effect through bonds. Bicycloheptane and bicyclooctane (seen left) are \npounds in which the change in acidity with substitution was attributed to the field effect. The C-X dipole is oriented away from the carboxylic acid group, and can draw electron density away because the molecule center is empty, with a low dielectric constant, so the electric field is able to propagate with minimal resistance.\nUtility of effect\nA dipole can align to stabilize or destabilize the formation or loss of a charge, thereby decreasing (if stabilized) or increasing (if destabilized) the activation barrier to a chemical event. Field effects can therefore tune the acidity or basicity of bonds within their fields by donating or withdrawing charge density. With respect to acidity, a common trend to note is that, inductively, an electron-withdrawing substituent in the vicinity of an acidic proton will lower the pKa (i.e. increase the acidity) and, correspondingly, an electron-donating substituent will raise the pKa. The reorganization of charge due to field effects will have the same result. An electric dipole field propagated through the space around, or in the middle of, a molecule in the direction of an acidic proton will decrease the acidity, while a dipole pointed away will increase the acidity and  concomitantly elongate the X-H bond. These effects can therefore help to tune the acidity/basicity of a molecule to protonate/deprotonate a specific compound, or enhance hydrogen bond-donor ability for molecular ",
    "source": "wikipedia",
    "title": "Field effect (chemistry)",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_2275867",
    "text": "Forensic chemistry is the application of chemistry and its subfield, forensic toxicology, in a legal setting.  A forensic chemist can assist in the identification of unknown materials found at a crime scene.  Specialists in this field have a wide array of methods and instruments to help identify unknown substances.  These include high-performance liquid chromatography, gas chromatography-mass spectrometry, atomic absorption spectroscopy, Fourier transform infrared spectroscopy, and thin layer chromatography.  The range of different methods is important due to the destructive nature of some instruments and the number of possible unknown substances that can be found at a scene. Forensic chemists prefer using nondestructive methods first, to preserve evidence and to determine which destructive methods will produce the best results.\nAlong with other forensic specialists, forensic chemists commonly testify in court as expert witnesses regarding their findings. Forensic chemists follow a set of standards that have been proposed by various agencies and governing bodies, including the Scientific Working Group on the Analysis of Seized Drugs.  In addition to the standard operating procedures proposed by the group, specific agencies have their own standards regarding the quality assurance and quality control of their results and their instruments.  To ensure the accuracy of what they are reporting, forensic chemists routinely check and verify that their instruments are working correctly and are still able to detect and measure various quantities of different substances.\n\nRole in investigations\nForensic chemists' analysis can provide leads for investigators, and they can confirm or refute their suspicions.  The identification of the various substances found at the scene can tell investigators what to look for during their search.  During fire investigations, forensic chemists can determine if an accelerant such as gasoline or kerosene was used; if so, this suggests that the fire was intentionally set.  Forensic chemists can also narrow down the suspect list to people who would have access to the substance used in a crime.  For example, in explosive investigations, the identification of RDX or C-4 would indicate a military connection as those substances are military grade explosives.  On the other hand, the identification of TNT would create a wider suspect list, since it is used by demolition companies as well as in the military.  During poisoning investigations, the detection of specific poisons can give detectives an idea of what to look for when they are interviewing potential suspects. For example, an investigation that involves ricin would tell investigators to look for ricin's precursors, the seeds of the castor oil plant.\nForensic chemists also help to confirm or refute investigators' suspicions in drug or alcohol cases.  The instruments used by forensic chemists can detect minute quantities, and accurate measurement can be important in crimes such a",
    "source": "wikipedia",
    "title": "Forensic chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_31295654",
    "text": "In chemistry, a free element is a chemical element that is not combined with or chemically bonded to other elements. These may either be chemically inert, or may form bonds with atoms of the same element. \nMetals, non-metals, and noble gases can all be found as free elements. Noble gases such as helium and argon are found in the monoatomic state due to the low reactivity of these atoms. Similarly, noble metals such as gold and platinum are also found in the pure state naturally. Non-metals are rarely found as free elements in the solid state — carbon is a notable exception, as it may be found as diamond and graphite. However, they commonly exist as gases, examples of which include molecular oxygen, ozone, and nitrogen, which together make up approximately 99% of the atmosphere. Because of their reactivity, the halogens do not naturally occur in the free elemental state, but they are both widespread and abundant in the form of their halide ions. They are, however, stable in their diatomic forms.\n\nSee also\nNative metal\nNoble metal\nNative element mineral\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Free element",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_46644932",
    "text": "In coordination chemistry and crystallography, the geometry index or structural parameter (τ) is a number ranging from 0 to 1 that indicates what the geometry of the coordination center is. The first such parameter for 5-coordinate compounds was developed in 1984. Later, parameters for 4-coordinate compounds were developed.\n\n5-coordinate compounds\nTo distinguish whether the geometry of the coordination center is trigonal bipyramidal or square pyramidal, the τ5 (originally just τ) parameter was proposed by Addison et al.:\n\n  \n    \n      \n        \n          τ\n          \n            5\n          \n        \n        =\n        \n          \n            \n              β\n              −\n              α\n            \n            \n              60\n              \n                ∘\n              \n            \n          \n        \n        ≈\n        −\n        0.01667\n        α\n        +\n        0.01667\n        β\n      \n    \n    {\\displaystyle \\tau _{5}={\\frac {\\beta -\\alpha }{60^{\\circ }}}\\approx -0.01667\\alpha +0.01667\\beta }\n  \n\nwhere: β > α are the two greatest valence angles of the coordination center.\nWhen τ5 is close to 0 the geometry is similar to square pyramidal, while if τ5 is close to 1 the geometry is similar to trigonal bipyramidal:\n4-coordinate compounds\nIn 2007 Houser et al. developed the analogous τ4 parameter to distinguish whether the geometry of the coordination center is square planar or tetrahedral. The formula is:\n\n  \n    \n      \n        \n          τ\n          \n            4\n          \n        \n        =\n        \n          \n            \n              \n                360\n                \n                  ∘\n                \n              \n              −\n              (\n              α\n              +\n              β\n              )\n            \n            \n              \n                360\n                \n                  ∘\n                \n              \n              −\n              2\n              θ\n            \n          \n        \n        ≈\n        −\n        0.00709\n        α\n        −\n        0.00709\n        β\n        +\n        2.55\n      \n    \n    {\\displaystyle \\tau _{4}={\\frac {360^{\\circ }-(\\alpha +\\beta )}{360^{\\circ }-2\\theta }}\\approx -0.00709\\alpha -0.00709\\beta +2.55}\n  \n\nwhere: α and β are the two greatest valence angles of coordination center; θ = cos−1(− 1⁄3) ≈ 109.5° is a tetrahedral angle.\nWhen τ4 is close to 0 the geometry is similar to square planar, while if τ4 is close to 1 then the geometry is similar to tetrahedral. However, in contrast to the τ5 parameter, this does not distinguish α and β angles, so structures of significantly different geometries can have similar τ4 values. To overcome this issue, in 2015 Okuniewski et al. developed parameter τ4′ that adopts values similar to τ4 but better differentiates the examined structures:\n\n  \n    \n      \n        \n          τ\n          \n            4\n          \n          ′\n        \n        =\n        \n          \n            \n              β\n              −\n              α\n ",
    "source": "wikipedia",
    "title": "Geometry index",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_28704496",
    "text": "The Gilchrist–Thomas process or Thomas process is a historical process for refining pig iron, derived from the Bessemer converter. It is named after its inventors who patented it in 1877: Percy Carlyle Gilchrist and his cousin Sidney Gilchrist Thomas. By allowing the exploitation of phosphorous iron ore, the most abundant, this process allowed the rapid expansion of the steel industry outside the United Kingdom and the United States.\n\nThe process differs essentially from the Bessemer process in the refractory lining of the converter. The latter, being made of dolomite ((Ca,Mg)(CO3)2) fired with tar, is basic (MgO giving O2− anions), whereas the Bessemer lining, made of packed sand, is acidic (SiO2 accepting O2− anions) according to the Lux-Flood theory of molten oxides. Phosphorus, by migrating from liquid iron to molten slag, allows both the production of a steel of satisfactory quality, and of phosphates sought after as fertilizer, known as \"Thomas meal\". The disadvantages of the basic process includes larger iron loss and more frequent relining of the converter vessel.\nAfter having favored the spectacular growth of the Lorraine iron and steel industry, the process progressively faded away in front of the Siemens-Martin Open-hearth furnace, which also used the benefit of basic refractory lining, before disappearing in the mid-1960s: with the development of gas liquefaction and the cryogenic separation of O2 from air, the use of pure oxygen became economically viable. Even if modern pure oxygen converters all operate with a basic medium, their performance and operation have little to do with their ancestor.\n\nBibliographic sources\n\nG. Reginald Bashforth, The manufacture of iron and steel, vol. 2: Steel production, London, Chapman & Hall Ltd, 1951, 461 p.\nThomas Turner (dir.), The metallurgy of iron: By Thomas Turner...: Being one of a series of treatises on metallurgy written by associates of the Royal school of mines, C. Griffin & company, limited, coll. \"Griffin's metallurgical series\", 1908, 3rd ed., 463 p. ISBN 978-1-177-69287-8\nWalter MacFarlane, The principles and practice of iron and steel manufacture, Longmans, Green, and Co, 1917, 5th ed.\nR.W. Burnie, Memoir and letters of Sidney Gilchrist Thomas, Inventor, John Murray, 1891\nWilliam Tulloch Jeans, The Creators of the Age of Steel, 1884, 356 p. ISBN 978-1-4179-5381-3\nHermann Wedding (translated from German by: William B. Phillips, Ph.D. & Ernst Prochaska), Wedding's basic Bessemer process [\"Basische Bessemer - oder Thomas-Process\"], New York Scientific Publishing Company, 1891, 224 p.\nJean Duflot, Encyclopædia Universalis, \"Sidérurgie\"\n",
    "source": "wikipedia",
    "title": "Gilchrist–Thomas process",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_13849906",
    "text": "This glossary of chemistry terms is a list of terms and definitions relevant to chemistry, including chemical laws, diagrams and formulae, laboratory tools, glassware, and equipment. Chemistry is a physical science concerned with the composition, structure, and properties of matter, as well as the changes it undergoes during chemical reactions; it features an extensive vocabulary and a significant amount of jargon.\nNote: All periodic table references refer to the IUPAC Style of the Periodic Table.\n\nA\nabsolute zero\nA theoretical condition concerning a system at the lowest limit of the thermodynamic temperature scale, or zero kelvins, at which the system does not emit or absorb energy (i.e. all atoms are at rest). By extrapolating the ideal gas law, the internationally agreed-upon value for absolute zero has been determined as −273.15 °C (−459.67 °F; 0.00 K).\n\nabsorbance\n\nabsorption\n1.  The physical or chemical process by which a substance in one state becomes incorporated into and retained by another substance of a different state. Absorption differs from adsorption in that the first substance permeates the entire bulk of the second substance, rather than just adhering to the surface.\n2.  The process by which matter (typically electrons bound in atoms) takes up the energy of electromagnetic radiation and transforms it into any of various types of internal energy, such as thermal energy. This type of absorption is the principle on which spectrophotometry is based.\n\nabundance\n\naccuracy\nHow close a measured value is to the actual or true value. Compare precision.\n\nacetyl\n\nachiral\n(of a molecule) Having the geometric symmetry of being indistinguishable from its own mirror image; lacking chirality.\n\nacid\n1.  (Brønsted–Lowry acid) Any chemical species or molecular entity that acts as a proton donor when reacting with another species, because it loses at least one proton (H+) which is then transferred or 'donated' to the other species, which by definition is a Brønsted–Lowry base. When dissolved in an aqueous solution, a proton donor which increases the concentration of hydronium ion (H3O+) by transferring protons to water molecules may also be called an Arrhenius acid. The term \"acid\", when not otherwise qualified, often refers implicitly to a Brønsted–Lowry acid.\n2.  (Lewis acid) Any chemical species or molecular entity that acts as an electron pair acceptor when reacting with another species, forming a covalent bond by accepting a lone pair of electrons donated by the other species, which is known as a Lewis base. This definition was intended as a generalization of the Brønsted–Lowry definition by proposing that acid-base reactions are best viewed as reorganizations of electrons rather than transfers of protons, with the acid being a species that accepts electron pairs from another species either directly or by releasing protons (H+) into the solution, which then accept electron pairs from the other species. The Lewis definition is inclusive of many B",
    "source": "wikipedia",
    "title": "Glossary of chemistry terms",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_47772755",
    "text": "C-glycosyltryptophan is an indolyl carboxylic amino acid with the structural formula C17H22N2O7. This sugar-loaded amino acid strongly correlates with age.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "C-glycosyl tryptophan",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_936085",
    "text": "Green chemistry, similar to sustainable chemistry or circular chemistry, is an area of chemistry and chemical engineering focused on the design of products and processes that minimize or eliminate the use and generation of hazardous substances. While environmental chemistry focuses on the effects of polluting chemicals on nature, green chemistry focuses on the environmental impact of chemistry, including lowering consumption of nonrenewable resources and technological approaches for preventing pollution.\nThe overarching goals of green chemistry—namely, more resource-efficient and inherently safer design of molecules, materials, products, and processes—can be pursued in a wide range of contexts.\n\nDefinition\nGreen chemistry (also called sustainable chemistry) is the design of chemical products and processes that reduce or eliminate the use and generation of hazardous substances. The concept integrates pollution-prevention and process-intensification approaches at laboratory and industrial scales to improve resource efficiency and minimize waste and risk across the life cycle of chemicals and materials.\nHistory\nGreen chemistry evolved and emerged from a variety of existing ideas and research efforts (such as Pollution Prevention, atom economy and catalysis) in the period leading up to the 1990s, in the context of increasing attention to problems of chemical pollution and resource depletion. The development of green chemistry in Europe and the United States was proceeded by a shift in environmental problem-solving strategies: a movement from command and control regulation and mandated lowering of industrial emissions at the \"end of the pipe,\" toward the broad interdisciplinary concept of prevention of pollution through the innovative design of production technologies themselves. The narrower set of concepts later recognized and re-named as green chemistry coalesced in the mid- to late-1990s, along with broader adoption of the new term in the Academic literature (which prevailed over earlier competing terms such as \"clean\" and \"sustainable\" chemistry).\nIn the United States, the Environmental Protection Agency played a significant supporting role in evolving green chemistry out of its earlier pollution prevention programs, funding, and cooperative coordination with industry. At the same time in the United Kingdom, researchers at the University of York, who used the term \"clean technology\" in the early 1990s, contributed to the establishment of the Green Chemistry Network within the Royal Society of Chemistry, and the launch of the journal Green Chemistry. In 1991, in the Netherlands, a special issue called 'green chemistry' [groene chemie] was published in Chemisch Magazine. In the Dutch context, the umbrella term green chemistry was associated with the exploitation of biomass as a renewable feedstock.\n",
    "source": "wikipedia",
    "title": "Green chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_79146569",
    "text": "Group Fertiberia (in English: Fertiberia Group) is a Spanish business conglomerate in the chemical industry. It has been operating since 1995 and has its origins in the historical company Fertiberia, whose activities have expanded throughout Spain, France and Portugal. Today, the group is one of the leading producers of fertilizers, ammonia and its byproducts in the European Union.\n\nHistory\nIn 1995, the Villar Mir Group took control of the assets of Fesa-Enfersa, which would form the basis for the re-foundation of the historic company Fertiberia. After a period of internal reorganization, which included plant closures and workforce reductions, Fertiberia began a period of expansion. This strategy included the creation of several subsidiary companies and the acquisition of other companies in the sector, such as Sefanitro, ASUR, Química del Estroncio, Fercampo, etc. This has led to Grupo Fertiberia being considered “the leading company in the fertilizer sector in Spain”, although its presence has subsequently extended to Portugal and France. In 2020, the Swedish-German group Triton Partners took control of Grupo Fertiberia.\nAffiliates\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Grupo Fertiberia",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_13335826",
    "text": "The International Chemical Identifier (InChI, pronounced  IN-chee) is a textual identifier for chemical substances, designed to provide a standard way to encode molecular information and to facilitate the search for such information in databases and on the web. Initially developed by the International Union of Pure and Applied Chemistry (IUPAC) and National Institute of Standards and Technology (NIST) from 2000 to 2005, the format and algorithms are non-proprietary. Since May 2009, it has been developed by the InChI Trust, a nonprofit charity from the United Kingdom which works to implement and promote the use of InChI.\nThe identifiers describe chemical substances in terms of layers of information — the atoms and their bond connectivity, tautomeric information, isotope information, stereochemistry, and electronic charge information.\nNot all layers have to be provided; for instance, the tautomer layer can be omitted if that type of information is not relevant to the particular application. The InChI algorithm converts input structural information into a unique InChI identifier in a three-step process: normalization (to remove redundant information), canonicalization (to generate a unique number label for each atom), and serialization (to give a string of characters).\nInChIs differ from the widely used CAS registry numbers in three respects: firstly, they are freely usable and non-proprietary; secondly, they can be computed from structural information and do not have to be assigned by some organization; and thirdly, most of the information in an InChI is human readable (with practice). InChIs can thus be seen as akin to a general and extremely formalized version of IUPAC names. They can express more information than the simpler SMILES notation and, in contrast to SMILES strings, every structure has a unique InChI string, which is important in database applications. Information about the 3-dimensional coordinates of atoms is not represented in InChI; for this purpose a format such as PDB can be used.\nThe InChIKey, sometimes referred to as a hashed InChI, is a fixed length (27 character) condensed digital representation of the InChI that is not human-understandable. The InChIKey specification was released in September 2007 in order to facilitate web searches for chemical compounds, since these were problematic with the full-length InChI. Unlike the InChI, the InChIKey is not unique: though collisions are expected to be extremely rare, there are known collisions.\nIn January 2009 the 1.02 version of the InChI software was released. This provided a means to generate so called standard InChI, which does not allow for user selectable options in dealing with the stereochemistry and tautomeric layers of the InChI string. The standard InChIKey is then the hashed version of the standard InChI string. The standard InChI will simplify comparison of InChI strings and keys generated by different groups, and subsequently accessed via diverse sources such as databa",
    "source": "wikipedia",
    "title": "International Chemical Identifier",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_78626621",
    "text": "Intrinsic DNA fluorescence is the fluorescence emitted directly by DNA when it absorbs ultraviolet (UV) radiation. It contrasts to that stemming from fluorescent labels that are either simply bound to DNA or covalently attached to it, widely used in biological applications; such labels may be chemically modified, not naturally occurring, nucleobases.\nThe intrinsic DNA fluorescence was discovered in the 1960s by studying nucleic acids in low temperature glasses. Since the beginning of the 21st century, the much weaker emission of nucleic acids in fluid solutions is being studied at room temperature by means sophisticated spectroscopic techniques, using as UV source femtosecond laser pulses, and following the evolution of the emitted light from femtoseconds to nanoseconds. The development of specific experimental protocols has been crucial for obtaining reliable results.  \nFluorescence studies combined to theoretical computations and transient absorption measurements bring information about the relaxation of the electronic excited states and, thus, contribute to understanding the very first steps of a complex series of events triggered by UV radiation, ultimately leading to DNA damage. The principles governing the behavior of the intrinsic RNA fluorescence, to which only a few studies have been dedicated,\n are the same as those described for DNA.\nThe knowledge of the fundamental processes underlying the DNA fluorescence paves the way for the development of label-free biosensors. The development of such optoelectronic devices for certain applications would have the advantage of bypassing the step of chemical synthesis or avoiding the uncertainties due to non-covalent biding of fluorescent dyes to nucleic acids.\n\nConditions for measuring the intrinsic DNA fluorescence\nDue to the weak intensity of the intrinsic DNA fluorescence, specific cautions are necessary in order to perform correct measurements and obtain reliable results. A first requirement concerns the purity of both the DNA samples and that of the chemicals and the water used to the preparation of the buffered solutions. The  buffer emission must be systematically recorded and, in certain cases, subtracted in an appropriate way. A second requirement is associated with the DNA damage provoked by the exciting UV light which alters its fluorescence. In order to overcome these difficulties, continuous stirring of the solution is needed. For measurements using laser excitation, the circulation of the DNA solution by means of a peristaltic pump is recommended; the reproducibility of successive fluorescence signal needs to be checked.\n",
    "source": "wikipedia",
    "title": "Intrinsic DNA fluorescence",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_53076456",
    "text": "Ioliomics (from a portmanteau of ions and liquids) is the study of ions in liquids (or liquid phases) and stipulated with fundamental differences of ionic interactions. Ioliomics covers a broad research area concerning structure, properties and applications of ions involved in various biological and chemical systems. The concept of this research discipline is related to other comprehensive research fields, such as genomics, proteomics, glycomics, petroleomics, etc., where the suffix -omics is used for describing the comprehensiveness of data.\n\nFundamental nature\nThe nature of chemical reactions and their description is one of the most fundamental problems in chemistry. The concepts of covalent and ionic bonds which emerged in the beginning of the 20th century specify the profound differences between their electronic structures. These differences, in turn, lead to dramatically different behavior of covalent and ionic compounds both in the solution and solid phase.  In the solid phase, ionic compounds, e.g. salts, are prone to formation of crystal lattices; in polar solvents, they dissociate into ions surrounded by solvate shells, thus rendering the solution highly ionic conductive.  In contrast to covalent bonds, ionic interactions demonstrate flexible, dynamic behavior, which allows tuning ionic compounds to obtain desired properties.\nImportance\nIonic compounds interact strongly with the solvent medium; therefore, their impact on chemical and biochemical processes involving ions can be significant. Even in the case of simplest ions and solvents, the presence of the former can lead to rearrangement and restructuring of the latter.  It is established that ionic reactions are involved in numerous phenomena at the scales of whole galaxies or single living cells. To name a few, in living cells, metal ions bind to metalloenzymes and other proteins therefore modulating their activity; ions are involved in the control of neuronal functioning during sleep – wakefulness cycles;  anomalous activity of ion channels results in the development of various disorders, such as Parkinson's and Alzheimer's diseases,  etc. Thus, despite the problems associated with the studies on properties and activities of ions in various chemical and biological systems, this research field is among the most urgent ones.\n",
    "source": "wikipedia",
    "title": "Ioliomics",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_6756239",
    "text": "Landolt–Börnstein is a collection of property data in materials science and the closely related fields of chemistry, physics and engineering published by Springer Nature.\n\nHistory\nOn July 28, 1882, Dr. Hans Heinrich Landolt and Dr. Richard Börnstein, both professors at the \"Landwirtschaftliche Hochschule\" (Agricultural College) at Berlin, signed a contract with the publisher Ferdinand Springer on the publication of a collection of tables with physical-chemical data. The title of this book \"Physikalisch-chemische Tabellen\" (Physical-Chemical Tables) published in 1883 was soon forgotten. Owing to its success the data collection has been known for more than a hundred years by each scientist only as \"The Landolt-Börnstein\".\n1250 copies of the 1st Edition were printed and sold. In 1894, the 2nd Edition was published, in 1905 the 3rd Edition, in 1912 the 4th Edition, and finally in 1923 the 5th Edition. Supplementary volumes of the latter were printed until as late as 1936. New Editions saw changes in large expansion of volumes, number of authors, updated structure, additional tables and coverage of new areas of physics and chemistry.\nThe 5th Edition was eventually published in 1923, consisting of two volumes and comprising a total of 1,695 pages. Sixty three authors had contributed to it. The growth that had already been noticed in previous editions, continued. It was clear, that \"another edition in approximately 10 years\" was no solution. A complete conceptual change of the Landolt–Börnstein had thus become necessary. For the meantime supplementary volumes in two-year intervals should be provided to fill in the blanks and add the latest data. The first supplementary volume of the 5th Edition was published in 1927, the second in 1931 and the third in 1935/36. The latter consisted of three sub-volumes with a total of 3,039 pages and contributions from 82 authors.\nThe 6th Edition (1950) was published in line with the revised general frame. The basic idea was to have four volumes instead of one, each of which was to cover different fields of the Landolt–Börnstein under different editors. Each volume was given a detailed table of contents. Two major restrictions were also imposed. The author of a contribution was asked to choose a \"Bestwert\" (optimum value) from the mass of statements of an experimental value in the publications of different authors, or derive a \"wahrscheinlichster Wert” (most possible value). The other change of importance was that not only diagrams became as important as tables, but that text also became necessary to explain the presented data.\n",
    "source": "wikipedia",
    "title": "Landolt–Börnstein",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_65216890",
    "text": "Organoantimony chemistry is the chemistry of compounds containing a carbon to antimony (Sb) chemical bond. Relevant oxidation states are SbV and SbIII. The toxicity of antimony limits practical application in organic chemistry.\n\nSyntheses\n\nReactions\nStibine oxides undergo a sort of polarized-olefin metathesis.  For example, they mediate a carbonyl-imine exchange (Ar is any activated arene):Ph3Sb=NSO2Ar + PhC=O → Ph3Sb=O + PhC=NSO2ArThe effect may extend vinylically: \n  \n    \n      \n        \n          \n            R\n            \n              2\n            \n            \n              \n            \n          \n          C\n          \n            =\n          \n          O\n          \n\n          \n          +\n          \n            HBrCHCO\n            \n              2\n            \n            \n              \n            \n          \n          R\n          \n            \n              →\n              \n                \n                  \n                    Bu\n                    \n                      3\n                    \n                    \n                      \n                    \n                  \n                  Sb\n                \n              \n            \n          \n          \n            R\n            \n              2\n            \n            \n              \n            \n          \n          C\n          \n            =\n          \n          \n            CHCO\n            \n              2\n            \n            \n              \n            \n          \n          R\n          \n\n          \n          +\n          HBr\n        \n      \n    \n    {\\displaystyle {\\ce {R2C=O{}+ HBrCHCO2R ->[{\\ce {Bu3Sb}}] R2C=CHCO2R{}+ HBr}}}\n  \nIn contrast, unstabilized ylides (R3Sb=CR'2; R' not electron-withdrawing) form only with difficulty (e.g. diazo reagents).\nLike other metals, stibanes vicinal to a leaving group can eliminate before a proton.  For example, diphenyl(β-hydroxyphenethyl)stibine decomposes in heat or acid to styrene:\n\nPh2SbCH2CH(OH)Ph → CH2=CHPh + Ph2SbOH\nAs tertiary stibines also insert into haloalkyl bonds, tertiary stibines are powerful dehalogenating agents.  However, stibanes poorly imitate active metal organometallics: only with difficulty do their ligands add to carbonyls or they power noble-metal cross couplings.\nStiboranes are gentle oxidants, converting acyloins to diketones and thiols to disulfides.  In air, tris(thiophenyl)stibine catalyzes a Hunsdiecker-like decarboxylative oxidation of anhydrides to alcohols.\nIn ultraviolet light, distibines radicalize; the resulting radicals can displace iodide.\n",
    "source": "wikipedia",
    "title": "Organoantimony chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_52754183",
    "text": "Liquid nitrogen wash is a process mainly used for the production of ammonia synthesis gas within fertilizer production plants. It is usually the last purification step in the ammonia production process sequence upstream of the actual ammonia production.\n\nCompeting technologies\nThe purpose of the final purification step upstream of the actual ammonia production is to remove all components that are poisonous for the sensitive ammonia synthesis catalyst. This can be done with the following concepts:\n\nMethanation, formally the standard concept with the disadvantage, that the methane content is not removed, but even increased, since in this process, the carbon oxides (carbon monoxide and carbon dioxide) are converted to methane.\nPressure Swing Adsorption, which can replace the low temperature shift, the carbon dioxide removal and the methanation, since this process produces pure hydrogen, which can be mixed with pure nitrogen.\nLiquid Nitrogen Wash, which produces an ammonia syngas for a so-called \"inert free\" ammonia synthesis loop, that can be operated without the withdrawal of a purge gas stream.\nFunctions\nThe liquid nitrogen wash has two principle functions: \n\nRemoval of impurities such as carbon monoxide, argon and methane from the crude hydrogen gas\nAddition of the required stoichiometric amount of nitrogen to the hydrogen stream to achieve the correct ammonia synthesis gas ratio of hydrogen to nitrogen of 3 : 1\nThe carbon monoxide must be removed completely from the synthesis gas (i.e. syngas) since it is poisonous for the sensitive ammonia synthesis catalyst. \nThe components argon and methane are inert gases within the ammonia synthesis loop, but would enrich there and call for a purge gas system with synthesis gas losses or additional expenditures for a purge gas separation unit. \nThe main sources for the supply of feed gases are partial oxidation processes.\nUpstream syngas preparations\nSince the synthesis gas exiting the partial oxidation process consists mainly of carbon monoxide and hydrogen, usually a sulfur tolerant CO shift (i.e. water-gas shift reaction) is installed in order to convert as much carbon monoxide into hydrogen as possible.\nShifting carbon monoxide and water into hydrogen also produces carbon dioxide, usually this is removed in an acid gas scrubbing process together with other sour gases as e.g. hydrogen sulfide (e.g. in a Rectisol Wash Unit).\n",
    "source": "wikipedia",
    "title": "Liquid nitrogen wash",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_30897833",
    "text": "Magnetochemistry is concerned with the magnetic properties of chemical compounds and elements. Magnetic properties arise from the spin and orbital angular momentum of the electrons contained in a compound. Compounds are diamagnetic when they contain no unpaired electrons. Molecular compounds that contain one or more unpaired electrons are paramagnetic. The magnitude of the paramagnetism is expressed as an effective magnetic moment, μeff. For first-row transition metals the magnitude of μeff is, to a first approximation, a simple function of the number of unpaired electrons, the spin-only formula. In general, spin–orbit coupling causes μeff to deviate from the spin-only formula. For the heavier transition metals, lanthanides and actinides, spin–orbit coupling cannot be ignored. Exchange interaction can occur in clusters and infinite lattices, resulting in ferromagnetism, antiferromagnetism or ferrimagnetism depending on the relative orientations of the individual spins.\n\nMagnetic susceptibility\nThe primary measurement in magnetochemistry is magnetic susceptibility. This measures the strength of interaction on placing the substance in a magnetic field. The volume magnetic susceptibility, represented by the symbol \n  \n    \n      \n        \n          χ\n          \n            v\n          \n        \n      \n    \n    {\\displaystyle \\chi _{v}}\n  \n is defined by the relationship\n\n  \n    \n      \n        \n          \n            \n              M\n              →\n            \n          \n        \n        =\n        \n          χ\n          \n            v\n          \n        \n        \n          \n            \n              H\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {M}}=\\chi _{v}{\\vec {H}}}\n  \n\nwhere, \n  \n    \n      \n        \n          \n            \n              M\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {M}}}\n  \n is the magnetization of the material (the magnetic dipole moment per unit volume), measured in amperes per meter (SI units), and \n  \n    \n      \n        \n          \n            \n              H\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {H}}}\n  \n is the magnetic field strength, also measured in amperes per meter. Susceptibility is a dimensionless quantity. For chemical applications the molar magnetic susceptibility (χmol) is the preferred quantity. It is measured in m3·mol−1 (SI) or cm3·mol−1 (CGS) and is defined as\n\n  \n    \n      \n        \n          χ\n          \n            mol\n          \n        \n        =\n        M\n        \n          χ\n          \n            v\n          \n        \n        \n          /\n        \n        ρ\n      \n    \n    {\\displaystyle \\chi _{\\text{mol}}=M\\chi _{v}/\\rho }\n  \n\nwhere ρ is the density in kg·m−3 (SI) or g·cm−3 (CGS) and M is molar mass in kg·mol−1 (SI) or g·mol−1 (CGS).\n\nA variety of methods are available for the measurement of magnetic susceptibility.\n\nWith the Gouy balance the weight change of the",
    "source": "wikipedia",
    "title": "Magnetochemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_1570072",
    "text": "Mathematical chemistry is the area of research engaged in novel applications of mathematics to chemistry; it concerns itself principally with the mathematical modeling of chemical phenomena. Mathematical chemistry has also sometimes been called computer chemistry, but should not be confused with computational chemistry.\nMajor areas of research in mathematical chemistry include chemical graph theory, which deals with topology such as the mathematical study of isomerism and the development of topological descriptors or indices which find application in quantitative structure-property relationships; and chemical aspects of group theory, which finds applications in stereochemistry and quantum chemistry. Another important area is molecular knot theory and circuit topology that describe the topology of folded linear molecules such as proteins and nucleic acids.\nThe history of the approach may be traced back to the 19th century. Georg Helm published a treatise titled \"The Principles of Mathematical Chemistry: The Energetics of Chemical Phenomena\" in 1894. Some of the more contemporary periodical publications specializing in the field are MATCH Communications in Mathematical and in Computer Chemistry, first published in 1975, and the Journal of Mathematical Chemistry, first published in 1987. In 1986 a series of annual conferences MATH/CHEM/COMP taking place in Dubrovnik was initiated by the late Ante Graovac.\nThe basic models for mathematical chemistry are molecular graph and topological index.\nIn 2005 the International Academy of Mathematical Chemistry (IAMC) was founded in Dubrovnik (Croatia) by Milan Randić. The Academy has 82 members (2009) from all over the world, including six scientists awarded with a Nobel Prize.\n\nSee also\nChemical reaction network theory – Area of applied mathematics\nCheminformatics – Computational chemistry\nCombinatorial chemistry – Compound library-based chemical synthesis method\nMolecular descriptor\nMolecular modelling – Discovering chemical properties by physical simulations\nList of quantum chemistry and solid state physics software\nList of software for molecular mechanics modeling\nRandom graph theory of gelation – Mathematical theory for sol–gel processes\n",
    "source": "wikipedia",
    "title": "Mathematical chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_710202",
    "text": "Mechanochemistry (or mechanical chemistry) is the initiation of chemical reactions by mechanical phenomena. Mechanochemistry thus represents a fourth way to cause chemical reactions, complementing thermal reactions in fluids, photochemistry, and electrochemistry. Conventionally mechanochemistry focuses on the transformations of covalent bonds by mechanical force. Not covered by the topic are many phenomena: phase transitions, dynamics of biomolecules (docking, folding), and sonochemistry. Mechanochemistry also encompasses mechanophores which are molecules that undergo predictable changes in response to applied stress. Two types of mechanophores are mechanochromic ones in which a force causes a change in molecular structure and subsequently color and acid releasing mechanophores that release small amounts of an acid such as HCl in response to an applied force. \nMechanochemistry is not the same as mechanosynthesis, which refers specifically to the machine-controlled construction of complex molecular products.\nIn natural environments, mechanochemical reactions are frequently induced by physical processes such as earthquakes, glacier movement or hydraulic action of rivers or waves. In extreme environments such as subglacial lakes, hydrogen generated by mechnochemical reactions involving crushed silicate rocks and water can support methanogenic microbial communities. And mechanochemistry may have generated oxygen in the ancient Earth by water splitting on fractured mineral surfaces at high temperatures, potentially influencing life's origin or early evolution.\n\nHistory\nThe primal mechanochemical project was to make fire by rubbing pieces of wood against each other, creating friction and hence heat, triggering combustion at the elevated temperature. Another method involves the use of flint and steel, during which a spark (a small particle of pyrophoric metal) spontaneously combusts in air, starting fire instantaneously.\nIndustrial mechanochemistry began with the grinding of two solid reactants. Mercuric sulfide (the mineral cinnabar) and copper metal thereby react to produce mercury and copper sulfide:\n\nHgS + 2Cu → Hg + Cu2S\nA special issue of Chemical Society Review was dedicated to mechanochemistry.\nScientists recognized that mechanochemical reactions occur in environments naturally due to various processes, and the reaction products have the potential to influence microbial communities in tectonically active regions. The field has garnered increasing attention recently as mechanochemistry has the potential to generate diverse molecules capable of supporting extremophilic microbes, influencing the early evolution of life, developing the systems necessary for the origin of life, or supporting alien life forms. The field has now inspired the initiation of a special research topic in the journal Frontiers in Geochemistry.\n",
    "source": "wikipedia",
    "title": "Mechanochemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_71234088",
    "text": "A mental gland is a part of the body found in many species of amphibians and reptiles.  Mental glands produce chemicals that conspecific animals use to communicate.\n\nLocation\nThe mental glands appear in pairs, one on each side of the head. They are located behind the end of the mandible.\nFunction\nMental glands produce hormones that are secreted through the skin. The secretions from mental glands have been implicated in mate selection, species identification, and other functions.\nScientists believe that the head bobbing behavior observed in turtles encountering another member of their own species may serve to disperse the chemicals from the mental glands through the air.  Certain courtship behaviors observed in salamanders, such as snapping, only appear in salamanders that have mental glands, so scientists believe they are also meant to spread the chemicals through the air.\nOrigins and evolution\nNot all reptiles and amphibians have mental glands. It is not unusual for some species in the same family to have mental glands while others do not.\nIn 2021, one team of scientists found that most turtles that have mental glands are aquatic.  They concluded that mental glands developed once in turtles, in the ancestor of the family Testudinoidea, and that all turtles that have mental glands develop them from tissue of homologous origin.  They inferred that turtles that do not have mental glands lost them.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Mental gland",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_61854053",
    "text": "Metal Assisted Chemical Etching (also known as MACE) is the process of wet chemical etching of semiconductors (mainly silicon) with the use of a metal catalyst, usually deposited on the surface of a semiconductor in the form of a thin film or nanoparticles. The semiconductor, covered with the metal, is then immersed in an etching solution containing an oxidizing agent and hydrofluoric acid. The metal on the surface catalyzes the reduction of the oxidizing agent and therefore in turn also the dissolution of silicon. In the majority of the conducted research this phenomenon of increased dissolution rate is also spatially confined, such that it is increased in close proximity to a metal particle at the surface. Eventually this leads to the formation of straight pores that are etched into the semiconductor (see figure to the right). This means that a pre-defined pattern of the metal on the surface can be directly transferred to a semiconductor substrate.\n\nHistory of development\nMACE is a relatively new technology in semiconductor engineering and therefore it has yet to be a process that is used in industry. The first attempts of MACE consisted of a silicon wafer that was partially covered with aluminum and then immersed in an etching solution. This material combination led to an increased etching rate compared to bare silicon. Often this very first attempt is also called galvanic etching instead of metal assisted chemical etching.\nFurther research showed that a thin film of a noble metal deposited on a silicon wafer's surface can also locally increase the etching rate. In particular, it was observed that noble metal particles sink down into the material when the sample is immersed in an etching solution containing an oxidizing agent and hydrofluoric acid (see image in the introduction).  This method is now commonly called the metal assisted chemical etching of silicon.   \nOther semiconductors were also successfully etched with MACE, such as silicon carbide or gallium nitride. However, the main portion of research is dedicated to MACE of silicon.\nIt has been shown that both noble metals such as gold, platinum, palladium, and silver, and base metals such as iron, nickel, copper, and aluminium can act as a catalyst in the process.\n",
    "source": "wikipedia",
    "title": "Metal assisted chemical etching",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_49072305",
    "text": "Methane functionalization is the process of converting methane in its gaseous state to another molecule with a functional group, typically methanol or acetic acid, through the use of transition metal catalysts.\nIn the realm of carbon-hydrogen bond activation and functionalization (C-H activation/functionalization), many recent efforts have been made in order to catalytically functionalize  the C-H bonds in methane. The large abundance of methane in natural gas or shale gas deposits presents a large potential for its use as a feedstock in modern chemistry. However, given its gaseous natural state, it is quite difficult to transport economically. Its ideal use would be as a raw starting material for methanol or acetic acid synthesis, with plants built at the source to eliminate the issue of transportation. Methanol, in particular, would be of great use as a potential fuel source, and many efforts have been applied to researching the feasibilities of a methanol economy.\nThe challenges of C-H activation and functionalization present themselves when several factors are taken into consideration. Firstly, the C-H bond is extremely inert and non-polar, with a high bond dissociation energy, making methane a relatively unreactive starting material. Secondly, any products formed from methane would likely be more reactive than the starting product, which would be detrimental to the selectivity and yield of the reaction. \n\nThe main strategy currently used to increase the reactivity of methane uses transition metal complexes to activate the carbon-hydrogen bonds. In a typical C-H activation mechanism, a transition metal catalyst coordinates to the C-H bond to cleave it, and convert it into a bond with a lower bond dissociation energy. By doing so, the product can be used in further downstream reactions, since it will usually have a new functional group attached to the carbon. It is also important to note the difference between the terms \"activation\" and \"functionalization,\" since both terms are often used interchangeably, but should be held distinct from each other. Activation refers to the coordination of a metal center to the C-H bond, whereas functionalization occurs when the coordinated metal complex is further reacted with a group \"X\" to result in the functionalized product.\n\nMethane activation\nThe four most common methods of transition metal catalyzed methane activation are the Shilov system, sigma bond metathesis, oxidative addition, and 1,2 addition reactions.\n\nThe Shilov system involves platinum based complexes to produce metal alkyls. It was first discovered when a hydrogen-deuterium exchanged was observed in a deuterated solution with the platinum tetrachloride anion. Shilov et al. then was able to catalytically convert methane into methanol or methyl chloride when a Pt(IV) salt was used as a stoichiometric oxidant. The process is simplified down into three main steps: (1) C-H activation, (2) a redox reaction to form an octahedral intermediate, foll",
    "source": "wikipedia",
    "title": "Methane functionalization",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_19780855",
    "text": "Micro x-ray fluorescence (μXRF) is an elemental analysis technique that relies on the same principles as x-ray fluorescence (XRF). Synchrotron X-rays may be used to provide elemental imaging with biological samples.    \nThe spatial resolution diameter of micro x-ray fluorescence is many orders of magnitude smaller than that of conventional XRF. While a smaller excitation spot can be achieved by restricting x-ray beam using a pinhole aperture, this method blocks much of the x-ray flux which has an adverse effect on the sensitivity of trace elemental analysis. Two types of x-ray optics, polycapillary and doubly curved crystal focusing optics, are able to create small focal spots of just a few micrometers in diameter. By using x-ray optics, the irradiation of the focal spot is much more intense and allows for enhanced trace element analysis and better resolution of small features. Micro x-ray fluorescence using x-ray optics has been used in applications such as forensics, small feature evaluations, elemental mapping, mineralogy, electronics, multi-layered coating analysis, micro-contamination detection, film and plating thickness, biology and environment.\n\nApplication in forensic science\nMicro x-ray fluorescence is among the newest technologies used to detect fingerprints. It is a new visualization technique which rapidly reveals the elemental composition of a sample by irradiating it with a thin beam of X-rays without disturbing the sample. It was discovered recently by scientists at the Los Alamos National Laboratory. The newly discovered technique was then first revealed at the 229th national meeting of the American Chemical Society (March, 2005). This new discovery could prove to be very beneficial to the law enforcement world, because it is expected that μXRF will be able to detect the most complex molecules in fingerprints.\nMichael Bernstein of the American Chemical Society describes how the process works \"Salts such as sodium chloride and potassium chloride excreted in sweat are sometimes present in detectable quantities in fingerprints. Using μXRF, the researchers showed that they could detect the sodium, potassium and chlorine from such salts. And since these salts are deposited along the patterns present in a fingerprint, an image of the fingerprint can be visualized producing an elemental image for analysis.\" This basically means that we can “see” a fingerprint because the salts are deposited mainly along the patterns present in a fingerprint.\nSince μXRF technology uses X-ray technology to detect fingerprints, instead of traditional techniques, the image comes out much clearer. Traditional fingerprints are performed by a technique using powders, liquids or vapors to add color to the fingerprint so it can be distinguished. But sometimes this process may alter the fingerprint or may not be able to detect some of the more complex molecules.\nAnother μXRF application in forensics is GSR (gunshot residue) determination. Some specific elements, ",
    "source": "wikipedia",
    "title": "Micro-X-ray fluorescence",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_50311973",
    "text": "Microfluidic cell culture integrates knowledge from biology, biochemistry, engineering, and physics to develop devices and techniques for culturing, maintaining, analyzing, and experimenting with cells at the microscale. It merges microfluidics, a set of technologies used for the manipulation of small fluid volumes (μL, nL, pL) within artificially fabricated microsystems, and cell culture, which involves the maintenance and growth of cells in a controlled laboratory environment. Microfluidics has been used for cell biology studies as the dimensions of the microfluidic channels are well suited for the physical scale of cells (in the order of magnitude of 10 micrometers). For example, eukaryotic cells have linear dimensions between 10 and 100 μm which falls within the range of microfluidic dimensions. A key component of microfluidic cell culture is being able to mimic the cell microenvironment which includes soluble factors that regulate cell structure, function, behavior, and growth. Another important component for the devices is the ability to produce stable gradients that are present in vivo as these gradients play a significant role in understanding chemotactic, durotactic, and haptotactic effects on cells.\n\nFabrication\nSome considerations for microfluidic devices relating to cell culture include:\n\nfabrication material (e.g., polydimethylsiloxane (PDMS), polystyrene)\nculture region geometry\ncontrol system for delivering and removing media when needed using either passive methods (e.g., gravity-driven flow, capillary pumps, or Laplace pressure based 'passive pumping') or a flow-rate controlled device (i.e., perfusion system)\nFabrication material is crucial as not all polymers are biocompatible, with some materials such as PDMS causing undesirable adsorption or absorption of small molecules. Additionally, uncured PDMS oligomers can leach into the cell culture media, which can harm the microenvironment. As an alternative to commonly used PDMS, there have been advances in the use of thermoplastics (e.g., polystyrene) as a replacement material.\nSpatial organization of cells in microscale devices largely depends on the culture region geometry for cells to perform functions in vivo. For example, long, narrow channels may be desired to culture neurons. The perfusion system chosen might also affect the geometry chosen. For example, in a system that incorporates syringe pumps, channels for perfusion inlet, perfusion outlet, waste, and cell loading would need to be added for the cell culture maintenance. Perfusion in microfluidic cell culture is important to enable long culture periods on-chip and cell differentiation.\nOther critical aspects for controlling the microenvironment include: cell seeding density, reduction of air bubbles as they can rupture cell membranes, evaporation of media due to an insufficiently humid environment, and cell culture maintenance (i.e. regular, timely media changes).\nCell's health is defined as the collective equilibrium act",
    "source": "wikipedia",
    "title": "Microfluidic cell culture",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_7981894",
    "text": "Microscale chemistry (often referred to as small-scale chemistry, in German: Chemie im Mikromaßstab) is an analytical method and also a teaching method widely used at school and at university levels, working with small quantities of chemical substances. While much of traditional chemistry teaching centers on multi-gramme preparations, milligrammes of substances are sufficient for microscale chemistry. In universities, modern and expensive lab glassware is used and modern methods for detection and characterization of the produced substances are very common. In schools and in many countries of the Southern hemisphere, small-scale working takes place with low-cost and even no-cost material. There has always been a place for small-scale working in qualitative analysis, but the new developments can encompass much of chemistry a student is likely to meet.\n\nHistory\nThere are two main strands of the modern approach. \nOne is based on the idea that many of the experiments associated with general chemistry (acids and bases, oxidation and reduction, electrochemistry, etc.) can be carried out in equipment much simpler (injection bottles, dropper bottles, syringes, wellplates, plastic pipettes) and therefore cheaper than the traditional glassware in a laboratory, thus enabling the expansion of the laboratory experiences of students in large classes and to introduce laboratory work into institutions too poorly equipped for standard-type work. Pioneering development in this area was carried out by  Egerton C. Grey (1928), Mahmoud K. El-Marsafy (1989) in Egypt, Stephen Thompson in the US and others. A further application of these ideas was the devising by Bradley of the Radmaste kits in South Africa, designed to make effective chemical experiments possible in developing countries in schools that lack the technical services (electricity, running water) taken for granted in many places.\nThe other strand is the introduction of this approach into synthetic work, mainly in organic chemistry. Here the crucial breakthrough was achieved by Mayo, Pike and Butcher and by Williamson who demonstrated that inexperienced students were able to carry out organic syntheses on a few tens of milligrams, a skill previously thought to require years of training and experience. These approaches were accompanied by the introduction of some specialised equipment, which was subsequently simplified by Breuer without great loss of versatility.\nThere is a great deal of published material available to help in the introduction of such a scheme, providing advice on choice of equipment, techniques and preparative experiments and the flow of such material is continuing through a column in the Journal of Chemical Education called 'The Microscale Laboratory' that has been running for many years.\nScaling down experiments, when combined with modern projection technology, opened up the possibility of carrying out lecture demonstrations of the most hazardous kind in total safety.\nThe approach has been ",
    "source": "wikipedia",
    "title": "Microscale chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_66400985",
    "text": "Microsegregation is a non-uniform chemical separation and concentration of elements or impurities in alloys after they have solidified.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Microsegregation",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_47305230",
    "text": "A mixed oxidant solution (MOS) is a type of disinfectant that has many uses including disinfecting, sterilizing, and eliminating pathogenic microorganisms in water. An MOS may have advantages such as a higher disinfecting power, stable residual chlorine in water, elimination of biofilm, and safety. The main components of an MOS are chlorine and its derivatives (ClO− and HClO), which are produced by electrolysis of sodium chloride. It may also contain high amounts of hydroxy radicals, chlorine dioxide, dissolved ozone, hydrogen peroxide and oxygen from which the name \"mixed oxidant\" is derived.\n\nPerformance\n\nComparisons\n\nApplications\nMixed oxidant solutions for water treatment may improve safety, lower general corrosion rates, increase performance, and save money. MOS may be more effective than bleach and can be used for a variety of applications. Some of these applications are cited below.\nCooling water treatment: An MOS for industrial cooling water treatment and disinfection improves safety and thermal efficiency, lowers general corrosion rates, increases performance, and saves money, resulting in a reduction of downtime, maintenance, and expense. Additionally, it can improve workplace safety by eliminating the handling and storage of hazardous chemicals while maintaining steady microbiological control.\nCooling tower water treatment: An MOS improves cooling tower efficiency, safety, and cost compared to conventional biocide treatment methods for legionella prevention, biofilm removal, and inactivation of other performance-inhibiting waterborne organisms.\nIndustrial process water and wastewater treatment: As the lowest cost supplier of chlorine for disinfection and oxidation of process water and wastewater prior to discharge, an MOS is used in industrial wastewater treatment. MOS chemistry is more effective at biofilm control. Biochemical and Chemical oxygen demand removal, breakpoint chlorination of ammonia and hydrogen sulfide removal.\nMunicipal wastewater: As one of the world's most precious natural resources, the reuse of water is becoming increasingly important. MOS is both the most cost-effective solution and the preferred technology for disinfection and oxidation of wastewater for reuse or reintroduction into the environment eliminating many of the negative problems associated with traditional chlorine disinfection.\nDrinking water & beverage facilities: An MOS is a proven disinfectant for improving the quality and safety of drinking water with significant economic savings. For providing clean, safe drinking water ranges from rural communities to large cities. Also providing clean, safe water at food and beverage facilities. It is ideally suited for carbonated soft drinks bottling, brewing, dairy farms and dairy and food processing applications.\nAquatics and pools: An alternative to chlorine for pool cleaning is an MOS. It can reduce skin and eye irritation, and skin redness and dryness often associated with chlorine. An MOS can also reduce",
    "source": "wikipedia",
    "title": "Mixed oxidant",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_286069",
    "text": "In chemistry, a mixture is a material made up of two or more different chemical substances which can be separated by physical method. It is an impure substance made up of 2 or more elements or compounds mechanically mixed together in any proportion. A mixture is the physical combination of two or more substances in which the identities are retained and are mixed in the form of solutions, suspensions or colloids.\nMixtures are one product of mechanically blending or mixing chemical substances such as elements and compounds, without chemical bonding or other chemical change, so that each ingredient substance retains its own chemical properties and makeup. Despite the fact that there are no chemical changes to its constituents, the physical properties of a mixture, such as its melting point, may differ from those of the components. Some mixtures can be separated into their components by using physical (mechanical or thermal) means. Azeotropes are one kind of mixture that usually poses considerable difficulties regarding the separation processes required to obtain their constituents (physical or chemical processes or, even a blend of them).\n\nCharacteristics of mixtures\nAll mixtures can be characterized as being separable by mechanical means (e.g. purification, distillation, electrolysis, chromatography, heat, filtration, gravitational sorting, centrifugation). Mixtures differ from chemical compounds in the following ways:\n\nThe substances in a mixture can be separated using physical methods such as filtration, freezing, and distillation.\nThere is little or no energy change when a mixture forms (see Enthalpy of mixing).\nThe substances in a mixture keep their separate properties.\nIn the example of sand and water, neither one of the two substances changed in any way when they are mixed. Although the sand is in the water it still keeps the same properties that it had when it was outside the water.\n\nmixtures have variable compositions, while compounds have a fixed, definite formula.\nwhen mixed, individual substances keep their properties in a mixture, while if they form a compound their properties can change.\nThe following table shows the main properties and examples for all possible phase combinations of the three \"families\" of mixtures :\n",
    "source": "wikipedia",
    "title": "Mixture",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_19555",
    "text": "A molecule is a group of two or more atoms that are held together by attractive forces known as chemical bonds; depending on context, the term may or may not include ions that satisfy this criterion. In quantum physics, organic chemistry, and biochemistry, the distinction from ions is dropped and molecule is often used when referring to polyatomic ions.\nA molecule may be homonuclear, that is, it consists of atoms of one chemical element, e.g. two atoms in the oxygen molecule (O2); or it may be heteronuclear, a chemical compound composed of more than one element, e.g. water (two hydrogen atoms and one oxygen atom; H2O). In the kinetic theory of gases, the term molecule is often used for any gaseous particle regardless of its composition. This relaxes the requirement that a molecule contains two or more atoms, since the noble gases are individual atoms. Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are typically not considered single molecules.\nConcepts similar to molecules have been discussed since ancient times, but modern investigation into the nature of molecules and their bonds began in the 17th century. Refined over time by scientists such as Robert Boyle, Amedeo Avogadro, Jean Perrin, and Linus Pauling, the study of molecules is today known as molecular physics or molecular chemistry.\n\nEtymology\nAccording to Merriam-Webster and the Online Etymology Dictionary, the word \"molecule\" derives from the Latin \"moles\" or small unit of mass. The word is derived from French molécule (1678), from Neo-Latin molecula, diminutive of Latin moles \"mass, barrier\". The word, which until the late 18th century was used only in Latin form, became popular after being used in works of philosophy by Descartes.\nHistory\nThe definition of the molecule has evolved as knowledge of the structure of molecules has increased. Earlier definitions were less precise, defining molecules as the smallest particles of pure chemical substances that still retain their composition and chemical properties. This definition often breaks down since many substances in ordinary experience, such as rocks, salts, and metals, are composed of large crystalline networks of chemically bonded atoms or ions, but are not made of discrete molecules.\nThe modern concept of molecules can be traced back towards pre-scientific and Greek philosophers such as Leucippus and Democritus who argued that all the universe is composed of atoms and voids. Circa 450 BC Empedocles imagined fundamental elements (fire (), earth (), air (), and water ()) and \"forces\" of attraction and repulsion allowing the elements to interact.\nA fifth element, the incorruptible quintessence aether, was considered to be the fundamental building block of the heavenly bodies. The viewpoint of Leucippus and Empedocles, along with the aether, was accepted by Aristotle and passed to medieval and renaissance Europe.\nIn a more concrete manner, however, the concept of aggregates or units of",
    "source": "wikipedia",
    "title": "Molecule",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_4653948",
    "text": "Nanochemistry is an emerging sub-discipline of the chemical and material sciences that deals with the development of new methods for creating nanoscale materials. The term \"nanochemistry\" was first used by Ozin in 1992 as 'the uses of chemical synthesis to reproducibly afford nanomaterials from the atom \"up\", contrary to the nanoengineering and nanophysics approach that operates from the bulk \"down\"'. Nanochemistry focuses on solid-state chemistry that emphasizes synthesis of building blocks that are dependent on size, surface, shape, and defect properties, rather than the actual production of matter. Atomic and molecular properties mainly deal with the degrees of freedom of atoms in the periodic table. However, nanochemistry introduced other degrees of freedom that controls material's behaviors by transformation into solutions. Nanoscale objects exhibit novel material properties, largely as a consequence of their finite small size. Several chemical modifications on nanometer-scaled structures approve size dependent effects.\n\nNanochemistry is used in chemical, materials and physical science as well as engineering, biological, and medical applications. Silica, gold, polydimethylsiloxane, cadmium selenide, iron oxide, and carbon are materials that show its transformative power. Nanochemistry can make the most effective contrast agent of MRI out of iron oxide (rust) which can detect cancers and kill them at their initial stages. Silica (glass) can be used to bend or stop lights in their tracks. Developing countries also use silicone to make circuits for the fluids used in pathogen detection. Nano-construct synthesis leads to the self-assembly of the building blocks into  functional structures that may be useful for electronic, photonic, medical, or bioanalytical problems. Nanochemical methods can be used to create carbon nanomaterials such as carbon nanotubes, graphene, and fullerenes which have gained attention in recent years due to their remarkable mechanical and electrical properties.\n\nHistory\nOne of the first scientific reports is the colloidal gold particles synthesized by Michael Faraday as early as 1857. By the early 1940's, precipitated and fumed silica nanoparticles were being manufactured and sold in USA and Germany as substitutes for ultrafine carbon black for rubber reinforcements.\n",
    "source": "wikipedia",
    "title": "Nanochemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_1403587",
    "text": "The philosophy of chemistry considers the methodology and underlying assumptions of the science of chemistry.  It is explored by philosophers, chemists, and philosopher-chemist teams.  For much of its history, philosophy of science has been dominated by the philosophy of physics, but the philosophical questions that arise from chemistry have received increasing attention since the latter part of the 20th century.\n\nFoundations of chemistry\nMajor philosophical questions arise as soon as one attempts to define chemistry and what it studies.  Atoms and molecules are often assumed to be the fundamental units of chemical theory, but traditional descriptions of molecular structure and chemical bonding fail to account for the properties of many substances, including metals and metal complexes and aromaticity.\nAdditionally, chemists frequently use non-existent chemical entities like resonance structures to explain the structure and reactions of different substances; these explanatory tools use the language and graphical representations of molecules to describe the behavior of chemicals and chemical reactions that in reality do not behave as straightforward molecules.\nSome chemists and philosophers of chemistry prefer to think of substances, rather than microstructures, as the fundamental units of study in chemistry.  There is not always a one-to-one correspondence between the two methods of classifying substances.  For example, many rocks exist as mineral complexes composed of multiple ions that do not occur in fixed proportions or spatial relationships to one another.\nA related philosophical problem is whether chemistry is the study of substances or reactions.  Atoms, even in a solid, are in perpetual motion and under the right conditions many chemicals react spontaneously to form new products.  A variety of environmental variables contribute to a substance's properties, including temperature and pressure, proximity to other molecules and the presence of a magnetic field.  As Schummer puts it, \"Substance philosophers define a chemical reaction by the change of certain substances, whereas process philosophers define a substance by its characteristic chemical reactions.\"\nPhilosophers of chemistry discuss issues of symmetry and chirality in nature. Organic (i.e., carbon-based) molecules are those most often chiral. Amino acids, nucleic acids and sugars, all of which are found exclusively as a single enantiomer in organisms, are the basic chemical units of life. Chemists, biochemists, and biologists alike debate the origins of this homochirality. Philosophers debate facts regarding the origin of this phenomenon, namely whether it emerged contingently, amid a lifeless racemic environment or if other processes were at play. Some speculate that answers can only be found in comparison to extraterrestrial life, if it is ever found. Other philosophers question whether there exists a bias toward assumptions of nature as symmetrical, thereby causing resistance to an",
    "source": "wikipedia",
    "title": "Philosophy of chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_64063915",
    "text": "A phosphorimidazolide is a chemical compound in which a phosphoryl mono-ester is covalently bound to a nitrogen atom in an imidazole ring. They are a type of phosphoramidate. These phosphorus (V) compounds are encountered as reagents used for making new phosphoanhydride bonds with phosphate mono-esters, and as reactive intermediates in phosphoryl transfer reactions in some enzyme-catalyzed transformations. They are also being studied as critical chemical intermediates for the polymerization of nucleotides in pre-biotic settings. They are sometimes referred to as phosphorimidazolidates, imidazole-activated phosphoryl groups, and P-imidazolides.\n\nRole in oligonucleotide formation\nPhosphorimidazolides have been investigated for their mechanistic role in abiogenesis (the natural process by which life arose from non-living matter). Specifically, they have been proposed as the active electrophilic species which may have mediated the formation of inter-nucleotide phosphodiester bonds, thereby enabling template-directed oligonucleotide replication before the advent of enzymes. Phosphorimidazolides were originally proposed as mediators of this process by Leslie Orgel in 1968. Early studies showed that divalent metal cations such as Mg2+, Zn2+, and Pb2+ and a complementary template were required for the formation of short oligonucleotides, although nucleotides exhibited 5'-2' connectivity instead of 5'-3' connectivity of present-day life forms. It was also shown that Montmorillonite clay could provide a surface for phosphorimidazolide-mediated oligonucleotide formation with lengths of 20-50 bases.\nThe research group of Jack W. Szostak has continued to investigate the role of phosphorimidazolides in pre-biotic nucleotide polymerization. The group has investigated a number of imidazole derivatives in the search for chemical moieties which provide longer oligonucleotides necessary for propagating genetic information. Significantly, they discovered that phosphorimidazolides promote template-directed oligonucleotide formation via imidazolium-bridged dinucleotide intermediates.\nJohn D. Sutherland and colleagues have proposed that phosphorimidazolides may have formed in the chemical environment of early Earth via the activation of ribonucleotide phosphates by methyl isocyanaide and acetaldehyde followed by substitution with imidazole.\n",
    "source": "wikipedia",
    "title": "Phosphorimidazolide",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_363430",
    "text": "Photochemistry is the branch of chemistry concerned with the chemical effects of light. Generally, this term is used to describe a chemical reaction caused by absorption of ultraviolet (wavelength from 100 to 400 nm), visible (400–750 nm), or infrared radiation (750–2500 nm).\nIn nature, photochemistry is of immense importance as it is the basis of photosynthesis, vision, and the formation of vitamin D with sunlight. It is also responsible for the appearance of DNA mutations leading to skin cancers.\nPhotochemical reactions proceed differently than temperature-driven reactions. Photochemical paths access high-energy intermediates that cannot be generated thermally, thereby overcoming large activation barriers in a short period of time, and allowing reactions otherwise inaccessible by thermal processes.  Photochemistry can also be destructive, as illustrated by the photodegradation of plastics.\n\nConcepts\nPhotoexcitation is the first step in a photochemical process: the reactant is elevated to a state of higher energy, an excited state.\nPhotochemical reactions\n\nSee also\nPhotonic molecule\nPhotoelectrochemical cell\nPhotochemical logic gate\nPhotosynthesis\nLight-dependent reactions\nList of photochemists\nSingle-photon source\nPhotogeochemistry\nPhotoelectric effect\nPhotolysis\nBlueprint\nReferences\n\nFurther reading\nBowen, E. J., Chemical Aspects of Light. Oxford: The Clarendon Press, 1942. 2nd edition, 1946.\nPhotochemistry\n",
    "source": "wikipedia",
    "title": "Photochemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_51934343",
    "text": "Photopharmacology is an emerging multidisciplinary field that combines photochemistry and pharmacology. Built upon the ability of light to change the pharmacokinetics and pharmacodynamics of bioactive molecules, it aims at regulating the activity of drugs in vivo by using light. The light-based modulation is achieved by incorporating molecular photoswitches such as azobenzene and diarylethenes or photocages such as o-nitrobenzyl, coumarin, and BODIPY compounds into the pharmacophore. This selective activation of the biomolecules helps prevent or minimize off-target activity and systemic side effects. Moreover, light being the regulatory element offers additional advantages such as the ability to be delivered with high spatiotemporal precision, low to negligible toxicity, and the ability to be controlled both qualitatively and quantitatively by tuning its wavelength and intensity.\n\nHistory\nThough photopharmacology is a relatively new field, the concept of using light in therapeutic applications came into practice a few decades ago. Photodynamic therapy (PDT) is a well-established clinically practiced protocol in which photosensitizers are used to produce singlet oxygen for destroying diseased or damaged cells or tissues. Optogenetics is another method that relies on light for dynamically controlling biological functions especially brain and neural. Though this approach has proven useful as a research tool, its clinical implementation is limited by the requirement for genetic manipulation. Mainly, these two techniques laid the foundation for photopharmacology. Today, it is a rapidly evolving field with diverse applications in both basic research and clinical medicine which has the potential to overcome some of the challenges limiting the range of applications of the other light-guided therapies.\nFigure 1. Schematic representation of the mechanism of (a) photopharmacology (b) photodynamic therapy, and (c) optogenetics.\nThe discovery of natural photoreceptors such as rhodopsins in the eye inspired the biomedical and pharmacology research community to engineer light-sensitive proteins for therapeutic applications. The development of synthetic photoswitchable molecules is the most significant milestone in the history of light-delivery systems. Scientists are continuing with their efforts to explore new photoswitches and delivery strategies with enhanced performance to target different biological molecules such as ion channels, nucleic acid, and enzyme receptors. Photopharmacology research progressed from in vitro to in vivo studies in a significantly short period of time yielding promising results in both forms. Clinical trials are underway to assess the safety and efficacy of these photopharmacological therapies further and validate their potential as an innovative drug delivery approach.\n",
    "source": "wikipedia",
    "title": "Photopharmacology",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_1246630",
    "text": "Phytochemistry is the study of phytochemicals, which are chemicals derived from plants. Phytochemists strive to describe the structures of the large number of secondary metabolites found in plants, the functions of these compounds in human and plant biology, and the biosynthesis of these compounds. Plants synthesize phytochemicals for many reasons, including to protect themselves against insect attacks and plant diseases. The compounds found in plants are of many kinds, but most can be grouped into four major biosynthetic classes: alkaloids, phenylpropanoids, polyketides, and terpenoids.\nPhytochemistry can be considered a subfield of botany or chemistry. Activities can be led in botanical gardens or in the wild with the aid of ethnobotany. Phytochemical studies directed toward human (i.e. drug discovery) use may fall under the discipline of pharmacognosy, whereas phytochemical studies focused on the ecological functions and evolution of phytochemicals likely fall under the discipline of chemical ecology. Phytochemistry also has relevance to the field of plant physiology.\n\nTechniques\nTechniques commonly used in the field of phytochemistry are extraction, isolation, and structural elucidation (MS,1D and 2D NMR) of natural products, as well as various chromatography techniques (MPLC, HPLC, and LC-MS).\nPhytochemicals\nMany plants produce chemical compounds for defence against herbivores. The major classes of pharmacologically active phytochemicals are described below, with examples of medicinal plants that contain them. Human settlements are often surrounded by weeds containing phytochemicals, such as nettle, dandelion and chickweed.\nMany phytochemicals, including curcumin, epigallocatechin gallate, genistein, and resveratrol are pan-assay interference compounds and are not useful in drug discovery.\nGenetics\nContrary to bacteria and fungi, most plant metabolic pathways are not grouped into biosynthetic gene clusters, but instead are scattered as individual genes. Some exceptions have been discovered: steroidal glycoalkaloids in Solanum, polyketides in Pooideae, benzoxazinoids in Zea mays, triterpenes in Avena sativa, Cucurbitaceae, Arabidopsis, and momilactone diterpenes in Oryza sativa.\n\n\n== References ==\n",
    "source": "wikipedia",
    "title": "Phytochemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_43939378",
    "text": "Pressure-induced hydration (PIH), also known as “super-hydration”, is a special case of pressure-induced insertion whereby water molecules are injected into the pores of microporous materials. In PIH, a microporous material is placed under pressure in the presence of water in the pressure-transmitting fluid of a diamond anvil cell.\nEarly physical characterization and initial diffraction experiments in zeolites were followed by the first unequivocal structural characterization of PIH in the small-pore zeolite natrolite (Na16Al16Si24O80·16H2O), which in its fully super-hydrated form, Na16Al16Si24O80·32H2O, doubles the amount of water it contains in its pores.\nPIH has now been demonstrated in natrolites containing Li, K, Rb and Ag as monovalent cations as well as in large-pore zeolites, pyrochlores, clays and graphite oxide.\nUsing the noble gases Ar, Kr, and Xe as well as CO2 as pressure-transmitting fluids, researchers have prepared and structurally characterized the products of reversible, pressure-induced insertion of Ar Kr, and CO2 as well as the irreversible insertion of Xe and water.\n\n\n== References ==\n\n",
    "source": "wikipedia",
    "title": "Pressure-induced hydration",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_77310246",
    "text": "Probico also known as PBCo, is a comprehensive formulation consisting of proanthocyanidin, biotin, and coenzyme Q10. Probico addresses three primary causes of hair loss: amino acid deficiency, inadequate hair synthesis, and follicle inflammation, excluding the hormonal imbalance and blood circulation issues which require prescription medications.\n\nChemical description\nThe key characteristics of the components of Probico are as follows:\n",
    "source": "wikipedia",
    "title": "Probico",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_23568658",
    "text": "Radioanalytical chemistry focuses on the analysis of sample for their radionuclide content.  Various methods are employed to purify and identify the radioelement  of interest through chemical methods and sample measurement techniques.\n\nHistory\nThe field of radioanalytical chemistry was originally developed by Marie Curie with contributions by Ernest Rutherford and Frederick Soddy.  They developed chemical separation and radiation measurement techniques on terrestrial radioactive substances.  During the twenty years that followed 1897 the concepts of radionuclides was born.  Since Curie's time, applications of radioanalytical chemistry have proliferated.  Modern advances in nuclear and radiochemistry research have allowed practitioners to apply chemistry and nuclear procedures to elucidate nuclear properties and reactions, used radioactive substances as tracers, and measure radionuclides in many different types of samples.\nThe importance of radioanalytical chemistry spans many fields including chemistry, physics, medicine, pharmacology, biology, ecology, hydrology, geology, forensics, atmospheric sciences, health protection, archeology, and engineering.  Applications include: forming and characterizing new elements, determining the age of materials, and creating radioactive reagents for specific tracer use in tissues and organs.  The ongoing goal of radioanalytical researchers is to develop more radionuclides and lower concentrations in people and the environment.\nRadiation decay modes\n\nRadiation detection principles\n\nChemical separation techniques\nDue to radioactive nucleotides have similar properties to their stable, inactive, counterparts similar analytical chemistry separation techniques can be used.  These separation methods include precipitation, Ion Exchange, Liquid Liquid extraction, Solid Phase extraction, Distillation, and Electrodeposition.\nRadioanalytical chemistry principles\n\nTypical radionuclides of interest\n\nQuality assurance\nAs this is an analytical chemistry technique quality control is an important factor to maintain.  A laboratory must produce trustworthy results.  This can be accomplished by a laboratories continual effort to maintain instrument calibration, measurement reproducibility, and applicability of analytical methods.  In all laboratories there must be a quality assurance plan.  This plan describes the quality system and procedures in place to obtain consistent results.  Such results must be authentic, appropriately documented, and technically defensible.\"  Such elements of quality assurance include organization, personnel training, laboratory operating procedures, procurement documents, chain of custody records, standard certificates, analytical records, standard procedures, QC sample analysis program and results, instrument testing and maintenance records, results of performance demonstration projects, results of data assessment, audit reports, and record retention policies.  \nThe cost of quality assurance is continu",
    "source": "wikipedia",
    "title": "Radioanalytical chemistry",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_49301338",
    "text": "Rayleigh fractionation describes the evolution of a system with multiple phases in which one phase is continuously removed from the system through fractional distillation. It is used in particular to describe isotopic enrichment or depletion as material moves between reservoirs in an equilibrium process. Rayleigh fractionation holds particular importance in hydrology and meteorology as a model for the isotopic differentiation of meteoric water due to condensation.\n\nThe Rayleigh equation\nThe original Rayleigh equation was derived by Lord Rayleigh for the case of fractional distillation of mixed liquids. \nThis is an exponential relation that describes the partitioning of isotopes between two reservoirs as one reservoir decreases in size. The equations can be used to describe an isotope fractionation process if: (1) material is continuously removed from a mixed system containing molecules of two or more isotopic species (e.g., water with 18O and 16O, or sulfate with 34S and 32S), (2) the fractionation accompanying the removal process at any instance is described by the fractionation factor a, and (3) a does not change during the process. Under these conditions, the evolution of the isotopic composition in the residual (reactant) material is described by:\n\n  \n    \n      \n        \n          \n            R\n            \n              R\n              \n                0\n              \n            \n          \n        \n        =\n        \n          \n            (\n            \n              \n                X\n                \n                  X\n                  \n                    0\n                  \n                \n              \n            \n            )\n          \n          \n            a\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle {\\frac {R}{R^{0}}}=\\left({\\frac {X}{X^{0}}}\\right)^{a-1}}\n  \n\nwhere R = ratio of the isotopes (e.g., 18O/16O) in the reactant, R0 = initial ratio, X = the concentration or amount of the more abundant (lighter) isotope (e.g.,16O), and X0 = initial concentration. Because the concentration of X >> Xh (heavier isotope concentration), X is approximately equal to the amount of original material in the phase. Hence, if \n  \n    \n      \n        f\n        =\n        X\n        \n          /\n        \n        \n          X\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle f=X/X^{0}}\n  \n = fraction of material remaining, then:\n\n  \n    \n      \n        R\n        =\n        \n          R\n          \n            0\n          \n        \n        \n          f\n          \n            a\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle R=R^{0}f^{a-1}}\n  \n\nFor large changes in concentration, such as they occur during e.g. distillation of heavy water, these formulae need to be integrated over the distillation trajectory. For small changes such as occur during transport of water vapour through the atmosphere, the differentiated equation will usually be sufficient.\n",
    "source": "wikipedia",
    "title": "Rayleigh fractionation",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_45413683",
    "text": "The scale of a chemical process refers to the rough ranges in mass or volume of a chemical reaction or process that define the appropriate category of chemical apparatus and equipment required to accomplish it, and the concepts, priorities, and economies that operate at each. While the specific terms used—and limits of mass or volume that apply to them—can vary between specific industries, the concepts are used broadly across industry and the fundamental scientific fields that support them. Use of the term \"scale\" is unrelated to the concept of weighing; rather it is related to cognate terms in mathematics (e.g., geometric scaling, the linear transformation that enlarges or shrinks objects, and scale parameters in probability theory), and in applied areas (e.g., in the scaling of images in architecture, engineering, cartography, etc.).\nPractically speaking, the scale of chemical operations also relates to the training required to carry them out, and can be broken out roughly as follows:\n\nprocedures performed at the laboratory scale, which involve the sorts of procedures used in academic teaching and research laboratories in the training of chemists and in discovery chemistry venues in industry,\noperations at the pilot plant scale, e.g., carried out by process chemists, which, though at the lowest extreme of manufacturing operations, are on the order of 200- to 1000-fold larger than laboratory scale, and used to generate information on the behavior of each chemical step in the process that might be useful to design the actual chemical production facility;\nintermediate bench scale sets of procedures, 10- to 200-fold larger than the discovery laboratory, sometimes inserted between the preceding two;\noperations at demonstration scale and full-scale production, whose sizes are determined by the nature of the chemical product, available chemical technologies, the market for the product, and manufacturing requirements, where the aim of the first of these is literally to demonstrate operational stability of developed manufacturing procedures over extended periods (by operating the suite of manufacturing equipment at the feed rates anticipated for commercial production).\nFor instance, the production of the streptomycin-class of antibiotics, which combined biotechnologic and chemical operations, involved use of a 130,000 liter fermenter, an operational scale approximately one million-fold larger than the microbial shake flasks used in the early laboratory scale studies.\nAs noted, nomenclature can vary between manufacturing sectors; some industries use the scale terms pilot plant and demonstration plant  interchangeably.\nApart from defining the category of chemical apparatus and equipment required at each scale, the concepts, priorities and economies that obtain, and the skill-sets needed by the practicing scientists at each, defining scale allows for theoretical work prior to actual plant operations (e.g., defining relevant process parameters used in the n",
    "source": "wikipedia",
    "title": "Scale (chemistry)",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_69512983",
    "text": "The School of Molecular Sciences is an academic unit of The College of Liberal Arts and Sciences at Arizona State University (ASU). The School of Molecular Sciences (SMS) is responsible for the study and teaching of the academic disciplines of chemistry and biochemistry at ASU.\n\nHistory\nChemistry instruction at ASU can be traced back to the early 1890s. At that time, the educational institution, a Normal School for the Territory of Arizona, “acquired...a supply of chemicals” for instructional purposes. Chemistry classes were held in Old Main during the late 1800s and into the early 1900s, taught by Frederick M. Irish.\nIn 1927, President Arthur John Matthews hired George Bateman, the first faculty to hold a PhD who was not also a principal or president of the school. Bateman taught chemistry classes, among other things, for forty years. He oversaw the development of the physical sciences at ASU, including new science facilities and degrees.\nIn 1946, new majors leading to degrees were added, including Physical and Biological Science. In 1947 the State of AZ designated $525,000 for a new science building.\nIn 1953 the first college, the College of Arts and Sciences was established with 14 departments. In 1954 Arizona State College was restructured into 4 colleges, which went into effect in the 1955–56 academic year: the College of Liberal Arts, the College of Education, the College of Applied Arts and Sciences, and the College of Business and Public Administration.\nIn 1957, the Department of Chemistry first appeared in the Arizona State College Bulletin (Vol. LXXII No. 2, April 1957), listed under the Division of Physical Sciences. Early chemists, such as LeRoy Eyring helped build ASU's strong science reputation; Roland K. Robins conducted cancer research as early as 1957.\nIn 1958, Arizona State College was renamed Arizona State University. Chemistry was the first department to be approved to offer a doctoral degree.\nIn 1960, George Boyd, the university's first coordinator of research, helped secure a portion of Harvey H. Nininger’s meteorites for ASU, making it the largest university-based meteorite collection in the world.\nIn 1961, Geochemist Carleton B. Moore became the first director of the Center for Meteorite Studies, which at the time was housed in the Department of Chemistry.\nIn 1963, Peter R. Buseck, who pioneered high-resolution transmission electron microscopy (TEM) research on meteorites and terrestrial minerals.\nIn 1963, ASU awarded its first doctoral degrees to four students, one of whom, Jesse W. Jones, was the first Chemistry PhD of ASU and the first African American to earn a PhD at ASU. Jones went on to teach chemistry at Baylor University for over 30 years.\nIn 1965 Robert Pettit was hired and began developing marine-organism research that led to the creation of anti-cancer drugs and, in 1973, what became the Cancer Research Institute. Pettit taught at ASU until his retirement in 2021.\nIn 1967, George Bateman, after enjoying a produ",
    "source": "wikipedia",
    "title": "School of Molecular Sciences",
    "topic": "Chemistry"
  },
  {
    "id": "wiki_75786010",
    "text": "The shape of the atomic nucleus depends on the variety of factors related to the size and shape of its nucleon (proton or neutron) constituents and the nuclear force holding them together. The spatial extent of the prolate spheroid  nucleon (and larger nuclides) is determined by root mean squared (RMS) charge radius of the proton, as determined mainly by electron and muon scattering experiments, as well as spectroscopic experiments. An important factor in the internal structure of the nucleus is the nucleon-nucleon potential, which ultimately governs the distance between individual nucleons, and the radial charge density of each nuclide. The charge density of some light nuclide indicates a lesser density of nucleonic matter in the center which may have implications for a nucleonic nuclear structure. A surprising non-spherical expectation for the shape of the nucleus originated in 1939 in the spectroscopic analysis of the quadrupole moments  while the prolate spheroid shape of the nucleon arises from analysis of the intrinsic quadruple moment. The simple spherical approximation of nuclear size and shape provides at best a textbook introduction to nuclear size and shape. The unusual cosmic abundance of alpha nuclides has inspired geometric arrangements of alpha particles as a solution to nuclear shapes, although the atomic nucleus generally assumes a prolate spheroid shape. Nuclides can also be discus-shaped (oblate deformation), triaxial (shape of an ellipsoid with its three principal axes having different lengths) or pear-shaped.\n\nOrigins of nuclear shape\nThe atomic nucleus is composed of protons and neutrons (collectively called nucleons). In the Standard Model of particle physics, nucleons are in the group called hadrons, the smallest known particles in the universe to have measurable size and shape. Each is in turn composed of three quarks. The spatial extent and shape of nucleons (and nuclides assembled from them) ultimately involves quark interactions within and between nucleons. The quark itself does not have measurable size at the experimental limit set by the electron (≈ 10−18 m in diameter). The size, or RMS charge radius, of the proton (the smallest nuclide) has a 2018 CODATA recommended value of 0.8414 (19) fm (10−15 m), although values may vary by a few percent according to the experimental method employed (see proton radius puzzle). Nuclide size ranges up to ≈ 6 fm. The largest stable nuclide, lead-208, has an RMS charge radius of 5.5012 fm, and the largest unstable nuclide americium-243 has an experimental RMS charge radius of 5.9048 fm.  The main source of nuclear radius values derives from elastic scattering experiments (electron and muon), but nuclear radii data also come from experiments on spectroscopic isotope shifts (x-ray and optical), β decay by mirror nuclei, α decay, and neutron scattering. Although the radius values delimit the spatial extent of the nucleus, spectroscopic and scattering experiments dating back to 1935 in",
    "source": "wikipedia",
    "title": "Shape of the atomic nucleus",
    "topic": "Chemistry"
  }
]